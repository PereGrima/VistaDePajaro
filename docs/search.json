[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística a vista de pájaro",
    "section": "",
    "text": "PRESENTACIÓN\nTodo esto son pruebas.\nDe qué va este libro\nQué pretende\nCuales son sus peculiaridades\nA quien va dirigido. Lo empezamos a escribir pensando en los profesores de enseñanza secundaria, con la idea de dar una visión general de la estadística, más amplia de la que ellos deben enseñar, que les diera perspectiva y seguridad transmitiendo las ideas clave que deben quedar en los alumnos, evitando planteamientos o enfoques exclusivamente matemáticos.\nAlgunas ideas clave:\n\nDistinguir el conocimiento basado en tradiciones, suposiciones o en el “siempre se ha considerado así” del conocimiento científico basado en datos.\nRecoger datos con la calidad y la representatividad requerida no es fácil. Conviene conocer algunas ideas clave a este respecto.\nLa información que contienen los datos se puede dar con medidas estadísticas o con representaciones gráficas.\nLos datos siempre están afectados por una cierta variabilidad aleatoria. Hay técnicas para separar la señal del ruido. Tan malo es perderse la señan como sobreinterpretar el ruido.\n\nAunque el contenido de este texto es de un nivel más básico que el habitual de los cursos generales de estadística que se imparten en muchos grados universitarios, creemos que si se entienden y se interiorizan estas ideas podemos considerar que se han cumplido los objetivos que se plantean en estos cursos.\n\nTodos los datos que se usan están aquí\n\nLicencia\n\nLA PORTADA ES DE OTRO LIBRO. Solo para ver como queda."
  },
  {
    "objectID": "presentacion.html",
    "href": "presentacion.html",
    "title": "1  Presentación",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Parte_Descriptiva.html",
    "href": "Parte_Descriptiva.html",
    "title": "ESTADÍSTICA DESCRIPTIVA",
    "section": "",
    "text": "Síntesis numérica de datos y representaciones gráficas.\nIdeas de:\n\nLibro Preguntas frecuentes: Media, mediana, por qué dividimos por n-1, como calcular los cuartiles.\nArtículo analogías: Síntesis numérica y retrato robot, el rio tiene una profundidad media de 1,5 m\nLibro matemáticas en primera plana: Buenos y malos usos de las representaciones gráficas."
  },
  {
    "objectID": "sintesisNumerica.html",
    "href": "sintesisNumerica.html",
    "title": "1  Síntesis numérica de datos",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n#    \n\n\nSample  \n\n\nIQR’s from Q\n\n\n\n\n\n\n1\n\n\n3\n\n\n3.3\n\n\n\n\n\n\n2\n\n\n2\n\n\n2.8\n\n\n\n\n\n\n3\n\n\n3\n\n\n2.7\n\n\n\n\n\n\n4\n\n\n4\n\n\n2.6\n\n\n\n\n\n\n5\n\n\n4\n\n\n2.4\n\n\n\n\n\n\n6\n\n\n3\n\n\n1.9\n\n\n\n\n\n\n7\n\n\n2\n\n\n1.7\n\n\n\n\n\n\n8\n\n\n1\n\n\n1.6\n\n\n\n\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "graficos.html",
    "href": "graficos.html",
    "title": "2  Representaciones gráficas",
    "section": "",
    "text": "Todavía no he hecho nada sobre esto, pero tengo mucho material y las ideas claras."
  },
  {
    "objectID": "Parte_Corre_Regre.html",
    "href": "Parte_Corre_Regre.html",
    "title": "CORRELACIÓN Y REGRESIÓN",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "correlacion.html",
    "href": "correlacion.html",
    "title": "12  Medidas de relación lineal",
    "section": "",
    "text": "APÉNDICE 9.1"
  },
  {
    "objectID": "regresion.html",
    "href": "regresion.html",
    "title": "11  Regresión",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "El País, 2024. https://elpais.com/economia/negocios/2024-05-04/mi-jefe-cobra-77-veces-mas-que-yo-estas-son-las-empresas-espanolas-con-mayor-desigualdad-salarial-entre-directivos-y-empleados.html.\n\n\nDraper, N. R., and H. Smith. 1998. Applied Regression Analysis.\n3rd ed. USA: John Wiley & Sons, Inc.\n\n\nINE. 2023. “Encuesta Anual de Estructura Salarial. Año\n2021.” Instituto Nacional de EStadística (INE), España. https://www.ine.es/prensa/ees_2021.pdf.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nMontgomery, D. C., and E. A. Peck. 1992. Introduction to Linear\nRegression Analysis. 2nd ed. USA: John Wiley & Sons, Inc.\n\n\nNavidi, W. 2010. Statistics for Engineers and Scientists. 3rd\ned. USA: McGraw Hill.\n\n\nPeña, D. 2002. Regresión y Diseño de Experimentos. 1st ed.\nMadrid: Alianza Editorial.\n\n\nRyan, T. A., B. L Joiner, and B. F. Ryan. 1976. The Minitab Student\nHandbook. 1st ed. USA: Duxbury Press.\n\n\nWeisberg, S. 2014. Applied Linear Regression. 4th ed. USA: John\nWiley & Sons, Inc.\n\n\nWilson, M. E., and L. E. Mather. 1974. “Lefe Expectancy.”\nJournal of the American Medical Association 229 (11): 1421–22.\n\n\nWolfram_Data_Repository. 2016. Sample Data: Black Cherry Trees. https://doi.org/10.24097/wolfram.76288.data."
  },
  {
    "objectID": "correlacion.html#necesidad-de-una-medida-de-relación",
    "href": "correlacion.html#necesidad-de-una-medida-de-relación",
    "title": "10  Correlación",
    "section": "10.1 Necesidad de una medida de relación",
    "text": "10.1 Necesidad de una medida de relación\nSee Knuth (1984) for additional discussion of literate programming."
  },
  {
    "objectID": "correlacion.html#la-covarianza",
    "href": "correlacion.html#la-covarianza",
    "title": "3  Correlación",
    "section": "3.2 La covarianza",
    "text": "3.2 La covarianza"
  },
  {
    "objectID": "correlacion.html#de-la-covarianza-al-coeficiente-de-correlación",
    "href": "correlacion.html#de-la-covarianza-al-coeficiente-de-correlación",
    "title": "3  Correlación",
    "section": "3.3 De la covarianza al coeficiente de correlación",
    "text": "3.3 De la covarianza al coeficiente de correlación"
  },
  {
    "objectID": "correlacion.html#propiedades-del-coeficiente-de-correlación",
    "href": "correlacion.html#propiedades-del-coeficiente-de-correlación",
    "title": "3  Correlación",
    "section": "3.4 Propiedades del coeficiente de correlación",
    "text": "3.4 Propiedades del coeficiente de correlación\n\n3.4.1 Acotado entre 0 y 1.\n\n\n3.4.2 Correlación no implica relación causa-efecto\n\n\n3.4.3 Correlación estadísticamente significativa\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "correlacion.html#covarianza",
    "href": "correlacion.html#covarianza",
    "title": "12  Medidas de relación lineal",
    "section": "12.2 Covarianza",
    "text": "12.2 Covarianza\nAparece como en las calculadoras con funciones estadísticas, en los paquetes de software y también en las hojas de cálculo, aunque en la práctica resulta poco útil como medida de relación lineal. Aquí la incluimos porque su expresión es fácil de justificar y ayuda a entender la fórmula del coeficiente de correlación.\n\nDeducción de la fórmula\nLa figura 12.3 muestra la relación entre dos variables \\(X\\) e \\(Y\\). En la derecha el diagrama se ha dividido en cuatro cuadrantes trazando una línea vertical que pasa por la media de los valores de \\(X\\) y una horizontal por la media de los valores de \\(Y\\). A estos valores medios los designamos \\(\\bar{x}\\) e \\(\\bar{y}\\) respectivamente. Los cuadrantes van del I al IV en el sentido de las agujas del reloj.\n\n\n\nFigura 12.3: Deducción de la fórmula de la covarianza\n\n\nEn todos los puntos del primer cuadrante la distancia \\(x - \\bar{x}\\) es positiva, ya que todos se encuentran a la derecha de \\(\\bar{x}\\). También será positiva la distancia \\(y - \\bar{y}\\) ya que todos están por encima de \\(\\bar{y}\\). Por tanto, el producto de ambas distancias \\((x - \\bar{x})(y - \\bar{y})\\) será positivo para todos los puntos que se hayan en el primer cuadrante.\nPara los del segundo cuadrante este producto es negativo ya que la distancia \\(x - \\bar{x}\\) sigue siendo positiva (todos los puntos se encuentran a la derecha de \\(\\bar{x}\\)), pero \\(y - \\bar{y}\\) será negativo (todos están por debajo de \\(\\bar{y}\\)).\nEn el tercer cuadrante el producto de las distancias es positivo porque ambas distancias son negativas y en el cuarto vuelve a ser negativo ya que \\(y - \\bar{y}\\) es positivo pero \\(x - \\bar{x}\\) es negativo.\nCon \\(n\\) puntos, la suma de todos estos productos será \\(\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})\\). Si la mayoría se encuentra en los cuadrantes I y III, tal como ocurre en la figura 12.3, el resultado del sumatorio será un valor positivo, mientras que si están en los cuadrantes II y IV el resultado será negativo. Si no existe ninguna relación entre \\(X\\) e \\(Y\\) los puntos se repartiran de forma más menos equilibrada, tendiendo a compensarse los productos positivos con los negativos y dando un resultado alrededor de cero.\nLa covarianza es el valor de ese sumatorio dividido por el número de puntos que intervienen en su cálculo, algo así como el promedio de los productos \\((x_i -\\bar {x})(y_i -\\bar {y})\\). Si nuestro interés no es conocer la covarianza de los datos disponibles, sino estimar su valor en la población, dividimos por \\(n-1\\) en vez de por \\(n\\), por la misma razón que lo hacemos cuando estimamos el valor de la varianza. Como lo habitual es esto último, escribimos la fórmula de la forma:\n\\[ \\widehat{\\text{Cov}}(X,Y) = \\frac{\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})}{n-1} \\]\n\n\nPropiedades\nLa covarianza tiene un gran protagonismo en el terreno de los modelos teóricos, pero apenas se usa para valorar la relación lineal en un conjunto de datos. Para este menester tiene el inconveniente de que su valor depende de las unidades utilizadas.\nSi se calcula la covarianza entre la estatura de madres e hijas con los datos representados en la figura 12.1 cuyas unidadess son pulgadas (in) se obtiene un valor de 3,005 in2. Sin embargo, si cambiamos las unidades a cm (1 pulgada = 2,54 cm) el gráfico tiene el mismo aspecto y el grado de relación sigue siendo exactamente el mismo, pero ahora el valor obtenido es 19,39 cm2, y si las estaturas se expresan en metros tenemos que la covarianza es igual a 0,0019 m2.\nPor otra parte, para su valoración no tenemos ningún marco de referencia que nos permita evaluar si el valor obtenido es grande o pequeño. Todos estos problemas quedan resueltos con el uso del coeficiente de correlación.\n\n\n\n\n\n\nMalas noticias para la covarianza\n\n\n\nDados unos valores de \\(X\\), la máxima covarianza no se obtiene cuando los valores de \\(Y\\) se alinean según una recta. Por ejemplo, sean los valores de \\(X\\) = 1, 2, 3, 4 y 5, si los de \\(Y\\) son: 2, 4, 6, 8, y 10 (relación lineal perfecta) la covarianza entre \\(X\\) e \\(Y\\) es igual a 5 pero si sustuimos el último 10 por 15, la covarianza aumenta y pasa a valer 7,5. Esto no deja en muy buen lugar a la covarianza como medida de relación lineal."
  },
  {
    "objectID": "correlacion.html#coeficiente-de-correlación",
    "href": "correlacion.html#coeficiente-de-correlación",
    "title": "12  Medidas de relación lineal",
    "section": "12.3 Coeficiente de correlación",
    "text": "12.3 Coeficiente de correlación\nPara calcular el coeficiente de correlación, \\(r\\), entre dos variables \\(X\\) e \\(Y\\) basta con calcular su covarianza y dividirla por el producto de las desviaciones típicas de \\(X\\) e \\(Y\\), es decir:\n\\[ r  = \\frac{\\frac{\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})}{n-1}}\n        {\\sqrt \\frac {{\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2}}{n-1} \\sqrt{ \\frac {\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 }{n-1}}} \\]\nEs fácil comprobar que desaparecen los denominadores tanto de la covarianza como de las desviaciones típicas, quedando:\n\\[ r  = \\frac{\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right ) \\left ( y_i - \\bar{y} \\right ) }\n        {\\sqrt {\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2}  \\sqrt{\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 }} \\]\nCon esta sencilla transformación se resuelven los problemas de la covarianza:\n\nEs un valor adimensional y, por tanto, no depende de las unidades utilizadas.\nSu valor está acotado entre -1 y 1 (correlación perfecta negativa y positiva repectivamente).\n\nQue es adimensional es evidente, puesto que el numerador tiene las mismas unidades que el denominador. Que es igual a 1 o -1 cuando la relación lineal es perfecta lo demostramos en un apéndice de este capítulo usando álgebra elemental. Que está acotado entre -1 y 1 no es trivial, pero lo demostramos en un apéndice del siguiente capítulo usando el concepto de coeficiente de determinación de la recta ajustada (también se puede demostrar como consecuencia directa de la desigualdad de Cauchy-Schwarz, pero eso no son matemáticas elementales) .\n\n\n\n\n\n\nCoeficiente de correlación “de Pearson”\n\n\n\nA la denominación del coeficiente de correlación a veces se le añade “de Pearson” porque fue este estadístico el que lo desarrolló. Aunque existen otros coeficientes de correlación de mucho menor uso, no es necesario el añadido. Si no se dice de que tipo es siempre entendemos que se refiere al de Pearson.\n\n\n\nCoeficiente de correlación y diagrama bivariante\nEl valor del coeficiente de correlación no sustituye la información que proporciona el diagrama bivariante. Un mismo coeficiente de correlación puede corresponder a situaciones muy distitnas y si solo nos dan el valor de \\(r\\) es imposible saber a cual de ellas corresponde.\nUna demostración de la importancia de no descuidar el análisis gráfico de los datos lo constituye el llamado “cuarteto de Anscombe”, formado por cuatro conjuntos de datos que presentan el mismo coeficiente de correlación y la misma recta ajustada pero que al analizarlos gráficamente muestran situaciones claramente distintas (figura 12.4).\n\n\n\nFigura 12.4: Cuarteto de Anscombe. Cuatro situaciones muy distintas pero todas ellas con el mismo coeficiente de correlación (\\(r = 0,816\\)) y la misma recta ajustada (\\(y = 3+0,5x\\))\n\n\n\n\nCorrelación estadísticamente significativa\nEl diagrama bivariante de la figura 12.5 está construido con los datos de 29 plantas y muestra la relación entre el peso de los frutos obtenidos y el tiempo transcurrido entre la plantación y la recolección. Se ha añadido la recta de regresión ajustada a estos puntos.\n\n\n\nFigura 12.5: Relación entre el peso de los frutos recogidos en 22 plantas y el tiempo transcurrido desde su plantación hasta la recolección\n\n\nParece dar la sensación de que cuanto más se tarda en recoger los frutos mayor peso se obtiene. El coeficiente de correlación es positivo y está bastante alejado de cero, \\(r=0,323\\). ¿Quiere esto decir que para maximizar el peso de la cosecha vale la pena esperar a los 150 días?\nSi dos variables son totalmente independientes, como dos conjuntos de números aleatorios, no por ello hay que esperar que su coeficiente de correlación sea igual a cero. Para que esto ocurra también la covarianza debe ser igual a cero y para ello se debe dar un equilibrio perfecto en los puntos de cada cuadrante para que se compensen perfectamente los productos de las distancias, lo cual es muy difícil que se dé en la práctica.\nPor tanto, cuando tenemos dos conjuntos de datos que provienen de poblaciones independientes, no hay que esperar que su coeficiente de correlación sea exactamente igual a cero. Estará en torno a cero.\n¿Qué significa “en torno a”? La distancia que exigimes respecto a cero para tomarnos en serio la correlación depende del número de datos de que se disponga. Si solo tenemos dos datos (dos puntos sobre el diagrama) el coeficiente de correlación valdrá -1 o 1 con independencia de la relación que haya entre esas variables, por tanto, ese resultado no tiene ningún valor. Si se tienen pocos datos se exige más distancia que si se tienen muchos porque es más fácil que -por casualidad- se obtengan valores extremos.\nEn nuestro caso de 22 datos ¿cuál es la distancia exigida? Vamos a generar 22 números aleatorios de una distribución Normal con media \\(\\mu = 137\\) y \\(\\sigma = 6.5\\) que podemos considerar que son los parámetros de la población de la que provienen los valores de los pesos, y otros 22 números aleatorios, totalmente independientes de los anteirores, en este caso también de una distribución Normal, pero con parámetros \\(\\mu = 6\\) y \\(\\sigma = 0.5\\) para simular los valores del tiempo hasta la recogida. Cuando se calcula el coefiente de correlación con ese conjunto de datos simulados se obtiene un valor que corresponde a muestras de poblaciones independientes. Este proceso se puede repetir muchas veces y cada vez se obtiene un valor del coeficiente de correlación que corresponde a una situación de variables independientes.\nHemos repetido este proceso 100.000 veces, de manera que hemos obtenido 100.000 valores del coefiente de correlación entre dos conjuntos de datos de origen similar a los nuestros y que son absolutamente independientes. Los resultamos obtenidos se resumen en el histograma de la figura 12.6 ===Esto hay que explicarlo mejor===\n\n\n\nFigura 12.6: Valores del coeficiente de correlación obtenidas por simulación\n\n\nA la vista de este histograma, podemos afirmar que si nos hubiera salido un coeficiente de correlación de, por ejemplo 0,8, podríamos afirmar con un riesgo de equivocarnos muy pequeño que nuestras variables estan correlacionadas porque si fueran independientes un valor como ese o mayor prácticamente no se da nunca. Sin embargo, si nuesto valor fuera \\(r=0,2\\) no podríamos decir que existe correlación porque valores como ese, e incluso mayores, son muy habituales entre variables independientes cuando se calcula con muestras de \\(n=22\\) observaciones. ===todo esto hay que redactarlo mejor===\nEn nuestro caso tenemos \\(r=0,323\\) se trata de una distancia de cero “normal” si no hay relación entre ambas variables…\nHablar de las tablas, de sus características básicas y de que trataremos de como se han construido en el siguiente capítulo.\n===Esto hay que explicarlo mejor===\n\n\nCorrelación no implica relación causa-efecto\n===empezar comentando el gráfico===\nQue dos variable presenten un coeficiente de correlación estadísticamente significativo (esten correlacionadas) no significa que el cambio en una provoque -sea la causa- del cambio en la otra.\nPuede ocurrir que haya una tercera variable no considerada relacionada con las dos que se observan, por ejemplo, se dice que hay una alta correlación entre el número de bomberos que acuden a apagar un incendio y los daños que ese incendio causa, pero, naturalmente, a nadie se le ocurre enviar menos bomberos para que haya menor daños porque estas dos variables, aunque están correlacionadas y en el diagrama bivariante se observe una nube de puntos que marca una clara tendencia de que al aumentar el número de bomberos aumentan los daños, en este caso la variable oculta relacionada con las dos que observamos es la magnitud del incencdio, relacionada con los daños y con el número de bomberos. Existen numerosos ejemplos chistosos de este tipo de situaciones…\n\n\n\nFigura 12.7: Cuatro situaciones que muestran diferentes grados de relación\n\n\nTambién puede ocurrir que la relación sea debida al azar. Si exploramos muchos pares de variables, seguro que algunas apareceran con una relación estrecha, aunque en realidad no tengan nada que ver. La página web https://www.tylervigen.com/spurious-correlations contiene muchos ejemplos de este tipo.\n\n\nCuriosidad sobre el coeficiente de correlación\nSi solo tenemos n = 2 puntos sobre el diagrama bivariante, el coeficiente de correlación solo puede tomar los valores -1 y 1, ambos con la misma probabilidad . Si tenemos n = 3 aparece una distribución muy rara tanto para explicar la variabidad ligada a fenómenos naturales como en los modelos teóricos que usamos habitualmente: los valores más frecuentes están en los extremos (-1 y 1) mientras que en torno al centro (alrededor de cero) se dan los de menor probabilidad.Para n = 4 todos los valores del coeficiente de correlación son igualmente probables , para n = 5 la distribución de probabilidad tiene forma de semielipse, y a media que aumenta el valor de n va apareciendo la típica forma de campana. (figura).\n\n\n\nFigura 12.8: Distribución del coeficiente de correlación según el tamaño, \\(n\\), de la muestra considerada\n\n\nLas distribuciones de la figura +++ se puede reproducir por simulación o directamente usando la función densidad de probabilidad de coefiente de correlación, aunque su expresión es un poco aparatosa:\n\\[ f(r \\mid \\rho =0) = \\frac{ \\Gamma \\left [\\frac{1}{2} (n-1) \\right ]} { \\Gamma \\left [\\frac{1}{2} (n-2) \\right ] \\sqrt{\\pi}\n} (1-r^2)^{\\frac{1}{2}(n-4)} \\]\nAclarar la notación Esta función no converge a la distribución Normal cuando n se hace grande, ya que está definida solo entre -1 y 1 mientras que el dominio de la distribución Normal no está acotado."
  },
  {
    "objectID": "variablesAleatorias.html",
    "href": "variablesAleatorias.html",
    "title": "3  Variables aleatorias",
    "section": "",
    "text": "Importancia, tipos, propiedades."
  },
  {
    "objectID": "distribucionesProbabilidad.html",
    "href": "distribucionesProbabilidad.html",
    "title": "4  Distribuciones de probabilidad",
    "section": "",
    "text": "Binomial, Poisson, Normal, otras (uniforme, equiprobable, curiosidades: unif+unif = triangular)"
  },
  {
    "objectID": "Proporciones.html",
    "href": "Proporciones.html",
    "title": "5  Estadística descriptiva",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "tamañoPoblacion.html",
    "href": "tamañoPoblacion.html",
    "title": "6  Estadística descriptiva",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Parte_Variabilidad.html",
    "href": "Parte_Variabilidad.html",
    "title": "SOBRE LA VARIABILIDAD",
    "section": "",
    "text": "Importancia de saberse mover en un contexto de variabilidad. No todas las variabilidades son iguales.\nEstoy pensando en empezar planteando el tema de la fecha de nacimiento de los futbolistas profesionales y a partir de ahí introducir la distribución binomial y su utilidad para separar señal y ruido.\nSiguiendo con el mundo de futbol, podría introducir la distribución de Poisson con el ejemplo del número de goles por partido.\nLa distribución Normal se puede introducir como límite de histograma. También partiendo de la distribución Normal.\nNo creo que -al menos aquí- haya que hablar de distribuciones de referencia teóricas: t-Student, Chi-cuadrado, F de Snededor."
  },
  {
    "objectID": "Parte_Estimacion.html",
    "href": "Parte_Estimacion.html",
    "title": "ESTIMACIÓN",
    "section": "",
    "text": "Lo típico. Como estimar las características de una población a partir de los datos de una muestra.\nPropiedades que deseamos tener en los estimadores\nEstimación de proporciones. Intervalo de confianza, margen de error, etc.\nEstimación de medias\nEstimación del tamaño de una población (peces y taxis)\nAnexo: Sondeos electorales."
  },
  {
    "objectID": "Parte_Experimentos.html",
    "href": "Parte_Experimentos.html",
    "title": "DISEÑO DE EXPERIMENTOS",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "floresAspirina.html",
    "href": "floresAspirina.html",
    "title": "10  Estadística descriptiva",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "duracionPilas.html",
    "href": "duracionPilas.html",
    "title": "11  Regresión",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "sintesisNumerica.html#medidas-de-tendencia-central",
    "href": "sintesisNumerica.html#medidas-de-tendencia-central",
    "title": "1  Síntesis numérica de datos",
    "section": "1.1 Medidas de tendencia central",
    "text": "1.1 Medidas de tendencia central\n\nMedia aritmética\nTanto el cálculo como el significado de la media aritmética son bien conocidos por los estudiantes. Seguramente ayuda su utilidad para saber si se aprobará una asignatura a la vista de los resultados obtenidos durante el curso.\nCuando los datos se presentan tabulados hay que tener presente que estos se repiten tantas veces como indica su frecuencia absoluta. Por ejemplo, en la Tabla 1.1, el número de hijos (X) es la variable considerada y el número de familias (n) que tienen ese número de hijos es su frecuencia absoluta.\n\n\nTabla 1.1: Número de hijos por familia\n\n\nNúm de hijos (X):\n0\n1\n2\n3\n4\n5\n\n\nNúm de familias (n):\n13\n21\n15\n8\n1\n2\n\n\n\n\nEl valor medio (\\(\\bar{X}\\)) del número de hijos por famila es igual a:\n\\[\n\\bar{X} = \\frac{\\sum_{i} n_i X_i }{N} \\:  = \\:  1,48\n\\] Donde \\(N\\) es el número total de datos disponibles (\\(N=60\\)) e \\(i\\) es el índice que en nuestro caso va de 1 a 6 (para valores de \\(X\\) de 0 a 5).\nPara cada valor de X (X_i), su frecuencia relativa es igual a \\(n_i/N\\). También podemos calcular la media usando la expresión:\n\\[\n\\sum_{i} f_i X_i \\:  = \\:  1,48\n\\] Esta expresión es análoga a la de la esperanza matemática para una variable aleatoria (cambiando frecuencia relativa por probabilidad) que veremos más adelante.\n=====\nNotación: Media, promedio, esperanza matemática pros y contras\n=====\n\n\nMediana\nEn una empresa con 500 trabajadores y la siguiente distribución de salario mensual:\n\n\nTabla 1.2: Distribución de salarios\n\n\n\n\n\n\n\n\n\n\n\n\nSalario mensual (€):\n1134\n1200\n2500\n3500\n5000\n7000\n10000\n\n\nNúm de trabajadores\n240\n97\n60\n50\n43\n8\n2\n\n\n\n\ntexto\n\n\nModa\ntexto"
  },
  {
    "objectID": "sintesisNumerica.html#medidas-de-dispersión",
    "href": "sintesisNumerica.html#medidas-de-dispersión",
    "title": "1  Síntesis numérica de datos",
    "section": "1.2 Medidas de dispersión",
    "text": "1.2 Medidas de dispersión\ntexto sobre la importancia de la dispersión y de medirla para valorarla\n\nRango\ntexto\n\n\nVarianza\ntexto\n\n\nDesviación típica\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto"
  },
  {
    "objectID": "sintesisNumerica.html#medidas-de-posición",
    "href": "sintesisNumerica.html#medidas-de-posición",
    "title": "1  Síntesis numérica de datos",
    "section": "1.3 Medidas de posición",
    "text": "1.3 Medidas de posición\ntexto\ntexto\n\nCuartilas\ntexto\ntexto\n\n\nDeciles. Percentiles\ntexto\ntexto"
  },
  {
    "objectID": "sintesisNumerica.html#porcentajes",
    "href": "sintesisNumerica.html#porcentajes",
    "title": "1  Síntesis numérica de datos",
    "section": "1.4 Porcentajes",
    "text": "1.4 Porcentajes\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\n¿Qué hace esto aquí?"
  },
  {
    "objectID": "conceptosGenerales.html",
    "href": "conceptosGenerales.html",
    "title": "1  Conceptos generales",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regresion.html#cálculo-de-los-coeficientes",
    "href": "regresion.html#cálculo-de-los-coeficientes",
    "title": "11  Regresión",
    "section": "11.2 Cálculo de los coeficientes",
    "text": "11.2 Cálculo de los coeficientes\nSe realiza con el objetivo de minimizar la suma de los cuadrados de los residuos. Pero existen otros métodos. Veamos algunos.\n\nA ojo\nSe traza la recta directamente sobre el papel o se identifican dos puntos de paso y a partir de ellos se calculan los coeficientes del modelo.\n\n\n\n\n\n\n\nPROS \n\nIntuitivo. Muy fácil de entender\nNo se comenten errores de mucho bulto\n\n\n\nCONS \n\nNo se logra el ajuste “perfecto” de acuerdo con el criterio establecido\nNo se tienen medidas de calidad del ajuste ni de significación de los coeficientes\nSolo sirve para regresión simple\n\n\n\n\nA pesar de sus evidentes limitaciones, si solo se trata de tener la recta no es un método tan malo como parece. Con un poco de práctica el ajuste no será muy distinto del “perfecto” y no se cometeran errores de mucho bulto debido a la presencia de valroes anómalos, cosa que sí puede ocurrir si se tratan los datos de forma automática sin mnirarlos.\n\n\nMétodo de Ishikawa\n\n\n\n\n\n\n\nPROS \n\nFácil de entender\nRobusto frente a la presencia de valores anómalos o con excesiva influencia\n\n\n\nCONS \n\nNo se tienen medidas de calidad del ajuste\nSolo sirve para regresión simple\n\n\n\n\n\n\nMinimizando la suma de los residuos\nEntendemos que se trata de minimizar la suma en valor absoluto, ya que un valor muy grande con signo negativo se logra simplemente aumentando los valores de \\(b_0\\) y de \\(b_1\\). Por tanto, se trata de minimizar \\(\\sum [\\, Y_i - (b_0 - b_1 X_i)]\\,\\). haciendo esta expresión igual a cero (mínimo valor posible), tenemos:\n\\[ n\\bar{Y} - nb_0 - b_1 n \\bar{X} = 0\\] Por tanto, con cualquier par de valores \\(b_0\\) y \\(b_1\\) que verifiquen la expresión \\(\\bar{Y} = b_0 + b_1 \\bar{X}\\) tendremos una suma de residuos en valor absoluto igual a cero.\nQue haya infinitas rectas que cumplan esa condición ya es mala señal, porque seguro que no todas son la más adecuada. Para los valores representados en la figura X tenemos que \\(\\bar{X}= 3\\frac{1}{3}\\) y \\(\\bar{Y}= 8\\frac{2}{3}\\). Rectas que cumplen la condicion de minimizar la suma de los residuos son, por ejemplo, la que tiene coeficientes \\(b_0=8\\frac{2}{3}\\) y \\(b_1=0\\), es decir: \\(Y = 8\\frac{2}{3}\\), o también \\(b_0 = 12\\) y \\(b_1 = -1\\)\nAQUÍ GRÁFICO\n\n\nMinimizando la suma de los residuos en valor absoluto\n\n\n\nMinizando la suma de los cuadrados de los residuos\n\n\nCon fuerza bruta\nCuriosidad con el redondeo\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regresion.html#definiciones",
    "href": "regresion.html#definiciones",
    "title": "11  Regresión",
    "section": "11.1 Definiciones",
    "text": "11.1 Definiciones\nResiduos\nNormalidad de la respuesta\nEstimación de los parámetros\nModelo, no ecuación"
  },
  {
    "objectID": "regresion.html#determinación-de-la-recta-ajustada",
    "href": "regresion.html#determinación-de-la-recta-ajustada",
    "title": "11  Regresión",
    "section": "11.2 Determinación de la recta ajustada",
    "text": "11.2 Determinación de la recta ajustada\nSe realiza con el objetivo de minimizar la suma de los cuadrados de los residuos. Pero existen otros métodos. Veamos algunos.\n\nA ojo\nSe traza la recta directamente sobre el papel o se identifican dos puntos de paso y a partir de ellos se calculan los coeficientes del modelo.\nA pesar de sus evidentes limitaciones, si solo se trata de tener la recta no es un método tan malo como parece. Con un poco de práctica el ajuste no será muy distinto del “perfecto” y no se cometeran errores de bulto debido a la presencia de valores anómalos, cosa que sí puede ocurrir si se tratan los datos de forma automática sin mnirarlos.\n\n\n\nFigura 11.1: Ajuste a ojo y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\n\n\n\n\n\nPROS \n\nIntuitivo. Muy fácil de entender\nNo se comenten errores de mucho bulto\n\n\n\nCONS \n\nNo se logra el ajuste “perfecto” de acuerdo con el criterio establecido\nNo se tienen medidas de calidad del ajuste ni de significación de los coeficientes\nSolo sirve para regresión simple\n\n\n\n\n\n\nMétodo de Ishikawa\nAquí texto,\n\n\n\nFigura 11.2: Ajuste a ojo y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 11.1: Método de Ishikawa. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nFácil de entender\nRobusto frente a la presencia de valores anómalos o con excesiva influencia\n\n\n\nCONS \n\nNo se tienen medidas de calidad del ajuste\nSolo sirve para regresión simple\n\n\n\n\n\n\n\nMinimizando la suma de los residuos\nEntendemos que se trata de minimizar la suma en valor absoluto, ya que un valor muy grande con signo negativo se logra simplemente aumentando los valores de \\(b_0\\) y/o de \\(b_1\\). Por tanto, se trata de minimizar \\(|\\sum(Y_i - (b_0 - b_1 X_i))|\\). Haciendo esta expresión igual a cero (mínimo valor posible), tenemos:\n\\[ n\\bar{Y} - nb_0 - b_1 n \\bar{X} = 0\\] Por tanto, con cualquier par de valores \\(b_0\\) y \\(b_1\\) que verifiquen la expresión \\(\\bar{Y} = b_0 + b_1 \\bar{X}\\), es decir, con cualquier recta que pase por (\\(X_0\\), \\(Y_0\\)) tendremos una suma de residuos en valor absoluto igual a cero.\nQue haya infinitas rectas que cumplan esa condición ya es mala señal, porque seguro que no todas son adecuadas. Para los valores representados en la figura X tenemos que \\(\\bar{X}= 6\\) y \\(\\bar{Y}= 9\\). Rectas que cumplen la condicion de minimizar la suma de los residuos son, por ejemplo, la que tiene coeficientes \\(b_0=9\\) y \\(b_1=0\\), es decir: \\(Y = 9\\), o también \\(b_0 = 12\\) y \\(b_1 = -0.5\\), es decir: \\(Y = 12 -0.5X\\).\n\n\n\nFigura 11.3: Dos ajustes -claramente muy malos- con suma de residuos igual a cero.\n\n\n\n\nTabla 11.2: Minimizar la suma de los residups. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nNinguna\n\n\n\nCONS \n\nDa un número infinito de soluciones (una de ellas coincide con el ajuste por mínimos cuadrados)\n\n\n\n\n\n\n\nMinimizando la suma de los residuos en valor absoluto\nDe entrada parece bastante más razonable que el anterior. Puede no tener solución única, pero los resultados que da no son disparados como en el caso anterior *referencia a figura*. Tiene solución única pero no existen expresiones para los coeficientes debido a las dificultades en el manejo de la función “valor absoluto”.\n\n\n\nDos ajustes -claramente muy malos- con suma de residuos igual a cero.\n\n\nMas información: Wikipedia\n\n\nMinizando la suma de los cuadrados de los residuos\naquí texto\n\n\n\nDos ajustes -claramente muy malos- con suma de residuos igual a cero."
  },
  {
    "objectID": "regresion.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "href": "regresion.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "title": "11  Regresión",
    "section": "11.3 Mínimos cuadrados. Cálculo de los coeficientes",
    "text": "11.3 Mínimos cuadrados. Cálculo de los coeficientes\nAquí texto\n\nCon fuerza bruta\n\n\nUsando las expresiones de \\(b_0\\) y \\(b_1\\) que minimizan la suma de los cuadrados de los residuos\nCuriosidad con el redondeo\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regresionSimple.html#definiciones",
    "href": "regresionSimple.html#definiciones",
    "title": "10  Regresión Simple",
    "section": "10.1 Definiciones",
    "text": "10.1 Definiciones\nResiduos\nIntercepto\nModelo, no ecuación\nRegresión simple vs. múltiple"
  },
  {
    "objectID": "regresionSimple.html#determinación-de-la-recta-ajustada",
    "href": "regresionSimple.html#determinación-de-la-recta-ajustada",
    "title": "10  Regresión Simple",
    "section": "10.1 Determinación de la recta ajustada",
    "text": "10.1 Determinación de la recta ajustada\nSi entre la respuesta y la variable regresora se observa una relación lineal se determina la ecuación de la recta que mejor se adapta a los puntos disponibles. Lo que significa “mejor” es discutible. Veamos algunas formas de hacerlo.\n\nA ojo\nSe traza la recta directamente sobre el papel o se identifican dos puntos de paso y a partir de ellos se calculan los coeficientes del modelo.\nA pesar de sus evidentes limitaciones, si solo se trata de tener la recta no es un método tan malo como parece. Con un poco de práctica el ajuste no será muy distinto del “perfecto” y no se cometeran errores de bulto debido a la presencia de valores anómalos, cosa que sí puede ocurrir si se tratan los datos de forma automática sin mnirarlos.\n\n\n\nFigura 10.2: Ajuste a ojo y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\n\n\n\n\n\nPROS \n\nIntuitivo. Muy fácil de entender.\nNo se comenten errores de mucho bulto.\n\n\n\nCONS \n\nNo se logra el ajuste “perfecto” de acuerdo con el criterio establecido. |\nNo se tienen medidas de calidad del ajuste. |\nSolo sirve para regresión simple.\n\n\n\n\n\n\nMétodo de Ishikawa\nSe identifica el primer y el tercer cuartil de los valores de \\(X\\) \\((X_{Q1}\\) y \\(X_{Q3})\\), e igual para los valores de \\(Y\\) \\((Y_{Q1}\\) y \\(Y_{Q3})\\). Se traza la recta por los puntos \\((X_{Q1}\\) y \\(Y_{Q1})\\) y \\((X_{Q3}\\) y \\(Y_{Q3})\\). Se obtiene una recta muy razonable sin necesidad de realizar cálculos ni de aplicar fórmulas de las que se desconoce su lógica.\n\n\n\nFigura 10.3: Ajuste por el método de Ishikawa y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 10.1: Método de Ishikawa. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nFácil de entender\nRobusto frente a la presencia de valores anómalos o con excesiva influencia\n\n\n\nCONS \n\nNo se tienen medidas de calidad del ajuste\nSolo sirve para regresión simple\n\n\n\n\n\n\n\n\n\n\n\nKaoru Ishikawa (1915-1989)\n\n\n\nFue un ingeniero japonés, considerado uno de los artífices del llamado “milagro japonés” que condujo los productos japoneses desde la mediocridad hasta arrasar en los mercados mundiales (electrónica, fotografía, automoción,…). Una de las claves del éxito fue el uso intensivo de técnicas estadísticas para el control y la mejora de la calidad. Ishikawa es conocido por proponer el uso de herramientas sencillas, que todos puedan entender y aplicar de forma habitual.\n\n\n\n\nHaciendo que la suma de los residuos sea igual a cero\nSe trata de obtener los valores de \\(b_0\\) y \\(b_1\\) que cumplen la expresión:\n\\[\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]= 0\\] qye es equivalente a:\n\\[ n\\bar{Y} - nb_0 - b_1 n \\bar{X} = 0\\] Por tanto, con cualquier par de valores \\(b_0\\) y \\(b_1\\) que verifiquen la expresión \\(\\bar{Y} = b_0 + b_1 \\bar{X}\\), es decir, con cualquier recta que pase por (\\(\\bar{X}\\), \\(\\bar{Y}\\)) tendremos una suma de residuos igual a cero.\nQue haya infinitas rectas que cumplan esa condición es una mala señal, porque seguro que no todas son adecuadas. Para los valores representados en la figura 10.4 tenemos que \\(\\bar{X}= 6\\) y \\(\\bar{Y}= 9\\). Rectas que hacen que la suma de los residuos sea igual a cero son, por ejemplo, la que tiene coeficientes \\(b_0=9\\) y \\(b_1=0\\), es decir: \\(Y = 9\\), o también \\(b_0 = 12\\) y \\(b_1 = -0.5\\), es decir: \\(Y = 12 -0.5X\\) y ambos son claramente muy malos ajustes.\n\n\n\nFigura 10.4: Dos ajustes -claramente muy malos- con suma de residuos igual a cero.\n\n\n\n\nTabla 10.2: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nNinguna\n\n\n\nCONS \n\nDa un número infinito de soluciones (una de ellas coincide con el ajuste por mínimos cuadrados)\n\n\n\n\n\n\n\nMinimizando la suma del valor absoluto de los residuos\nSe trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left| Y_i - (b_0 - b_1 X_i) \\right|= 0\\]\nPuede no tener solución única, pero los resultados posibles son mucho más razonables que en el caso anterior. Un problema específico de este caso es que no existen expresiones analíticas para los coeficientes debido a las dificultades en el manejo de la función “valor absoluto”.\nLa figura 10.5 muestra dos diagramas con los mismos 4 puntos y ñas rectas que cumplen el criterio estableciso, en todas ellas la suma del valor absoluto de los residuos es igual a 2. La línea azul, que es la misma en los dos diagramas, es la que también minimiza la suma de los cuadrados de los residuos.\n\n\n\nFigura 10.5: Posibles opciones para minimizar la suma de los residuos en valor absoluto.\n\n\n\n\nTabla 10.3: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nPoco sensible a la presencia de valores anómalos\n\n\n\nCONS \n\nNo da un ajuste equilibrado entre todos los puntos\nNo existe expresión analítica para el cálculo de los coeficientes\n\n\n\n\n\n\n\n\nMinimizando la suma de los cuadrados de los residuos\nElevamos los residuos al cuadrado en vez de usar su valor absoluto jpara evitar que al realizar la suma se compensen los positivos y negativos.\nPor tanto, ahora se trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]^2= 0\\] Dedicamos el próximo apartado al cálculo de los coeficientes \\(b_0\\) y \\(b_1\\). En este caso la solución es única y existen soluciones analíticas.\nEn vez de decir que el criterio de ajuste ha sido “minimizar la suma de los cuadrados de los residuos”, decimos que hemos ajustado por “mínimos cuadrados”. Este es el método de ajuste utilizado en la inmensa mayoría de los casos, produce un ajsute equilibrado entre todos los puntos y tienen importantes propiedades. Medidas como la dfesviación típica, o técnicas de análisis de datos como el análisis de la varianza están basados en un criterio similar. Un inconveniente es que el modelo obtenido es bastante sensible a los valores anómalos, cosa que no ocurre si se minimiza la suma de valores absolutos.\nEn la figura 10.6 (primera fila) tenemos una situación típica en que el ajuste por mínimo cuadrados es claramente mejor que minimizando la suma del valor absoluto de los residuos, pero en la segunda fila vemos un caso en que hay un valor que no sigue el patrón de comportamiento general. Este punto, probablemente un error, afecta de forma notable al ajuste por mínimos cuadrados pero no lo hace si se usa el valor absoluto.\n\n\n\nFigura 10.6: **** y ajuste minimizando la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 10.4: Minimizar la suma de los cuadrados de los residuos. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nNinguna\n\n\n\nCONS \n\nDa un número infinito de soluciones (una de ellas coincide con el ajuste por mínimos cuadrados)"
  },
  {
    "objectID": "regresionSimple.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "href": "regresionSimple.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "title": "13  Regresión Simple",
    "section": "13.2 Mínimos cuadrados. Cálculo de los coeficientes",
    "text": "13.2 Mínimos cuadrados. Cálculo de los coeficientes\nExiste una fórmula cerrada y con solución única para cada coeficiente, pero vamos a empezar identificando el valor de los coeficientes sin hacer uso de las fórmulas. Naturalmente, es mucho más rápido y más práctico usarlas o -mejor todavía- usar un paquete de software estadístico o una hoja de cálculo, pero hacerlo sin fórmulas permite entender perfectamente qué es lo que se está haciendo, y también descubrir algún detalle interesante.\n\nSin fórmulas\nRealizamos a ojo una primera estimación del valor de los coeficientes. A continuación, mediante un pequeño programa -o también usando una hoja de cálculo-, hacemos un barrido de los valores de \\(b_0\\) y \\(b_1\\) en torno a los estimados, identidicando el par que minimiza la suma de los cuadrados de los residuos.\nVayamos al diagrama de la figura 13.7 (izquierda) que ya habíamos visto en las figuras 13.2 y 13.3. La recta ajustada a ojo pasa por los puntos (-4,75; 0) y (5,75, 60) por lo que sus coeficientes son: \\(b_1\\) = 5,71 y \\(b_0\\) = 27,14. Sería mucha casualidad que esos fueran los valores exactos que estamos buscando, pero no andarán muy lejos. Vamos a crear una malla de valores de \\(b_0\\) y \\(b_1\\). Los valores de \\(b_0\\) variarán de 2 a 8 con incrementos de 0,1 y para cada uno de esos, los de \\(b_0\\) irán de 20 a 35 también en saltos de 0,1. A cada combinación de esos dos valores corresponde a una recta, y a cada recta una suma de los cuadrados de los residuos. El par de valores que minimizan esa suma de cuadrados son: \\(b_0\\) = 27,0 y \\(b_1\\) = 4,8.\n\n\n\nFigura 13.7: Recta ajustada a ojo (izq.) y suma de los cuadrados de los residuos para cada par de valores \\(b_0\\) y \\(b_1\\). En rojo, valores que la minimizan (der.).\n\n\n\n\n\n\n\n\nParaboloide de la suma de cuadrados\n\n\n\nCon los datos de nuestro ejemplo, la superficie que representa la suma de los cuadrados de los residuos es un paraboloide donde la localización del mínimo es visulamente muy clara. Pero lo nomal es que las curvas de nivel sean muy elípticas de manera que la representación no queda tan clara. Nosotros hemos logrado esa forma regular haciendo que la media de los valores de \\(X\\) sea igual a cero. De esta forma, los coeficientes son independientes y las curvas de nivel apararecen como círculos prácticamente concéntricos quedando más clara la idea que queremos representar.\n\n\n\n\nUsando las fórmulas\nEn el diagrama que representa la relación entre \\(X\\) e \\(Y\\) cada punto puede ser identificado por sus coordenadas \\((x_i, y_i)\\) con \\(1 \\leq i \\leq n\\) siendo \\(n\\) el número total de puntos. [creo que esto es redundante y habría que mejorarlo]\nCada uno de los puntos tiene un residuo asociado \\(e_i\\) y ese residuo es la diferencia entre el valor real de \\(y\\), es decir, \\(y_i\\) y su valor estimado \\(\\hat{y}_i\\), el que estará sobre la recta y que será igual a \\(b_0 + b_1 x_i\\). Por tanto, el valor del residuo asociado al punto \\(i\\) lo podemos escribir de la forma:\n\\[ e_i = y_i - \\left( b_0 + b_1 x_i \\right) \\] Por tanto, la suma de los cuadrados de los residuos, \\(S\\), será:\n\\[ S = \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right )^2 \\]\nTanto los valores de \\(y_i\\) como los de \\(x_i\\) vienen dados. La suma de cuadrados \\(S\\) es función de los valores de \\(b_0\\) y de \\(b_1\\). Se trata de hallar los valores de \\(b_0\\) y de \\(b_1\\) que minimizan esa suma de cuadrados. El mínimo lo tendremos en el punto en que la derivada de \\(S \\left(b_0, b_1 \\right )\\) respecto a \\(b_0\\) y respecto a \\(b_1\\) es igual a cero. Seguro que es un mínimo porque el máximo no está definido.\n\\[ \\frac{\\partial S}{\\partial b_0} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) \\]\n\\[ \\frac{\\partial S}{\\partial b_1} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) x_i \\]\nIgualando a cero estas expresiones:\n\\[ \\sum_{í=1}^n y_i - nb_0 - b_1 \\sum_{í=1}^n x_i = 0  \\tag{13.1}\\]\n\\[ \\sum_{í=1}^n x_i y_i - b_0 \\sum_{í=1}^n x_i - b_1 \\sum_{í=1}^n  x_i^2  = 0  \\tag{13.2}\\]\nDividiendo por \\(n\\) todos los términos de la ecuación 13.1 tenemos:\n\\[ b_0 = \\bar{y} - b_1 \\bar{x} \\]\n\n\n\n\n\n\nLa recta ajustada pasa por el punto \\((\\bar{x}, \\bar{y})\\)\n\n\n\nDe la anterior expresón para \\(b_0\\) también se decude que \\(\\bar{y} = b_0 + b_1\\bar{x}\\). Es decir, la recta ajustada minimizando la suma de los cuadrados de los residuos siempre pasa por el punto \\((\\bar{x}, \\bar{y})\\) .\n\n\nSustituyendo la expresión de \\(b_0\\) en la ecuación 13.2 tenemos:\n\\[ \\sum_{í=1}^n x_i y_i - \\bar{y} \\sum_{í=1}^n x_i +  b_1\\bar{x} \\sum_{í=1}^n x_i- b_1 \\sum_{í=1}^n  x_i^2  = 0 \\]\nPara aligerar la notación no pondremos los límites a los sumatorios, que siempre son desde \\(i=1\\) hasta \\(n\\). Despejando \\(b_1\\) llegamos a:\n\\[ b_1 = \\frac{\\sum x_i y_i - \\bar{y} \\sum x_i}{\\sum x_i^2 - \\bar{x} \\sum x_i} \\] También la expresión de \\(b_1\\) se suele dar de la forma (ver Apéndice 10.1):\n\\[ b_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}  \\tag{13.3}\\]\nA partir de la ecuación 13.3 y recordando las expresiones de la covarianza y del coeficiente de correlación, llegamos a una expresión que también se ve con frecuencia en los libros de texto, seguramente porque una calculadora sencilla da directamente los tres valores que intervienen:\n\\[ b_1 = \\frac{Cov(XY)}{s_X^2} = \\frac{r_{XY} s_X s_Y}{s_X^2} = r_{XY} \\frac{s_Y}{s_X} \\] Calculando los coeficientes que corresponden a los datos de la figura 13.7 se obtiene:\n\\[ b_0 = 26,9615 \\qquad  \\qquad b_1 = 4,8616 \\] Ahora sí, con todos los decimales que queramos, aunque dar más decimales de los que tienen los datos es añadir números que no aportant ninguna información, dan una falsa sensación de precisión y complican la lectura del resultado.\n\n\n\n\n\n\n\nPor qué le llamamos modelo de regresión\n\n\n\nSean, por ejemplo, los puntos: (4; 3), (6; 8), (8; 12), (10; 10), (12; 12). La recta ajustada es: \\(y = 1+x\\). Si en vez de ajustar \\(y = f(x)\\) se ajusta \\(x=f(y)\\) ¿se obtendrá la ecuación resultante de despejar \\(x\\) en \\(y=f(x)\\), es decir: \\(x = -1 + y\\)? Si ajustamos \\(x=f(y)\\) la ecuación será: \\(x=1,57+0,714x\\). No es lo mismo minimizar la suma de los cuadrados de los residuos medidos en dirección vertical que en dirección horizontal (esto último no son los residuos)."
  },
  {
    "objectID": "regresionMultiple.html#necesidad-de-una-medida-de-relación",
    "href": "regresionMultiple.html#necesidad-de-una-medida-de-relación",
    "title": "11  Regresión Múltiple",
    "section": "11.1 Necesidad de una medida de relación",
    "text": "11.1 Necesidad de una medida de relación\nSee Knuth (1984) for additional discussion of literate programming."
  },
  {
    "objectID": "regresionMultiple.html#covarianza",
    "href": "regresionMultiple.html#covarianza",
    "title": "11  Regresión Múltiple",
    "section": "11.2 Covarianza",
    "text": "11.2 Covarianza\nCon mucho protagonismo en la explicación del comportamiento de variables aleatorias a nivel teórico, pero prácticamente no se usa para cuantificar la relación de dos variables a partir de dos conjuntos de datos.\n\n11.2.1 Deducción de la fórmula\nAquí deducción de la fórmula\n\n\n11.2.2 Propiedades\nAquí propiedades"
  },
  {
    "objectID": "regresionMultiple.html#coeficiente-de-correlación",
    "href": "regresionMultiple.html#coeficiente-de-correlación",
    "title": "11  Regresión Múltiple",
    "section": "11.3 Coeficiente de correlación",
    "text": "11.3 Coeficiente de correlación\nEste sí se usa.\n\n11.3.1 De la covarianza al coeficiente de correlación\nDividir por el producto de las desviaciones típicas.\n\n\n11.3.2 Propiedades del coeficiente de correlación\nAcotado entre 0 y 1.\n\n\n11.3.3 Correlación no implica relación causa-efecto\n\n\n11.3.4 Correlación estadísticamente significativa\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regresionSimple.html#medida-de-calidad-del-ajuste",
    "href": "regresionSimple.html#medida-de-calidad-del-ajuste",
    "title": "11  Regresión Simple",
    "section": "11.4 Medida de calidad del ajuste",
    "text": "11.4 Medida de calidad del ajuste\nR2"
  },
  {
    "objectID": "regresionSimple.html#lo-que-tenemos-es-una-muestra",
    "href": "regresionSimple.html#lo-que-tenemos-es-una-muestra",
    "title": "11  Regresión Simple",
    "section": "11.5 Lo que tenemos es una muestra",
    "text": "11.5 Lo que tenemos es una muestra\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regresionSimple.html#las-cosas-se-complican-lo-que-tenemos-es-una-muestra",
    "href": "regresionSimple.html#las-cosas-se-complican-lo-que-tenemos-es-una-muestra",
    "title": "13  Regresión Simple",
    "section": "13.6 Las cosas se complican: Lo que tenemos es una muestra",
    "text": "13.6 Las cosas se complican: Lo que tenemos es una muestra\nLa interpretación de los resultados se complica cuando caemos en la cuenta de que los datos disponibles son solo una muestra de la población de interés. Supongamos que deseamos estudiar la relación entre peso y estatura en los jóvenes de cierta edad (haríamos bien en separar hombres y mujeres, pero aquí vamos a ignorar ese aspecto que trataremos en el siguiente capítulo) y que disponemos de una muestra de -pongamos- 20 jóvenes. Con los datos de esa muestra ajustamos una recta pero, en realidad, esa no es la recta que andamos buscando. Si hubiéramos tomado otra muestra la recta sería otra -distitnta- pero tan válida como la primera. Entonces, ¿cómo se interpreta la recta obtenida?\nComo en otros casos, una forma de ver lo que ocurre es simulando. En los diagramas de la figura 13.14 las estaturas (\\(X\\)) se han generado aleatoriamente de una distribución N(170; 8) y a cada estatura se le ha asignado un peso (\\(Y\\)) mediante la expresión \\(Y = X -100 +e\\), donde \\(e\\) un valor también generado aleatoriamente de una distribución N(0; 5). Tanto los valores de la estatura (en cm) como los obtenidos para los pesos (en kg) son valores razonables para una población joven. Hemos repetido la simulación 6 veces y, como es natural, cada vez hemos obtenido unos datos distintos y, por tanto, también una recta ajustada distinta.\n\n\n\nFigura 13.14: Rectas ajustadas a partir de muestras de n=20 datos de una misma población. La línea negra representa el modelo teórico\n\n\nEn la figura 13.15 (izq.) se han superpuesto los 6 diagramas anteriores pudiéndose observar el haz de rectas que se obtiene. A la derecha tenemos la misma situación superponiendo 50 simulaciones (cada una con 20 datos) añadiendo, de color verde, la recta que representa el modelo teórico, es decir, la población.\n\n\n\nFigura 13.15: Superposición de 6 (izq.) y 50 (der.) simulaciones de conjuntos de 20 datos del modelo representado con una línea verde en el gráfico de la derecha.\n\n\n\nDistribución de los coeficientes\nLa buena noticia es que si los datos cumplen unas ciertas condiciones –que en general se cumplirán– los valores de los coeficientes pertenecen a distribuciones Normales con parámetros conocidos. Siguiendo con el ejemplo anterior hemos repetido 10.000 veces la simulación obteniendo otras tantas rectas ajustadas. La 13.16 muestra los histogramas de los 10.000 valores obtenidos para \\(b_0\\) y \\(b_1\\).\n\n\n\nFigura 13.16: Distribución de los coeficientes.\n\n\nObserve que las medias de las distribuciones coinciden con verdadero valor del parámetro estimado (estamos de suerte). Las desviaciones típicas dependen de:\n\nNúmero de datos: Cuanto más datos mayor información y menos incertidumbre, por tanto, menos desviación típica en la distribución de los coeficientes.\nDesviación típica de la respuesta: A mayor variabilidad de la respuesta mayor incertidumbre y mayor variabilidad en la distribución del los coeficientes.\nEl rango de variación de los valores de la variable regresora: Si los valores de \\(x\\) están muy próximos a su media habrá mayor variabilidad en los distribución de los coeficientes. Quizá este aspecto no es tan intuitivo como los anteriores, pero se entiende muy bien a la vista de un gráfico como el de la figura 13.16. En la izquierda tenemos el mismo gráfico que en la figura 13.15 con valores de X generados de una distribución N(170; 8) mientras que en el de la derecha se ha construido de la misma forma pero los valores de X se han generadod de una N(170; 3). Al tener menos variabilidad los valores de X tenemos mayor variabilidad en los valroes de los coeficientes.\n\n\n\n\nFigura 13.17: Distribución de los coeficientes.\n\n\nConocer la distribución de los coeficientes hace posible calcular intervalos de confianza o realizar constrastes de hipótesis sobre sus valores.\n\n\nCondiciones que deben reunir los datos\nPara que los coeficientes tengan las distribuciones descritas anteriormente los datos utilizados para ajustar el modelo deben cumplir las siguientes condiciones:\n\nDistribución de \\(Y\\): Dado un valor de \\(X\\), los valores de \\(Y\\) deben seguir una distribución Normal. Si \\(X\\) es la estatura e \\(Y\\) es el peso, no hace falta suponer que el peso –globalmente– sigue un distribución Normal, pero sí que los pesos para las personas de una determinada estatura siguen esa distribución (figura 13.18).\n\n\n\n\nFigura 13.18: Los valores del peso (en general, \\(Y\\)) siguen una distribución Normal para cada valor de la estatura (en general, \\(X\\)). Observe que hay una mayor densidad de puntos –observaciones– en torno a la línea verde que representa el modelo teórico.\n\n\n\nVariabilidad de \\(Y\\): Supondremos que la variabilidad de \\(Y\\) no depende del valor de \\(X\\). En nuestro ejemplo sería suponer que la variabilidad en el peso de las personas que miden 1,60 es la misma que en las personas que miden 1,80 m. Es decir, las dos “campanas” que aparecen en la figura 13.18, y las correspondientes a cualquiera de las estaturas, tienen la misma anchura. Es posible que esto no sea exactamente así porque es habitual que cuando aumenta el nivel de la respuesta aumente también su variabilidad. Si esto ocurre lo veremos en el diagrama bivariante: la nube de puntos se irá ensanchando a medida que aumenta el valor de \\(X\\). En este caso quizá convenga transformar los datos, aunque ya estaríamos ante una situación más complicada que las que pretendemos tratar aquí.\nValores de \\(X\\): No hay ninguna exigencia especial sobre estos valores. Solo es necesario que la variable sea cuantitativa. Es decir, el día de la semana, codificado como: lunes = 1, martes = 2, … no puede ser una variable regresora porque el modelo entedería que el domingo es igual a 7 veces el lunes. Aun así, también hay formas de incluir este tipo de variables. Lo veremos en el próximo capítulo en el caso de que solo puedan tomar dos valoros posibles.\nIndependencia de los residuos: La desviación respecto al valor previsto (valor sobre la recta) en en un punto no da ninguna pista sobre la desviación en el punto siguiente. Esto no ocurre con las variables que evolucionan en el tiempo, como la temperatura o la cotización de acciones en la bolsa, en que el valor de un día está influenciado por el valor de día anterior.\n\nCuando se ajustan modelos de regresión simple, la observación del diagrama bivariante de \\(Y\\) frente a \\(X\\) ya permite valorar si es razonable suponer que se cumplen los supuestos requeridos. SI nada hace suponer lo contrario, supondremos que las condiciones se cumplen. Si no se cumplen exactamente (en realidad, nunca se cumpliran “exactamente”) los intervalos de confianza y las pruebas de significación en que estamos interesdos, seguirán siendo válidos a efectos prácticos.\n\n\nPruebas de significación e intervalos de confianza\nHemos visto que si se cumplen determinadas condiciones podemos considerar que los coeficientes del modelo son valores de una distribución Normal centrada en el verdadero valor del parámetro estimado (el coeficiente en la población) y desviación típica conocida.\nEsto nos permite realizar pruebas de significación para los coeficientes del modelo.\n\nContraste \\(H_0: \\beta_0=0\\) frente a \\(H_1: \\beta_0 \\neq 0\\). Observe que el contraste se realiza sobre el valor teórico, no sobre el valor obtenido que ya vemos cual es. Este contraste tiene interés cuando se tienen razones para pensar que la recta pasa por el origen y se desea verificar que los resultados obtenidos no están en contradicción con ese supuesto.\nContraste \\(H_0: \\beta_1=0\\) frente a \\(H_1: \\beta_1 \\neq 0\\). Este es el contraste que tiene más interés. Se trata de verificar que la pendiente de la recta es significativamente distinta de cero. Si no lo es, una recta horizontal es compatible con los da5tos por lo que 5no se puede descartar que la variable regrora no sirvwe para explicar el comportaminto de la respuersta.\n\nHemos comentado anteriormente que la desviación típica de los coeficientes depende de la varianza de los errores, es decir, de la variabilidaed que presentan los puntos en torno a la recta ajustada. La varianza de los errores no se conoce, se estima a través de la varianza de los residuos. Esto provoca que los valroes que tenemos de la desciaciín típica de los coeficientes sea también un valro estimado y, por tanto +++++\nTenemos que :\n\\[\\begin{equation}\n    \\begin{aligned}\n        b_0 &\\sim N \\left(\\beta_0; \\; \\sigma_{\\beta_0} \\right) \\\\[5pt]\n        b_1 &\\sim N \\left(\\beta_1; \\; \\sigma_{\\beta_0} \\right)\n    \\end{aligned}\n\\end{equation}\\]\nPor tanto, si contrastamos \\(H_0: \\beta_0=0\\) frente a \\(H_1: \\beta_0 \\neq 0\\) a partir del valor obtenido para \\(b_0\\) tendremos:\n\\[ \\frac{b_0-0}{\\sigma_{\\beta_0}} \\sim N (0; 1) \\] y analogamente para \\(b_1\\):\n\\[ \\frac{b_1-0}{\\sigma_{\\beta_1}} \\sim N (0; 1) \\]\nPero los valroes de \\(\\sigma_{\\beta_0}\\) y los de \\(\\sigma_{\\beta_1}\\) no se conocen exactamente, pero se pueden estimar a partir de los datos disponibles. En la estimación de la desviación típica de los coeficientes aparece la variaza de los residuos. Los residuos tienen dos restricciones por lo que la t-Student tiene \\(n-2\\) grados de libertad.\n-Pruebas de significación para \\(b_1\\)\nTiene especial interés contrastar la hipótesis nula de que la pendiente de la recta (\\(\\beta_1\\)) es igual a cero.\n-Intervalos de confianza para las predicciones.\n\nSalida paquete de software\n\n-IC para las observaciones.\n\n\n\n\n\n\nLigando las pruebas se significación para \\(r\\) y para \\(b_1\\)\n\n\n\nAquí texto"
  },
  {
    "objectID": "regresionSimple.html#calidad-del-ajuste",
    "href": "regresionSimple.html#calidad-del-ajuste",
    "title": "13  Regresión Simple",
    "section": "13.3 Calidad del ajuste",
    "text": "13.3 Calidad del ajuste\nEl gráfico de la izquierda de la figura 13.8 muestra la relación entre la longitud de la circunferencia (X) de los troncos de un determinado tipo de árbol y el volumen de madera (Y) que se puede obtener de ellos (Fuente: Wolfram_Data_Repository 2016). Se observa que a más circunferencia mayor volumen de madera, tal como era de esperar, y la ecuación de la recta ajustada es útil para estimar cuanta madera se obtendrá de un tronco de determinado diámetro. Sin embargo, el gráfico de la derecha se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables. En este caso el modelo ajustado no sirve absolutamente para nada. Pero los dos modelos tienen el mismo aspecto y solo a la vista del valor de sus coeficientes es imposible saber cual de los dos es útil.\n\n\n\nFigura 13.8: Relación muy clara y relación inexistent, situaciones que no pueden distinguirse solo a la vista del modelo ajustado.\n\n\nEs necesario, por tanto, completar el modelo con una medida que informe de la calidad del ajuste obtenido. Esa medida es el coeficiente de determinación \\(R^2\\).\nPara calcular el valor de \\(R^2\\) empezamos poniéndonos en el peor de los casos: suponemos que \\(X\\) e \\(Y\\) son independientes, es decir, que el valor de \\(X\\) no aporta ninguna información sobre el valor de \\(Y\\). En este caso, la recta que muestra la relación entre ambas variables es una recta horizontal: la estimación del valor de \\(Y\\) es siempre la misma, sin importar el valor de \\(X\\), y la mejor apuesta para ese valor de \\(Y\\) -a falta de cualquier otra información- es su valor medio \\(\\bar{y}\\). A la suma de los cuadrados de los residuos correspondientes a esa recta horizontal que pasa por \\(\\bar{y}\\) le llamamos \\(Q_Y\\).\nA continuación calculamos la suma de los cuadrados de los residuos correspondientes a nuestra recta ajustada (la que minimiza la suma de los cuadrados de los residuos) y le llamaremos \\(Q_R\\). Cuanto mejor sea el ajuste menor será el valor de \\(Q_R\\) y mayor la diferencia entre \\(Q_Y\\) y \\(Q_R\\).\nEl valor de \\(R^2\\) es igual a la proporción de \\(Q_Y\\) explicada por \\(X\\), es decir, la proporción en que disminuye \\(Q_Y\\) gracias a la introducción de \\(X\\) como variable explicativa, es decir:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} \\]\nVeamos este cálculo en un ejemplo con datos sencillos. En la figura 13.9 tenemos 5 puntos que podrían representar la relación entre el peso y la estatura de 5 individuos. Si, ignorando la información aportada por la estatura, siempre damos una estimación del peso igual a su valor medio, será como ajustar a una recta horizontal y tendremos una suma de los cuadrados de los residuos \\(Q_Y = 56\\). Sin embargo, si utilizamos la información que aporta la estatura y realizamos el ajuste minimizando la suma de los cuadrados de los residuos tenemos \\(Q_R = 16\\).\n\n\n\nFigura 13.9: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\nHemos reducido la suma de los cuadrados de los residuos de 56 a 16, por tanto:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} = \\frac{56 - 16}{56} = 0.7143 \\]\nNormalmente nos referimos a este valor como un porcentaje. En este caso sería el 71.43%. En los ejemplos de la figura 13.8 estos valores son del 93,5% (volumen de madera) y 1,5% (edad al morir).\n\n\n\n\n\n\n\\(R^2\\) es el cuadrado del coeficiente de correlación \\(r\\)\n\n\n\nEfectivamente, en el caso que estamos considerando de regresión simple, el coeficiente de determinación \\(R^2\\) es igual al cuadrado del coeficiente de correlación \\(r\\). Este último puede variar entre \\(-1\\) y \\(1\\) por lo que, obviamente, \\(R^2\\) varía entre 0 y 1. La demostración es corta y fácil de encontrar en internet. Por ejemplo en:"
  },
  {
    "objectID": "regresionSimple.html#relación-polinómica-entre-x-e-y",
    "href": "regresionSimple.html#relación-polinómica-entre-x-e-y",
    "title": "11  Regresión Simple",
    "section": "11.5 Relación polinómica entre \\(x\\) e \\(y\\)",
    "text": "11.5 Relación polinómica entre \\(x\\) e \\(y\\)\nLa relación entre \\(X\\) e \\(Y\\) puede no ajustarse a una recta pero sí a una parábola o, en general, a un modelo polinómico. También se puede ajustar a otro tipo de modelo que se puede linealizar transformando los valores de X, los de Y, o ambos. Por ejemplo, nuestros datos se podrían adaptar al modelo \\(y = \\beta_0 e^{\\beta_1 x}\\) y realizando el cambio \\(y' = \\ln y\\) se obtiene el modelo lineal: \\(y' = \\ln \\beta_0 + \\beta_1 x\\) a partir del cual se deducen de forma inmediata los coeficientes del modelo original. El ejemplo que vamos a ver se ajusta aun modelo polinómino de segundo gramdo. Interesados en otras transformaciones para lienealizar la relación pueden consultar Montgomery y Peck (1992), pág. 90.\n\n\n\nFigura 11.8: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\nAquí más texto\n\n\n\nFigura 11.9: Múltiples opciones para minimizar la suma de los residuos en valor absoluto."
  },
  {
    "objectID": "regresionSimple.html#caso-particular-recta-por-el-origen",
    "href": "regresionSimple.html#caso-particular-recta-por-el-origen",
    "title": "11  Regresión Simple",
    "section": "11.5 Caso particular: Recta por el origen",
    "text": "11.5 Caso particular: Recta por el origen\nEn algunos casos, cuando por nuestro conocimiento del fenómeno que estudiamos sabemos que si \\(x=0\\) entonces seguro que \\(y=0\\), y la zona que estamos estudiando (nuestro rango de valroes) está próximo al origen, podemos forzar que la recta pase por el punto \\((0, 0)\\) para íncluya el conocimiento que ya tenemos del fennómeno que estamos estudiando. Conviene insistir en que también es necesario que los valores disponibles estén cerca del origen. Por ejemplo, al modelar el peso en función de la estatura sabemos que si la estatura es cero, el peso también será cero, pero no ajustamos a una recta por el origen porque los valores que se ajustan están lejos del origen y no se puede extrapolar la relación.\nSi la recta debe pasar por el origen su ecuación será: \\(y = b_1 X\\). Para deducir la expresión de \\(b_1\\) solo hay que minimizar: \\(S = \\sum (y_i -b_1x_i)^2\\) y siguiendo el procedimiento habitual tenemos: \\[ \\frac{\\mathrm{d}S}{\\mathrm{d}b_1}  = \\frac{\\mathrm{d}(\\sum y_i^2-\\sum 2y_ib_1x_i+ \\sum b_1^2x_i^2)} {\\mathrm{d} b_1} = -2 \\sum x_iy_i +2 \\sum x_i^2b_1\\]\nEstá claro que la segunda derivada es positiva (es una suma de cuadrados) por lo que tendremos un mínimo. Igualando a cero y despejando el valor de \\(b_1\\) se obtiene: \\[b_1 = \\frac{\\sum x_iy_i}{\\sum x_i^2} \\]\nEn este caso la recta no se ha ajustado con el método de los mínimos cuadrados (minimizando la suma de los cuadrados de los residuos) y algunas propiedades que se derivan esa formad e ajuste no se cumplen en este caso. Por ejemplo, el valor de \\(R^2\\) no es el cuadrado del coeficiente de correlación. R2 no es una buena medida de la calidad del ajuste."
  },
  {
    "objectID": "regresionSimple.html#relación-no-lineal-entre-x-e-y",
    "href": "regresionSimple.html#relación-no-lineal-entre-x-e-y",
    "title": "13  Regresión Simple",
    "section": "13.4 Relación no lineal entre \\(X\\) e \\(Y\\)",
    "text": "13.4 Relación no lineal entre \\(X\\) e \\(Y\\)\nSi a la vista del diagrama bivariante se observa que la relación entre \\(X\\) e \\(Y\\) no es lineal, se puede utilizar el aspecto de la nube de puntos y el conocimiento del fenómeno que se estudia para plantear un modelo que se ajuste a los datos. Los modelos polinómicos de segundo grado son muy versátiles y pueden ser una buena opción. También se puede ajustar a modelos linealizables transformando los valores de \\(X\\), los de \\(Y\\), o ambos. Si nuestros datos se ajustan a una función del tipo \\(y = \\beta_0 e^{\\beta_1 x}\\), podemos realizar el cambio \\(y' = \\ln y\\) obteniendo el modelo lineal: \\(y' = \\ln \\beta_0 + \\beta_1 x\\) a partir del cual se deducen de forma inmediata los coeficientes del modelo original. Interesados en este tipo de transformaciones para linealizar la dependencia pueden consultar Montgomery y Peck (1992) pág. 90. o Peña (2002) pág. 314.\nLa figura 13.10 (izquierda) muestra los datos de producción de electricidad de un aerogenerador según sea la velocidad del viento (datos en: Montgomery y Peck (1992), pág. 92). Se observa una relación no lineal ya que cuando la velocidad del viento es baja, pequeños incrementos en la velocidad tienen un impacto importante en la producción de electricidad, mientras que para velocidades altas la producción tiende a estabilizarse. Ajustando a una parábola se obtiene \\(y = -1,156 + 0,7229x -0,03812x^2\\) con un coeficiende de determinación \\(R^2 = 96,8%\\), lo cual no está nada mal.\nOtra opción es estudiar la producción de electricidad en función de la inversa de la velocidad del viento (figura de la derecha). Creamos la variable \\(X' = 1/X\\) y obtenemos el ajuste: \\(y = 2,979 - 6,935/x\\) con un \\(R^2 = 97,9%\\) que es también un valor excelente y, además, con un modelo más compacto. En general, trabajar con la inversa de \\(X\\) puede ser una buena alternativa al modelo cuadrático.\n\n\n\nFigura 13.10: Texto figura.\n\n\nHay que tener en cuenta que el modelo más adecuado no necesariamente es el que tiene el \\(R^2\\) más elevado. Nos interesa que el modelo sea compacto y que pueda interpretarse y sea coherente con nuestro conocimiento del fenómeno en estudio. Si vamos aumentando el grado del polinomio ajustado cada vez tendremos un mayor valor de \\(R^2\\) incluso, si tenemos pocos datos, podemos llegar a un \\(R^2\\) del 100% siendo el modelo obtenido totalmente inútil.\nVolviendo a los datos de la figura 13.9 donde a partir de los pesos de 5 personas (efectivamente son muy pocas, es solo un ejemplo) queremos modelar la relación entre peso y estatura, el modelo lineal es el más razonable. Si ajustamos los datos a un modelo cuadrático se tiene un máximo de peso en torno a una estatura de 175 cm que no tiene sentido. El polinomio de tercer grado presenta una forma que tampoco parece razonable y el de cuarto grado es un modelo con 5 parámetros (los 4 coeficientes y la ordenada en el origen) y como tenemos 5 puntos ajusta perfectamente, pero ni es un modelo razonable ni sirve en absoluto para estimar el peso de un individuo a partir de su altura (sí lo explica para los 5 individuos usados para construir el modelo, pero para esos ya lo sabíamos). Recuerde que dos puntos se ajustan perfectamente a un modelo con dos parámetros (una recta) tres puntos a un modelo con tres parámetros, … etc. Estos son modelos que explican muy bien lo que ya se sabe, pero son totalmente inútiles para hacer predicciones que es lo que -en general- se pretende.\n\n\n\nFigura 13.11: Aumentando el grado del polinomio la curva de adapta a los puntos pero solo explica lo que ya sabemos."
  },
  {
    "objectID": "regresionSimple.html#transformación-logarítmica",
    "href": "regresionSimple.html#transformación-logarítmica",
    "title": "13  Regresión Simple",
    "section": "13.5 Transformación logarítmica",
    "text": "13.5 Transformación logarítmica\nEn algunos casos, los valores de \\(X\\), los de \\(Y\\), o ambos, siguen una distribución asimétrica, con valores que aparecen agrupados cerca del origen y muy dispersos hacia los valores altos. Un ejemplo típico de esta situación se da al analizar la relación entre el peso del cerebro y el peso de cuerpo en 62 especies de mamímeros (Weisberg 2014, pág. 186). La mayoria de esos mamíferos pesan poco –la mediana es de 3,34 kg– pero algunos, como los elefantes, pesan varias toneladas y algo similar ocurre con el peso de los cerebros. Al realizar el diagrama bivariante del peso del cerebro (\\(Y\\)) frente al peso de cuerpo (\\(X\\)) prácticamente todos los puntos aparecen amontonados en la zona próxima al origen, En estas condiciones ajustar un modelo de regresión no tiene sentido, porque la mayoría de datos actúan como un solo punto y los que estan alejados tienen una gran influencia sobre la recta ajustada.\n\n\n\nFigura 13.12: Relación entre el peso del cerebro y el peso del cuerpo en 62 especies de mamíferos.\n\n\nUno puede caer en la tentación de considerar a los elefantes como valores anómalos y eliminarlos, pero esa no es una buena decisión por dos razones:\n\nRestringe la validez del modelo, ya no valdrá para todos los mamíferos considerados.\nAl eliminar esos valores y reescalar el gráfico aparecen otros valores anómalos: la persona humana (que da más reparo eliminar), la jirafa, el caballo, la vaca… y al final nos vamos quedando sin puntos.\n\nEn casos como este, la transformación logarítmica “estira” los datos permitiendo un ajuste en el que todos los puntos tienen una influencia similar. Realizando esta transformación en nuestros datos se obtiene -casi parece un milagro- una nube de puntos tal como esperamos tener cuando ajustamos a una recta.\n\n\n\nFigura 13.13: Peso del cerebro frente al peso del cuerpo antes y después de la transformación logarítmica.\n\n\nEl modelo obtenido es:\n\\[\\log(Y) = 0,9271 + 0,7517 \\log(X) \\quad \\text{con} \\quad R^2 = 91,95\\%\\] Volviendo a las variable soriginales nos queda (el cuerpo está en kg y el del cerebro en g):\n\\[Y = 8,45 · X^{3/4}\\] La transformación logarítmica de los datos es, sin duda, una buena opción en casos como este, pero también tiene efectos secundarios no deseados.\nEn primer lugar hay que tener en cuenta que los residuos (diferencia entre el valor real y el valor previsto) también están en escala logarítmica. POr ejemplo, para el elefante africano (el mayor, parece que la recta pasa por el punto) el valor real del peso del cerebro es de 5712 g y la previsión es de 6229 (+9%) y para el elefante asiático el valor real es de 4603 g mientras que el valor previsto es de 3031 g (-34%). El mamífero que presenta mayor residuo positivo es la persona humana (valor real: 1320, previsto: 185, -86%) mientras que el de mayor residuo negativo corresponde al Yapok (en inglés: Water opossum). Seguramente más interesante que el modelo en sí es conocer qué animales se separan más -por encima y por debajo- del patrón general. Sobre este tema existen muchas publicaciones. Los interesados pueden empezar explorando la Wikipedia y las referencias que incluye.\n\n\n\n\n\n\nTransformación logarítmica: No importa la base\n\n\n\nEn efecto, sea \\(y = \\ln (x)\\) y \\(z = \\log_{10}(x)\\). Tendremos que \\(e^y = x\\) y también que \\(10^z = x\\), luego \\(e^y = 10^z\\). Por tanto, \\(\\ln(e^y) = \\ln(10^z)\\) y es inmediato que: \\(y = \\ln(10)·z\\).\nPor tanto, cambiar la base del logaritmo equivale a multiplicar por una constante. En particular, para pasar del logaritmo neperiano al decimal basta con multiplicar por \\(\\ln(10)\\). El aspecto del diagrama bivariante es el mismo con independencia de la base utilizada para la transformación logarítmica, solo cambian las escalas, aunque para que al volver a las variables originales la expresión sea más compacta puede interesar elegir una base u otra.\n\n\nPosibilidad de usar los datos de Hooker, parece relación lineal pero no lo es y la transformación logarítmica da buen resultado"
  },
  {
    "objectID": "correlacion.html#necesidad-de-una-medida-de-la-relación",
    "href": "correlacion.html#necesidad-de-una-medida-de-la-relación",
    "title": "10  Medidas de relación lineal",
    "section": "10.1 Necesidad de una medida de la relación",
    "text": "10.1 Necesidad de una medida de la relación\nVeamos los cuatro casos de la figura 11.10. El primero muestra la relación entre la presión atmosférica (en pulgadas de Hg) y la temperatura de ebullición del agua (en ºF). Los datos fueron tomados por el botánico Joseph D. Hookeren en distintos puntos de la cordillera del Himalaya a mediados del siglo XIX. Si la presión atmosférica, y a partir de ella la altitud, se podía evaluar a partir de la temperatura de ebullición del agua (muy fácil de medir) se podía evitar el uso de los frágiles y difíciles de transportar barómetros que entonces se usaban. Fue un éxito, porque entre las dos variables se observa una relación casi perfecta.\nEl diagrama 2 muestra la relación entre la longitud de la circunferencia de los troncos de un determinado tipo de árbol el volumen de madera que se puede obtener de ellos (Fuente: Wolfram_Data_Repository 2016). Se observa una estrecha relación entre ambas variables. El diagrama 3 se ha obtenido con datos de esos mismos árboles pero representa el volumen de madera en función de la altura del arbol. EN este caso también se observa una relación, pero no tan clara como en el caso anterior.\nFinalmente, el diagrama 4 se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables.\n\n\n\n\n\n\n\n\n\n\n\nFigura 10.2: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\n\n\n\nAunque la información fundamental queda reflejada en el diagrama bivariante, en algunos casos, especialmente cuando es dudosa solo a la vista del gráfico, también resulta útil cuantificarla con una medida objetiva de relación lineal. Vamos a ver de donde sale y como se interpreta la covarianza y el coeficiente de correlación (a veces se añade “de Pearson” porque fue este estadístico el que lo desarrolló. Existen otros coeficientes de correlación de mucho menor uso, si no se dice de que tipo es siempre entendemos que se refiere al de Pearson)."
  },
  {
    "objectID": "correlacion.html#observación-y-cuantificación-de-la-relación",
    "href": "correlacion.html#observación-y-cuantificación-de-la-relación",
    "title": "12  Medidas de relación lineal",
    "section": "12.1 Observación y cuantificación de la relación",
    "text": "12.1 Observación y cuantificación de la relación\nVeamos los cuatro casos de la figura 12.2. El primero muestra la relación entre la presión atmosférica (en pulgadas de Hg) y la temperatura de ebullición del agua (en ºF). Los datos fueron tomados por el botánico Joseph D. Hookeren en distintos puntos de la cordillera del Himalaya a mediados del siglo XIX. Si la presión atmosférica, y a partir de ella la altitud, se podían evaluar a partir de la temperatura de ebullición del agua (muy fácil de medir) se podía evitar el uso de los frágiles y difíciles de transportar barómetros de la época. Fue un éxito, porque entre las dos variables se observa una relación casi perfecta.\nEl diagrama 2 muestra la relación entre la longitud de la circunferencia de los troncos de un determinado tipo de árbol y el volumen de madera que se puede obtener de ellos (Ryan, Joiner, y Ryan 1976). Se observa una estrecha relación entre ambas variables. El diagrama 3 se ha obtenido con datos de esos mismos árboles pero representa el volumen de madera en función de la altura del arbol. En este caso también se observa una relación, pero no tan clara como en el caso anterior.\nFinalmente, el diagrama 4 se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables.\n\n\n\nFigura 12.2: Cuatro situaciones que muestran diferentes grados de relación\n\n\nAunque la información fundamental queda reflejada en el diagrama bivariante, cuando a la vista del gráfico la relación es dudosa, o cuando interesa cuantificar la relación observada, disponemos de medidas de relación lineal. Las dos de uso más habitual son la covarianza y el coeficiente de correlación, mucho más esta última por las razones que veremos a continuación.\n\n\n\n\n\n\nNotación usada\n\n\n\nPara la covarianza no tenemos un símbolo específico. Cuando nos referimos a modelos teóricos, o a nivel poblacional, escribimos \\(\\text{Cov}(X,Y)\\). Para el valor calculado a partir de unos datos concretos (una muestra) escribimos \\(\\widehat{\\text{Cov}}(X,Y)\\) colocando un “sombrero” encima de \\(\\text{Cov}\\) para indicar que se trata de un estimador. Para el coeficiente de correlación utilizamos la regla general de asignar una letra latina al valor en la muestra, en este caso \\(r\\) y la letra griega correspondiente, \\(\\rho\\), para la población."
  },
  {
    "objectID": "correlacion.html#apéndice-1",
    "href": "correlacion.html#apéndice-1",
    "title": "9  Medidas de relación lineal",
    "section": "APÉNDICE 1",
    "text": "APÉNDICE 1\n\nCoeficiente de correlación cuando la relación lineal es perfecta\nSi todos los puntos están alineados tenemos que: \\(y_i =a+bx_i\\) y también: \\(\\sum y_i = \\sum (a+bx_i)\\), de donde \\(n\\bar{y} = na + bn \\bar{x}\\), es decir: \\(\\bar {y} =a+b \\bar{x}\\). Así pues, podemos escribir el numerador de la expresión de \\(r\\) de la forma:\n\\[\\begin{equation*}\n    \\begin{split}\n        \\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right ) \\left ( y_i - \\bar{y} \\right ) &= \\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right ) \\left ( a+bx_i - (a+b\\bar{x}) \\right ) \\\\\n        &= \\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )   b \\left ( x_i - \\bar{x} \\right ) = b \\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2\n    \\end{split}\n\\end{equation*}\\]\nCon un razonamineto similar, en el denominador podemos escribir el término en que aparecen los valores \\(y_i\\) de la forma: \\[\\begin{equation*}\n    \\begin{split}\n        \\sqrt{\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 } &= \\sqrt{\\sum_{i=1}^n \\left ( a+bx_i - ( a+b \\bar{x} ) \\right )^2 } =  b \\sqrt{ \\sum_{i=1}^n \\left ( x_i -  \\bar{x}  \\right )^2 } \\\\\n    \\end{split}\n\\end{equation*}\\]\nRecuperando el término correspondiente a los valores de \\(x\\), el denominador nos queda:\n\\[  \\sqrt {\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2} \\cdot b \\sqrt{\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2 } = b \\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2 \\]\nIgual que el numerador, aunque con la diferencia de que su valor seguro que es positivo (proviene de un producto de desviaciones típicas) mientras que el valor de \\(b\\) en el numerador será positivo o negativo según sea el signo de la pendiente de la recta.\nAsí pues, si los puntos se alinean según una recta, el valor del coeficiente de correlación será -1 o 1 según la pendiente sea negativa o positiva.\n\n\n\n\nDraper, N. R., y H. Smith. 1998. Applied Regression Analysis. 3.ª ed. USA: John Wiley & Sons, Inc.\n\n\nRyan, T. A., B. L Joiner, y B. F. Ryan. 1976. The Minitab Student Handbook. 1.ª ed. USA: Duxbury Press.\n\n\nWeisberg, S. 2014. Applied Linear Regression. 4.ª ed. USA: John Wiley & Sons, Inc."
  },
  {
    "objectID": "planteamientoGeneral.html",
    "href": "planteamientoGeneral.html",
    "title": "1  Plantemiento general",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "queEs.html",
    "href": "queEs.html",
    "title": "¿Qué es la estadística?",
    "section": "",
    "text": "Estoy haciendo pruebas.\n\nEstadística y matemáticas\ntexto\n\n\nEstadística y probabilidad\ntexto\n\n\nEstadística y Data Science\ntexto"
  },
  {
    "objectID": "estMatematicas.html",
    "href": "estMatematicas.html",
    "title": "Estadística y matemáticas",
    "section": "",
    "text": "Estoy haciendo pruebas."
  },
  {
    "objectID": "estProbabilidad.html",
    "href": "estProbabilidad.html",
    "title": "Estadística y probabilidad",
    "section": "",
    "text": "Estoy haciendo pruebas."
  },
  {
    "objectID": "estDataScience.html",
    "href": "estDataScience.html",
    "title": "Estadística y Data Science",
    "section": "",
    "text": "Estoy haciendo pruebas."
  },
  {
    "objectID": "regresionMultiple.html#las-cosas-no-son-como-parecen",
    "href": "regresionMultiple.html#las-cosas-no-son-como-parecen",
    "title": "14  Regresión Múltiple",
    "section": "14.1 Las cosas no son como parecen",
    "text": "14.1 Las cosas no son como parecen\nEjemplo con 3 variables. Ejemplo examen."
  },
  {
    "objectID": "regresionMultiple.html#fuerza-bruta.-aquí-va-en-serio.",
    "href": "regresionMultiple.html#fuerza-bruta.-aquí-va-en-serio.",
    "title": "14  Regresión Múltiple",
    "section": "14.2 Fuerza bruta. Aquí va en serio.",
    "text": "14.2 Fuerza bruta. Aquí va en serio.\nBest subsets"
  },
  {
    "objectID": "regresionMultiple.html#ojo-con-las-explicaciones-milagrosas.",
    "href": "regresionMultiple.html#ojo-con-las-explicaciones-milagrosas.",
    "title": "11  Regresión Múltiple",
    "section": "11.4 Ojo con las explicaciones milagrosas.",
    "text": "11.4 Ojo con las explicaciones milagrosas.\nTantos parámetros como datos"
  },
  {
    "objectID": "regresionMultiple.html#a-tener-en-cuenta",
    "href": "regresionMultiple.html#a-tener-en-cuenta",
    "title": "14  Regresión Múltiple",
    "section": "14.6 A tener en cuenta",
    "text": "14.6 A tener en cuenta\nAnálisis exploratoio más complicado Lo mismo con el análisis de los residuos\nOtras medidas de calidad del ajuste.\nLa regresión no sirve para todo"
  },
  {
    "objectID": "regresionMultiple.html#un-caso-sencillo-una-variable-cuantitativa-y-otra-cualitativa",
    "href": "regresionMultiple.html#un-caso-sencillo-una-variable-cuantitativa-y-otra-cualitativa",
    "title": "14  Regresión Múltiple",
    "section": "14.3 Un caso sencillo: Una variable cuantitativa y otra cualitativa",
    "text": "14.3 Un caso sencillo: Una variable cuantitativa y otra cualitativa\npeso, altura, sexo, y datos UFFI (creo)"
  },
  {
    "objectID": "regresionMultiple.html#ojo-con-las-explicaciones-milagrosas",
    "href": "regresionMultiple.html#ojo-con-las-explicaciones-milagrosas",
    "title": "14  Regresión Múltiple",
    "section": "14.4 Ojo con las explicaciones milagrosas",
    "text": "14.4 Ojo con las explicaciones milagrosas\nTantos parámetros como datos"
  },
  {
    "objectID": "regresionMultiple.html#modelos-explicativos-y-modelos-predictivos",
    "href": "regresionMultiple.html#modelos-explicativos-y-modelos-predictivos",
    "title": "14  Regresión Múltiple",
    "section": "14.5 Modelos explicativos y modelos predictivos",
    "text": "14.5 Modelos explicativos y modelos predictivos\ntexto"
  },
  {
    "objectID": "regresionSimple.html",
    "href": "regresionSimple.html",
    "title": "13  Regresión Simple",
    "section": "",
    "text": "Apéndice 10.1"
  },
  {
    "objectID": "Parte_Contraste.html",
    "href": "Parte_Contraste.html",
    "title": "CONTRASTE DE HIPÓTESIS",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Conceptos_CH.html",
    "href": "Conceptos_CH.html",
    "title": "8  Conceptos",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Proceso_CH.html",
    "href": "Proceso_CH.html",
    "title": "9  Esquema de razonamiento",
    "section": "",
    "text": "Todo provisional."
  },
  {
    "objectID": "tiposError_CH.html",
    "href": "tiposError_CH.html",
    "title": "10  Tipos de error",
    "section": "",
    "text": "Este título es provisional"
  },
  {
    "objectID": "regresionSimple.html#distribución-de-los-coeficientes",
    "href": "regresionSimple.html#distribución-de-los-coeficientes",
    "title": "13  Regresión Simple",
    "section": "13.7 Distribución de los coeficientes",
    "text": "13.7 Distribución de los coeficientes\nLa buena noticia es que si los datos cumplen unas ciertas condiciones –que en general se cumplirán– los valores de los coeficientes pertenecen a distribuciones Normales con una media igual al verdadero valor del parámetro estimado (el valor obtenido es un estimador insesgado, estamos de suerte) y una desviación típica que depende del número de datos (cuantos más datos menos incertidumbre y, por tanto, menos desviación típica) y también depende de la variabilidad de la respuesta (cuanto menos variabilidad menos incertidumbre y menos desviación típica).\n==Nos enfrentamos a esta situación realizando unas suposiciones sobre el comportamiento de los datos y si esas suposiciones se cumplen los coeficientes del modelo tienen unas propiedades conocidas que facilitan la interpretación del modelo.== texto alternativo.\nConocer la distribución de los coeficientes hace posible calcular intervalos de confianza para sus valores o realizar constrastes de hipótesis sobre sus valores. Tiene especial interés contrastar la hipótesis nula de que la pendiente de la recta (\\(\\beta_1\\)) es igual a cero.\n\n\n\nFigura 13.16: Distribución de los coeficientes.\n\n\nLo que pedimos respecto al comportamiento de los datos es:\nValores de \\(X\\): No hay ninguna exigencia sobre estos valores. Pueden corresponder a una variable discreta o continua y no importa cual sea su distribución.\nValores de \\(Y\\): Dado un valor de \\(X\\), los valroes de \\(Y\\) deben seguir una distribución Normal. Si \\(X\\) es la estatura e \\(Y\\) es el peso, no hace falta suponer que el peso sigue una distribución Normal, sino que los pesos para las personas de una determinada altura siguen una distribución Normal. Más claro: No todos los que miden 1,70 m pesan lo mismo, pues bien, suponemos que los pesos de los mioden 1,70 siguen una distribución Normal. Y quien dice 1,70 dice cualqueir otra altura. Obserque es to no es lo mismo que decir que los pesos -globalmente- siguen una distribución Normal. Formalmente se escribe:\nSobre la variabilidad: La variabilidad de Y no depende del valor de X. Es decir, suponemos que la variabilidad en el peso de las personas que miden 1,60 es la misma que para las personas que miden 1,80 m. Es posible que esto no sea exactamente así porque es habitual que cuando aumenta el nivel de la respuesta aumente también su variabilidad…\nAunque las hipótesis no se cumplan exactamente esperamos …\nA no ser que los datos pongan de manifiesto…\nTambién suponemos que los residuos son indepencientes, es decir, la desviación en un punto no da ninguna pista sobre la desviación en el siguiente. Esto no ocurre con las variables que evolucionan en el tiempo, como la temperatura o la cotización de acciones en la bolsa (Consejo: no crea que usando la estadística se hará rico en la bolsa).\nSi esas hipóstessi se cumplen los coeficientes siguen distribuciones Normales con parámetros conocidos.\n-Pruebas de significación para \\(b_1\\)\n-Intervalos de confianza para las predicciones.\n-IC para la recta.\n-IC para las observaciones.\n\n\n\n\n\n\nLigando las pruebas se significación para \\(r\\) y para \\(b_1\\)\n\n\n\nAquí texto\n\n\n\nCondiciones que deben reunir los datos\nCuanto más pedimos a los datos, más exigentes debemos ser con las condiciones que deben cumplir.\nNormalidad de la respuesta\nEstimación de los parámetros"
  },
  {
    "objectID": "regresionSimple.html#ajuste-a-una-recta",
    "href": "regresionSimple.html#ajuste-a-una-recta",
    "title": "13  Regresión Simple",
    "section": "13.1 Ajuste a una recta",
    "text": "13.1 Ajuste a una recta\nSi entre la respuesta y la variable regresora se observa una relación lineal se determina la ecuación de la recta que mejor se adapta a los puntos disponibles. Lo que significa “mejor” es discutible. Veamos algunas formas de hacerlo.\n\nA ojo\nSe traza la recta directamente sobre el papel o se identifican dos puntos de paso y a partir de ellos se calculan los coeficientes del modelo.\nA pesar de sus evidentes limitaciones, si solo se trata de tener la recta no es un método tan malo como parece. Con un poco de práctica el ajuste no será muy distinto del “perfecto” y no se cometeran errores de bulto debido a la presencia de valores anómalos, cosa que sí puede ocurrir si se tratan los datos de forma automática sin mnirarlos.\n\n\n\nFigura 13.2: Ajuste a ojo y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\n\n\n\n\n\nPROS \n\nIntuitivo. Muy fácil de entender.\nNo se comenten errores de mucho bulto.\n\n\n\nCONS \n\nNo se logra el ajuste “perfecto” de acuerdo con el criterio establecido. |\nNo se tienen medidas de calidad del ajuste. |\nSolo sirve para regresión simple.\n\n\n\n\n\n\nMétodo de Ishikawa\nSe identifica el primer y el tercer cuartil de los valores de \\(X\\) \\((X_{Q1}\\) y \\(X_{Q3})\\), e igual para los valores de \\(Y\\) \\((Y_{Q1}\\) y \\(Y_{Q3})\\). Se traza la recta por los puntos \\((X_{Q1}\\) y \\(Y_{Q1})\\) y \\((X_{Q3}\\) y \\(Y_{Q3})\\). Se obtiene una recta muy razonable sin necesidad de realizar cálculos ni de aplicar fórmulas de las que se desconoce su lógica.\n\n\n\nFigura 13.3: Ajuste por el método de Ishikawa y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 13.1: Método de Ishikawa. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nFácil de entender\nRobusto frente a la presencia de valores anómalos o con excesiva influencia\n\n\n\nCONS \n\nNo se tienen medidas de calidad del ajuste\nSolo sirve para regresión simple\n\n\n\n\n\n\n\n\n\n\n\nKaoru Ishikawa (1915-1989)\n\n\n\nFue un ingeniero japonés, considerado uno de los artífices del llamado “milagro japonés” que condujo los productos japoneses desde la mediocridad hasta arrasar en los mercados mundiales (electrónica, fotografía, automoción,…). Una de las claves del éxito fue el uso intensivo de técnicas estadísticas para el control y la mejora de la calidad. Ishikawa es conocido por proponer el uso de herramientas sencillas, que todos puedan entender y aplicar de forma habitual.\n\n\n\n\nHaciendo que la suma de los residuos sea igual a cero\nSe trata de obtener los valores de \\(b_0\\) y \\(b_1\\) que cumplen la expresión:\n\\[\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]= 0\\] qye es equivalente a:\n\\[ n\\bar{Y} - nb_0 - b_1 n \\bar{X} = 0\\] Por tanto, con cualquier par de valores \\(b_0\\) y \\(b_1\\) que verifiquen la expresión \\(\\bar{Y} = b_0 + b_1 \\bar{X}\\), es decir, con cualquier recta que pase por (\\(\\bar{X}\\), \\(\\bar{Y}\\)) tendremos una suma de residuos igual a cero.\nQue haya infinitas rectas que cumplan esa condición es una mala señal, porque seguro que no todas son adecuadas. Para los valores representados en la figura 13.4 tenemos que \\(\\bar{X}= 6\\) y \\(\\bar{Y}= 9\\). Rectas que hacen que la suma de los residuos sea igual a cero son, por ejemplo, la que tiene coeficientes \\(b_0=9\\) y \\(b_1=0\\), es decir: \\(Y = 9\\), o también \\(b_0 = 12\\) y \\(b_1 = -0.5\\), es decir: \\(Y = 12 -0.5X\\) y ambos son claramente muy malos ajustes.\n\n\n\nFigura 13.4: Dos ajustes -claramente muy malos- con suma de residuos igual a cero.\n\n\n\n\nTabla 13.2: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nNinguna\n\n\n\nCONS \n\nDa un número infinito de soluciones (una de ellas coincide con el ajuste por mínimos cuadrados)\n\n\n\n\n\n\n\nMinimizando la suma del valor absoluto de los residuos\nSe trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left| Y_i - (b_0 - b_1 X_i) \\right|= 0\\]\nPuede no tener solución única, pero los resultados posibles son mucho más razonables que en el caso anterior. Un problema específico de este caso es que no existen expresiones analíticas para los coeficientes debido a las dificultades en el manejo de la función “valor absoluto”.\nLa figura 13.5 muestra dos diagramas con los mismos 4 puntos y ñas rectas que cumplen el criterio estableciso, en todas ellas la suma del valor absoluto de los residuos es igual a 2. La línea azul, que es la misma en los dos diagramas, es la que también minimiza la suma de los cuadrados de los residuos.\n\n\n\nFigura 13.5: Posibles opciones para minimizar la suma de los residuos en valor absoluto.\n\n\n\n\nTabla 13.3: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nPoco sensible a la presencia de valores anómalos\n\n\n\nCONS \n\nNo da un ajuste equilibrado entre todos los puntos\nNo existe expresión analítica para el cálculo de los coeficientes\n\n\n\n\n\n\n\n\nMinimizando la suma de los cuadrados de los residuos\nElevamos los residuos al cuadrado en vez de usar su valor absoluto para evitar que al sumarlos se compensen los positivos y negativos. Ahora se trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]^2= 0\\]\nEn vez de decir que el criterio de ajuste ha sido “minimizar la suma de los cuadrados de los residuos”, decimos que hemos ajustado por “mínimos cuadrados”. Este es el método usado en la inmensa mayoría de los casos, produce un ajuste equilibrado de la nube de puntos y está en perfecta sintonía con otras técnicas y medidas, que se contruyen en torno a criterios similares. Un inconveniente de este método de ajuste es que el modelo obtenido es sensible a la presencia de valores anómalos, cosa que no ocurre si se minimiza la suma de valores absolutos.\nEn la primera fila de la figura 13.6 tenemos una situación típica en que el ajuste por mínimos cuadrados da un mejor resultado que minimizando la suma del valor absoluto. Sin embargo, en la segunda fila tenemos un cado de puntos perfectamente alineados excepto un que muy probablemente sería un valor anómalo. Si minimizamos el valor aboluto de los residuos el ajuste ignora el valor anómalo mientras que ajustado por mínimos cuadrados el valor anómalo tiene una notable influencia sobre la recta ajustada.\n\n\n\nFigura 13.6: Ajustes obtenidos minimizando la suma de valores absolutos y la suma de los cuadrados de los residuos\n\n\n\n\nTabla 13.4: Minimizar la suma de los cuadrados de los residuos. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nProporciona un ajuste muy razonable. El que queremos hacer cuando ajustamos a ojo.\nEncaja perfectamente con el resto de técnicas estadísticas que utilizamos.\n\n\n\nCONS \n\nLos valores anómales (errores o valores singulares) pueden tener bastante influencia sobre el modelo estimado. Hay que estar atentos para tratar esos puntos adecuadamente"
  },
  {
    "objectID": "01_queEs.html",
    "href": "01_queEs.html",
    "title": "¿Qué es la estadística?",
    "section": "",
    "text": "Estoy haciendo pruebas.\n\nEstadística y matemáticas\ntexto\n\n\nEstadística y probabilidad\ntexto\n\n\nEstadística y Data Science\ntexto"
  },
  {
    "objectID": "10_Parte_Descriptiva.html",
    "href": "10_Parte_Descriptiva.html",
    "title": "ESTADÍSTICA DESCRIPTIVA",
    "section": "",
    "text": "Síntesis numérica de datos y representaciones gráficas.\nIdeas de:\n\nLibro Preguntas frecuentes: Media, mediana, por qué dividimos por n-1, como calcular los cuartiles.\nArtículo analogías: Síntesis numérica y retrato robot, el rio tiene una profundidad media de 1,5 m\nLibro matemáticas en primera plana: Buenos y malos usos de las representaciones gráficas."
  },
  {
    "objectID": "11_sintesisNumerica.html#medidas-de-tendencia-central",
    "href": "11_sintesisNumerica.html#medidas-de-tendencia-central",
    "title": "1  Síntesis numérica de datos",
    "section": "1.1 Medidas de tendencia central",
    "text": "1.1 Medidas de tendencia central\nAquí texto sobre la importancia de no pretender describir datos solo con medidas de tendencia central.\n\nMedia aritmética\nTanto el cálculo como el significado de la media aritmética son bien conocidos por los estudiantes. Además de que la idea es sencilla, su cálculo tiene interés para saber si se aprobará una asignatura a la vista de las notas del curso. [pretendo empezar con un toque de humor pero no sé si se entiende]\nCuando los datos se presentan tabulados no hay que olvidar que cada uno se repite tantas veces como indica su frecuencia absoluta. En la Tabla 1.1, la variable considerada es el número de hijos por familia (\\(x\\)) y el número de familias para cada uno de esos valores es la frecuencia absoluta (\\(n\\)).\n\n\nTabla 1.1: Número de hijos por familia\n\n\nNúmero de hijos (\\(x\\)):\n0\n1\n2\n3\n4\n5\n\n\nNúmero de familias (\\(n\\)):\n13\n21\n15\n8\n1\n2\n\n\n\n\nEl valor medio (\\(\\bar{x}\\)) del número de hijos por famila es igual a:\n\\[ \\bar{x} = \\frac{\\sum_{i} n_i \\, x_i }{N} \\:  = \\:  1,48 \\]\nDonde \\(N\\) es el número total de datos (en este caso \\(N=60\\)) e \\(i\\) es el índice que aquí va de 1 a 6 para valores de \\(x\\) de 0 a 5.\nPara cada valor de \\(x_i\\) su frecuencia relativa es \\(f_i = n_i/N\\). También podemos calcular la media usando la expresión:\n\\[ \\bar{x} = \\sum_{i} f_i \\, x_i   =  1,48 \\] Esta expresión es análoga a la de la esperanza matemática (solo hay que cambiar frecuencia relativa por probabilidad) que veremos más adelante,\n\n\n\n\n\n\nMedia (mean), promedio (average) y esperanza matemática (expected value)\n\n\n\nAquí texto\n\n\n\n\nMediana\nSe ordenan los valores y el que queda en centro es la mediana. Si el número de valores es par se toma como mediana el promedio de los dos centrales.\nLa mediana tiene unas propiedades de las que carece la media, por lo que es un buen complemento informativo e incluso puede ser más útil en algunos casos. Sus ventajas respecto a la media son:\n\nEs más robusta frente a la presencia de valores anómalos. Supongamos que nuestros datos son:\n\\[2, \\, 5, \\, 6, \\,7 \\,\\text{ y  }\\, 9\\] La media es 5,6 y la mediana es 6. Si al introducir los datos al ordenador nos equivocamos y en último lugar en vez de 9 introducimos 99, la media pasa a ser 23,8 mientras que la mediana sigue siendo 6. Esto hace que la mediana sea útil cuando nos enfrentamos a un conjunto de datos que puede contener valores anómalos (posibles errores, problemas con la importación…) ya que da una idea menos contaminada por esos valores anomálos, del valor en torno al cual se mueven los datos.\nPor su propia definición, la mediana deja un 50% de las observaciones por debajo y otro 50% por encima y esto le da unas ventajas que la media no tiene. Si queremos saber si en nuestra empresa estamos entre los que cobran más o entre los que cobran menos, debemos comparar nuestro salario con la mediana, no con la media. Si sólo hay 10 trabajadores y los salarios son (en las unidades que corresponda):\n\\[8, \\, 8, \\, 9, \\,9, \\, 10, \\, 10, \\, 11, \\,11, \\, 12 \\,\\text{ y  }\\, 100\\] todos menos uno (en este caso el 90%) están por debajo de la media, que es 18,8. Esto no pasa nunca con la mediana: si estamos por encima de la mediana, estamos con el 50% de los que más cobra. Otro ejemplo. Si en un examen las notas van de 0 a 10 y se aprueba sacando una nota igual o superior a 5, si la nota media es 5 no sabemos cuántos han aprobado. Si se han examinado 50 estudiantes, puede ser que 41 hayan suspendido con un 4; 8 hayan sacado un 10 y uno haya obtenido un 6. Esto da media 5, aunque el 82 % ha suspendido. Si la mediana es 5, seguro que la mitad han aprobado.\n\nSi la distribución de los datos es simétrica, la media y la mediana coinciden y entonces todo son ventajas. Por ejemplo, en una distribución Normal la media y la mediana son iguales, por tanto, si los valores que tenemos provienen de una Normal, la media y la mediana no andarán muy lejos una de otra. En cualquier caso, siempre podemos calcular las dos y aprovechar lo mejor de cada una.\nSegún publicó el diario El País (El País, 2024), el ejecutivo de las empresas cotizadas en la bolsa española mejor pagado en 2023 tuvo unos ingresos de 23,77 millones de euros. Esta empresa tenía ese año 2299 empleados en España, suponiendo que ese ejecutivo formara parte de la plantilla y que todos menos él cobraran solo el salario mínimo (15.120 €/año), se podría decir que en promedio cobran 25.453 €/año, un 68 % más de lo que cobrarían todos menos uno. [no sé si poner este párrafo, quizá se podría poner ejercicio y que busquen los datos]\n\n\nModa\nDado un conjunto de valores, la moda es el que más se repite. A difencia de la media y la mediana puede no ser única y se puede aplicar también a varibles cualitativas (no numéricas). Si realizamos sobre el método de transporte que usan los estudiantes para ir a su centro de estudio la moda puede ser el autobús o ir andando. No es una medida muy relevante. Siempre aparece en los libros pero rara vez en los resúmenes estadísticos.\nA modo de resumen de este apartado, la figura 1.1 muestra la distribución de los salarios en España en 2021. Tiene dos “Salarios más frecuentes” (observe que se evita el término “moda”, quizá porque se puede prestar a malas interpretaciones). Cuando se da esta circunstancia se dice que la distribución es bimodal. El salario medio es mayor que el mediano porque los salarios muy altos tiran de la media pero apenas afectan a la mediana. \n\n\n\nFigura 1.1: Distribución de los salarios en España en 2021. (Fuente: INE 2023)."
  },
  {
    "objectID": "11_sintesisNumerica.html#medidas-de-dispersión",
    "href": "11_sintesisNumerica.html#medidas-de-dispersión",
    "title": "1  Síntesis numérica de datos",
    "section": "1.2 Medidas de dispersión",
    "text": "1.2 Medidas de dispersión\nQuizá lo más importante de este capítulo -y de todo el libro- es interiorizar la importancia de considerar la variabilidad para para describir datos, y para analizarlos y tomar decisiones tomándola en consideración.\n\nRango\ntexto\n\n\nVarianza\ntexto\n\n\nDesviación típica\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto"
  },
  {
    "objectID": "11_sintesisNumerica.html#medidas-de-posición",
    "href": "11_sintesisNumerica.html#medidas-de-posición",
    "title": "1  Síntesis numérica de datos",
    "section": "1.3 Medidas de posición",
    "text": "1.3 Medidas de posición\ntexto\ntexto\n\nCuartiles\ntexto\n\n\n\n\n\n\nLos cuartiles como zona\n\n\n\nAunque un cuartil es un número y no una zona, a veces se dice que un valor pertenece al primer o al tercer cuartil y lo que eso significa no siempre está claro. Podria pensarse que pertenece al primer cuartil cuando es menor que Q1 y al tercero cuando es mayor que la mediana y menor que Q3, pero no siempre es así. La relevancia de las revistas científicas se mide por su “factor de impacto”, un número relacionado con las citas en otras revistas de los artículos que publican. Para cada ámbito de conocimiento se realizan rankins de revistas según su factor de impacto y todos queremos publicar en las que lo tienen más alto, que no son las del cuarto, seguido por las del tercer cuartil, sino en del primero y el segundo. El ranking se hace de más a menos.\n\n\n\n\nDeciles. Percentiles\ntexto\ntexto"
  },
  {
    "objectID": "11_sintesisNumerica.html#porcentajes",
    "href": "11_sintesisNumerica.html#porcentajes",
    "title": "1  Síntesis numérica de datos",
    "section": "1.4 Porcentajes",
    "text": "1.4 Porcentajes\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\n¿Qué hace esto aquí?"
  },
  {
    "objectID": "12_graficos.html",
    "href": "12_graficos.html",
    "title": "2  Representaciones gráficas",
    "section": "",
    "text": "Todavía no he hecho nada sobre esto, pero tengo mucho material y las ideas claras."
  },
  {
    "objectID": "20_Parte_Variabilidad.html",
    "href": "20_Parte_Variabilidad.html",
    "title": "SOBRE LA VARIABILIDAD",
    "section": "",
    "text": "Importancia de saberse mover en un contexto de variabilidad. No todas las variabilidades son iguales.\nEstoy pensando en empezar planteando el tema de la fecha de nacimiento de los futbolistas profesionales y a partir de ahí introducir la distribución binomial y su utilidad para separar señal y ruido.\nSiguiendo con el mundo de futbol, podría introducir la distribución de Poisson con el ejemplo del número de goles por partido.\nLa distribución Normal se puede introducir como límite de histograma. También partiendo de la distribución Normal.\nNo creo que -al menos aquí- haya que hablar de distribuciones de referencia teóricas: t-Student, Chi-cuadrado, F de Snededor."
  },
  {
    "objectID": "32_estimacionMedias.html",
    "href": "32_estimacionMedias.html",
    "title": "6  Estimación de medias",
    "section": "",
    "text": "Similar a estimación de proporciones."
  },
  {
    "objectID": "31_estimacionProporciones.html",
    "href": "31_estimacionProporciones.html",
    "title": "5  Estimación de proporciones",
    "section": "",
    "text": "Lo típico. Libro de respuestas.\nAnexo sobre sondeos electorales."
  },
  {
    "objectID": "21_variablesAleatorias.html",
    "href": "21_variablesAleatorias.html",
    "title": "3  Variables aleatorias",
    "section": "",
    "text": "Importancia, tipos, propiedades."
  },
  {
    "objectID": "22_distribucionesProbabilidad.html",
    "href": "22_distribucionesProbabilidad.html",
    "title": "4  Distribuciones de probabilidad",
    "section": "",
    "text": "Binomial, Poisson, Normal, otras (uniforme, equiprobable, curiosidades: unif+unif = triangular)"
  },
  {
    "objectID": "30_Parte_Estimacion.html",
    "href": "30_Parte_Estimacion.html",
    "title": "ESTIMACIÓN",
    "section": "",
    "text": "Lo típico. Como estimar las características de una población a partir de los datos de una muestra.\nPropiedades que deseamos tener en los estimadores\nEstimación de proporciones. Intervalo de confianza, margen de error, etc.\nEstimación de medias\nEstimación del tamaño de una población (peces y taxis)\nAnexo: Sondeos electorales."
  },
  {
    "objectID": "33_estimacionTamañoPoblacion.html",
    "href": "33_estimacionTamañoPoblacion.html",
    "title": "7  Estimación del tamaño de la población",
    "section": "",
    "text": "Peces y taxis."
  },
  {
    "objectID": "40_Parte_Contraste.html",
    "href": "40_Parte_Contraste.html",
    "title": "CONTRASTE DE HIPÓTESIS",
    "section": "",
    "text": "No tengo claros los detalles de lo que habría que poner aquí.\nLas ideas clave están en el libro de Preguntas Frecuentes."
  },
  {
    "objectID": "50_Parte_Experimentos.html",
    "href": "50_Parte_Experimentos.html",
    "title": "DISEÑO DE EXPERIMENTOS",
    "section": "",
    "text": "Anécdota de la catadora de te.\nCreo que un capítulo en torno a las flores y la Aspirina y otro en torno a la duración de pilas caras y baratas."
  },
  {
    "objectID": "60_Parte_Corre_Regre.html",
    "href": "60_Parte_Corre_Regre.html",
    "title": "CORRELACIÓN Y REGRESIÓN",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "61_correlacion.html",
    "href": "61_correlacion.html",
    "title": "13  Medidas de relación lineal",
    "section": "",
    "text": "APÉNDICE 9.1"
  },
  {
    "objectID": "61_correlacion.html#observación-y-cuantificación-de-la-relación",
    "href": "61_correlacion.html#observación-y-cuantificación-de-la-relación",
    "title": "13  Medidas de relación lineal",
    "section": "13.1 Observación y cuantificación de la relación",
    "text": "13.1 Observación y cuantificación de la relación\nVeamos los cuatro casos de la figura 13.2. El primero muestra la relación entre la presión atmosférica (en pulgadas de Hg) y la temperatura de ebullición del agua (en ºF). Los datos fueron tomados por el botánico Joseph D. Hookeren en distintos puntos de la cordillera del Himalaya a mediados del siglo XIX. Si la presión atmosférica, y a partir de ella la altitud, se podían evaluar a partir de la temperatura de ebullición del agua (muy fácil de medir) se podía evitar el uso de los frágiles y difíciles de transportar barómetros de la época. Fue un éxito, porque entre las dos variables se observa una relación casi perfecta.\nEl diagrama 2 muestra la relación entre la longitud de la circunferencia de los troncos de un determinado tipo de árbol y el volumen de madera que se puede obtener de ellos (Ryan, Joiner, y Ryan 1976). Se observa una estrecha relación entre ambas variables. El diagrama 3 se ha obtenido con datos de esos mismos árboles pero representa el volumen de madera en función de la altura del arbol. En este caso también se observa una relación, pero no tan clara como en el caso anterior.\nFinalmente, el diagrama 4 se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables.\n\n\n\nFigura 13.2: Cuatro situaciones que muestran diferentes grados de relación\n\n\nAunque la información fundamental queda reflejada en el diagrama bivariante, cuando a la vista del gráfico la relación es dudosa, o cuando interesa cuantificar la relación observada, disponemos de medidas de relación lineal. Las dos de uso más habitual son la covarianza y el coeficiente de correlación, mucho más esta última por las razones que veremos a continuación.\n\n\n\n\n\n\nNotación usada\n\n\n\nPara la covarianza no tenemos un símbolo específico. Cuando nos referimos a modelos teóricos, o a nivel poblacional, escribimos \\(\\text{Cov}(X,Y)\\). Para el valor calculado a partir de unos datos concretos (una muestra) escribimos \\(\\widehat{\\text{Cov}}(X,Y)\\) colocando un “sombrero” encima de \\(\\text{Cov}\\) para indicar que se trata de un estimador. Para el coeficiente de correlación utilizamos la regla general de asignar una letra latina al valor en la muestra, en este caso \\(r\\) y la letra griega correspondiente, \\(\\rho\\), para la población."
  },
  {
    "objectID": "61_correlacion.html#covarianza",
    "href": "61_correlacion.html#covarianza",
    "title": "13  Medidas de relación lineal",
    "section": "13.2 Covarianza",
    "text": "13.2 Covarianza\nAparece como en las calculadoras con funciones estadísticas, en los paquetes de software y también en las hojas de cálculo, aunque en la práctica resulta poco útil como medida de relación lineal. Aquí la incluimos porque su expresión es fácil de justificar y ayuda a entender la fórmula del coeficiente de correlación.\n\nDeducción de la fórmula\nLa figura 13.3 muestra la relación entre dos variables \\(X\\) e \\(Y\\). En la derecha el diagrama se ha dividido en cuatro cuadrantes trazando una línea vertical que pasa por la media de los valores de \\(X\\) y una horizontal por la media de los valores de \\(Y\\). A estos valores medios los designamos \\(\\bar{x}\\) e \\(\\bar{y}\\) respectivamente. Los cuadrantes van del I al IV en el sentido de las agujas del reloj.\n\n\n\nFigura 13.3: Deducción de la fórmula de la covarianza\n\n\nEn todos los puntos del primer cuadrante la distancia \\(x - \\bar{x}\\) es positiva, ya que todos se encuentran a la derecha de \\(\\bar{x}\\). También será positiva la distancia \\(y - \\bar{y}\\) ya que todos están por encima de \\(\\bar{y}\\). Por tanto, el producto de ambas distancias \\((x - \\bar{x})(y - \\bar{y})\\) será positivo para todos los puntos que se hayan en el primer cuadrante.\nPara los del segundo cuadrante este producto es negativo ya que la distancia \\(x - \\bar{x}\\) sigue siendo positiva (todos los puntos se encuentran a la derecha de \\(\\bar{x}\\)), pero \\(y - \\bar{y}\\) será negativo (todos están por debajo de \\(\\bar{y}\\)).\nEn el tercer cuadrante el producto de las distancias es positivo porque ambas distancias son negativas y en el cuarto vuelve a ser negativo ya que \\(y - \\bar{y}\\) es positivo pero \\(x - \\bar{x}\\) es negativo.\nCon \\(n\\) puntos, la suma de todos estos productos será \\(\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})\\). Si la mayoría se encuentra en los cuadrantes I y III, tal como ocurre en la figura 13.3, el resultado del sumatorio será un valor positivo, mientras que si están en los cuadrantes II y IV el resultado será negativo. Si no existe ninguna relación entre \\(X\\) e \\(Y\\) los puntos se repartiran de forma más menos equilibrada, tendiendo a compensarse los productos positivos con los negativos y dando un resultado alrededor de cero.\nLa covarianza es el valor de ese sumatorio dividido por el número de puntos que intervienen en su cálculo, algo así como el promedio de los productos \\((x_i -\\bar {x})(y_i -\\bar {y})\\). Si nuestro interés no es conocer la covarianza de los datos disponibles, sino estimar su valor en la población, dividimos por \\(n-1\\) en vez de por \\(n\\), por la misma razón que lo hacemos cuando estimamos el valor de la varianza. Como lo habitual es esto último, escribimos la fórmula de la forma:\n\\[ \\widehat{\\text{Cov}}(X,Y) = \\frac{\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})}{n-1} \\]\n\n\nPropiedades\nLa covarianza tiene un gran protagonismo en el terreno de los modelos teóricos, pero apenas se usa para valorar la relación lineal en un conjunto de datos. Para este menester tiene el inconveniente de que su valor depende de las unidades utilizadas.\nSi se calcula la covarianza entre la estatura de madres e hijas con los datos representados en la figura 13.1 cuyas unidadess son pulgadas (in) se obtiene un valor de 3,005 in2. Sin embargo, si cambiamos las unidades a cm (1 pulgada = 2,54 cm) el gráfico tiene el mismo aspecto y el grado de relación sigue siendo exactamente el mismo, pero ahora el valor obtenido es 19,39 cm2, y si las estaturas se expresan en metros tenemos que la covarianza es igual a 0,0019 m2.\nPor otra parte, para su valoración no tenemos ningún marco de referencia que nos permita evaluar si el valor obtenido es grande o pequeño. Todos estos problemas quedan resueltos con el uso del coeficiente de correlación.\n\n\n\n\n\n\nMalas noticias para la covarianza\n\n\n\nDados unos valores de \\(X\\), la máxima covarianza no se obtiene cuando los valores de \\(Y\\) se alinean según una recta. Por ejemplo, sean los valores de \\(X\\) = 1, 2, 3, 4 y 5, si los de \\(Y\\) son: 2, 4, 6, 8, y 10 (relación lineal perfecta) la covarianza entre \\(X\\) e \\(Y\\) es igual a 5 pero si sustuimos el último 10 por 15, la covarianza aumenta y pasa a valer 7,5. Esto no deja en muy buen lugar a la covarianza como medida de relación lineal."
  },
  {
    "objectID": "61_correlacion.html#coeficiente-de-correlación",
    "href": "61_correlacion.html#coeficiente-de-correlación",
    "title": "13  Medidas de relación lineal",
    "section": "13.3 Coeficiente de correlación",
    "text": "13.3 Coeficiente de correlación\nPara calcular el coeficiente de correlación, \\(r\\), entre dos variables \\(X\\) e \\(Y\\) basta con calcular su covarianza y dividirla por el producto de las desviaciones típicas de \\(X\\) e \\(Y\\), es decir:\n\\[ r  = \\frac{\\frac{\\sum_{i=1}^{n} (x_i -\\bar {x})(y_i -\\bar {y})}{n-1}}\n        {\\sqrt \\frac {{\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2}}{n-1} \\sqrt{ \\frac {\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 }{n-1}}} \\]\nEs fácil comprobar que desaparecen los denominadores tanto de la covarianza como de las desviaciones típicas, quedando:\n\\[ r  = \\frac{\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right ) \\left ( y_i - \\bar{y} \\right ) }\n        {\\sqrt {\\sum_{i=1}^n \\left ( x_i - \\bar{x} \\right )^2}  \\sqrt{\\sum_{i=1}^n \\left ( y_i - \\bar{y} \\right )^2 }} \\]\nCon esta sencilla transformación se resuelven los problemas de la covarianza:\n\nEs un valor adimensional y, por tanto, no depende de las unidades utilizadas.\nSu valor está acotado entre -1 y 1 (correlación perfecta negativa y positiva repectivamente).\n\nQue es adimensional es evidente, puesto que el numerador tiene las mismas unidades que el denominador. Que es igual a 1 o -1 cuando la relación lineal es perfecta lo demostramos en un apéndice de este capítulo usando álgebra elemental. Que está acotado entre -1 y 1 no es trivial, pero lo demostramos en un apéndice del siguiente capítulo usando el concepto de coeficiente de determinación de la recta ajustada (también se puede demostrar como consecuencia directa de la desigualdad de Cauchy-Schwarz, pero eso no son matemáticas elementales) .\n\n\n\n\n\n\nCoeficiente de correlación “de Pearson”\n\n\n\nA la denominación del coeficiente de correlación a veces se le añade “de Pearson” porque fue este estadístico el que lo desarrolló. Aunque existen otros coeficientes de correlación de mucho menor uso, no es necesario el añadido. Si no se dice de que tipo es siempre entendemos que se refiere al de Pearson.\n\n\n\nCoeficiente de correlación y diagrama bivariante\nEl valor del coeficiente de correlación no sustituye la información que proporciona el diagrama bivariante. Un mismo coeficiente de correlación puede corresponder a situaciones muy distitnas y si solo nos dan el valor de \\(r\\) es imposible saber a cual de ellas corresponde.\nUna demostración de la importancia de no descuidar el análisis gráfico de los datos lo constituye el llamado “cuarteto de Anscombe”, formado por cuatro conjuntos de datos que presentan el mismo coeficiente de correlación y la misma recta ajustada pero que al analizarlos gráficamente muestran situaciones claramente distintas (figura 13.4).\n\n\n\nFigura 13.4: Cuarteto de Anscombe. Cuatro situaciones muy distintas pero todas ellas con el mismo coeficiente de correlación (\\(r = 0,816\\)) y la misma recta ajustada (\\(y = 3+0,5x\\))\n\n\n\n\nCorrelación estadísticamente significativa\nEl diagrama bivariante de la figura 13.5 está construido con los datos de 29 plantas y muestra la relación entre el peso de los frutos obtenidos y el tiempo transcurrido entre la plantación y la recolección. Se ha añadido la recta de regresión ajustada a estos puntos.\n\n\n\nFigura 13.5: Relación entre el peso de los frutos recogidos en 22 plantas y el tiempo transcurrido desde su plantación hasta la recolección\n\n\nParece dar la sensación de que cuanto más se tarda en recoger los frutos mayor peso se obtiene. El coeficiente de correlación es positivo y está bastante alejado de cero, \\(r=0,323\\). ¿Quiere esto decir que para maximizar el peso de la cosecha vale la pena esperar a los 150 días?\nSi dos variables son totalmente independientes, como dos conjuntos de números aleatorios, no por ello hay que esperar que su coeficiente de correlación sea igual a cero. Para que esto ocurra también la covarianza debe ser igual a cero y para ello se debe dar un equilibrio perfecto en los puntos de cada cuadrante para que se compensen perfectamente los productos de las distancias, lo cual es muy difícil que se dé en la práctica.\nPor tanto, cuando tenemos dos conjuntos de datos que provienen de poblaciones independientes, no hay que esperar que su coeficiente de correlación sea exactamente igual a cero. Estará en torno a cero.\n¿Qué significa “en torno a”? La distancia que exigimes respecto a cero para tomarnos en serio la correlación depende del número de datos de que se disponga. Si solo tenemos dos datos (dos puntos sobre el diagrama) el coeficiente de correlación valdrá -1 o 1 con independencia de la relación que haya entre esas variables, por tanto, ese resultado no tiene ningún valor. Si se tienen pocos datos se exige más distancia que si se tienen muchos porque es más fácil que -por casualidad- se obtengan valores extremos.\nEn nuestro caso de 22 datos ¿cuál es la distancia exigida? Vamos a generar 22 números aleatorios de una distribución Normal con media \\(\\mu = 137\\) y \\(\\sigma = 6.5\\) que podemos considerar que son los parámetros de la población de la que provienen los valores de los pesos, y otros 22 números aleatorios, totalmente independientes de los anteirores, en este caso también de una distribución Normal, pero con parámetros \\(\\mu = 6\\) y \\(\\sigma = 0.5\\) para simular los valores del tiempo hasta la recogida. Cuando se calcula el coefiente de correlación con ese conjunto de datos simulados se obtiene un valor que corresponde a muestras de poblaciones independientes. Este proceso se puede repetir muchas veces y cada vez se obtiene un valor del coeficiente de correlación que corresponde a una situación de variables independientes.\nHemos repetido este proceso 100.000 veces, de manera que hemos obtenido 100.000 valores del coefiente de correlación entre dos conjuntos de datos de origen similar a los nuestros y que son absolutamente independientes. Los resultamos obtenidos se resumen en el histograma de la figura 13.6 ===Esto hay que explicarlo mejor===\n\n\n\nFigura 13.6: Valores del coeficiente de correlación obtenidas por simulación\n\n\nA la vista de este histograma, podemos afirmar que si nos hubiera salido un coeficiente de correlación de, por ejemplo 0,8, podríamos afirmar con un riesgo de equivocarnos muy pequeño que nuestras variables estan correlacionadas porque si fueran independientes un valor como ese o mayor prácticamente no se da nunca. Sin embargo, si nuesto valor fuera \\(r=0,2\\) no podríamos decir que existe correlación porque valores como ese, e incluso mayores, son muy habituales entre variables independientes cuando se calcula con muestras de \\(n=22\\) observaciones. ===todo esto hay que redactarlo mejor===\nEn nuestro caso tenemos \\(r=0,323\\) se trata de una distancia de cero “normal” si no hay relación entre ambas variables…\nHablar de las tablas, de sus características básicas y de que trataremos de como se han construido en el siguiente capítulo.\n===Esto hay que explicarlo mejor===\n\n\nCorrelación no implica relación causa-efecto\n===empezar comentando el gráfico===\nQue dos variable presenten un coeficiente de correlación estadísticamente significativo (esten correlacionadas) no significa que el cambio en una provoque -sea la causa- del cambio en la otra.\nPuede ocurrir que haya una tercera variable no considerada relacionada con las dos que se observan, por ejemplo, se dice que hay una alta correlación entre el número de bomberos que acuden a apagar un incendio y los daños que ese incendio causa, pero, naturalmente, a nadie se le ocurre enviar menos bomberos para que haya menor daños porque estas dos variables, aunque están correlacionadas y en el diagrama bivariante se observe una nube de puntos que marca una clara tendencia de que al aumentar el número de bomberos aumentan los daños, en este caso la variable oculta relacionada con las dos que observamos es la magnitud del incencdio, relacionada con los daños y con el número de bomberos. Existen numerosos ejemplos chistosos de este tipo de situaciones…\n\n\n\nFigura 13.7: Cuatro situaciones que muestran diferentes grados de relación\n\n\nTambién puede ocurrir que la relación sea debida al azar. Si exploramos muchos pares de variables, seguro que algunas apareceran con una relación estrecha, aunque en realidad no tengan nada que ver. La página web https://www.tylervigen.com/spurious-correlations contiene muchos ejemplos de este tipo.\n\n\nCuriosidad sobre el coeficiente de correlación\nSi solo tenemos n = 2 puntos sobre el diagrama bivariante, el coeficiente de correlación solo puede tomar los valores -1 y 1, ambos con la misma probabilidad . Si tenemos n = 3 aparece una distribución muy rara tanto para explicar la variabidad ligada a fenómenos naturales como en los modelos teóricos que usamos habitualmente: los valores más frecuentes están en los extremos (-1 y 1) mientras que en torno al centro (alrededor de cero) se dan los de menor probabilidad.Para n = 4 todos los valores del coeficiente de correlación son igualmente probables , para n = 5 la distribución de probabilidad tiene forma de semielipse, y a media que aumenta el valor de n va apareciendo la típica forma de campana. (figura).\n\n\n\nFigura 13.8: Distribución del coeficiente de correlación según el tamaño, \\(n\\), de la muestra considerada\n\n\nLas distribuciones de la figura +++ se puede reproducir por simulación o directamente usando la función densidad de probabilidad de coefiente de correlación, aunque su expresión es un poco aparatosa:\n\\[ f(r \\mid \\rho =0) = \\frac{ \\Gamma \\left [\\frac{1}{2} (n-1) \\right ]} { \\Gamma \\left [\\frac{1}{2} (n-2) \\right ] \\sqrt{\\pi}\n} (1-r^2)^{\\frac{1}{2}(n-4)} \\]\nAclarar la notación Esta función no converge a la distribución Normal cuando n se hace grande, ya que está definida solo entre -1 y 1 mientras que el dominio de la distribución Normal no está acotado."
  },
  {
    "objectID": "62_regresionSimple.html",
    "href": "62_regresionSimple.html",
    "title": "14  Regresión Simple",
    "section": "",
    "text": "Apéndice 10.1"
  },
  {
    "objectID": "62_regresionSimple.html#ajuste-a-una-recta",
    "href": "62_regresionSimple.html#ajuste-a-una-recta",
    "title": "14  Regresión Simple",
    "section": "14.1 Ajuste a una recta",
    "text": "14.1 Ajuste a una recta\nSi entre la respuesta y la variable regresora se observa una relación lineal se determina la ecuación de la recta que mejor se adapta a los puntos disponibles. Lo que significa “mejor” es discutible. Veamos algunas formas de hacerlo.\n\nA ojo\nSe traza la recta directamente sobre el papel o se identifican dos puntos de paso y a partir de ellos se calculan los coeficientes del modelo.\nA pesar de sus evidentes limitaciones, si solo se trata de tener la recta no es un método tan malo como parece. Con un poco de práctica el ajuste no será muy distinto del “perfecto” y no se cometeran errores de bulto debido a la presencia de valores anómalos, cosa que sí puede ocurrir si se tratan los datos de forma automática sin mnirarlos.\n\n\n\nFigura 14.2: Ajuste a ojo y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 14.1: Ajuste a ojo. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nIntuitivo. Muy fácil de entender.\nNo se comenten errores de mucho bulto.\n\n\n\nCONS \n\nNo se logra el ajuste “perfecto” de acuerdo con el criterio establecido. |\nNo se tienen medidas de calidad del ajuste. |\nSolo sirve para regresión simple.\n\n\n\n\n\n\n\nMétodo de Ishikawa\nSe identifica el primer y el tercer cuartil de los valores de \\(X\\) \\((X_{Q1}\\) y \\(X_{Q3})\\), e igual para los valores de \\(Y\\) \\((Y_{Q1}\\) y \\(Y_{Q3})\\). Se traza la recta por los puntos \\((X_{Q1}\\) y \\(Y_{Q1})\\) y \\((X_{Q3}\\) y \\(Y_{Q3})\\). Se obtiene una recta muy razonable sin necesidad de realizar cálculos ni de aplicar fórmulas de las que se desconoce su lógica.\n\n\n\nFigura 14.3: Ajuste por el método de Ishikawa y el que minimiza la suma de los cuadrados de los residuos.\n\n\n\n\nTabla 14.2: Método de Ishikawa. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nFácil de entender\nRobusto frente a la presencia de valores anómalos o con excesiva influencia\n\n\n\nCONS \n\nNo se tienen medidas de calidad del ajuste\nSolo sirve para regresión simple\n\n\n\n\n\n\n\n\n\n\n\nKaoru Ishikawa (1915-1989)\n\n\n\nFue un ingeniero japonés, considerado uno de los artífices del llamado “milagro japonés” que condujo los productos japoneses desde la mediocridad hasta arrasar en los mercados mundiales (electrónica, fotografía, automoción,…). Una de las claves del éxito fue el uso intensivo de técnicas estadísticas para el control y la mejora de la calidad. Ishikawa es conocido por proponer el uso de herramientas sencillas, que todos puedan entender y aplicar de forma habitual.\n\n\n\n\nHaciendo que la suma de los residuos sea igual a cero\nSe trata de obtener los valores de \\(b_0\\) y \\(b_1\\) que cumplen la expresión:\n\\[\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]= 0\\] qye es equivalente a:\n\\[ n\\bar{Y} - nb_0 - b_1 n \\bar{X} = 0\\] Por tanto, con cualquier par de valores \\(b_0\\) y \\(b_1\\) que verifiquen la expresión \\(\\bar{Y} = b_0 + b_1 \\bar{X}\\), es decir, con cualquier recta que pase por (\\(\\bar{X}\\), \\(\\bar{Y}\\)) tendremos una suma de residuos igual a cero.\nQue haya infinitas rectas que cumplan esa condición es una mala señal, porque seguro que no todas son adecuadas. Para los valores representados en la figura 14.4 tenemos que \\(\\bar{X}= 6\\) y \\(\\bar{Y}= 9\\). Rectas que hacen que la suma de los residuos sea igual a cero son, por ejemplo, la que tiene coeficientes \\(b_0=9\\) y \\(b_1=0\\), es decir: \\(Y = 9\\), o también \\(b_0 = 12\\) y \\(b_1 = -0.5\\), es decir: \\(Y = 12 -0.5X\\) y ambos son claramente muy malos ajustes.\n\n\n\nFigura 14.4: Dos ajustes -claramente muy malos- con suma de residuos igual a cero.\n\n\n\n\nTabla 14.3: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nNinguna\n\n\n\nCONS \n\nDa un número infinito de soluciones (una de ellas coincide con el ajuste por mínimos cuadrados)\n\n\n\n\n\n\n\nMinimizando la suma del valor absoluto de los residuos\nSe trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left| Y_i - (b_0 - b_1 X_i) \\right|= 0\\]\nPuede no tener solución única, pero los resultados posibles son mucho más razonables que en el caso anterior. Un problema específico de este caso es que no existen expresiones analíticas para los coeficientes debido a las dificultades en el manejo de la función “valor absoluto”.\nLa figura 14.5 muestra dos diagramas con los mismos 4 puntos y ñas rectas que cumplen el criterio estableciso, en todas ellas la suma del valor absoluto de los residuos es igual a 2. La línea azul, que es la misma en los dos diagramas, es la que también minimiza la suma de los cuadrados de los residuos.\n\n\n\nFigura 14.5: Posibles opciones para minimizar la suma de los residuos en valor absoluto.\n\n\n\n\nTabla 14.4: Suma de los residuos igual a cero. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nPoco sensible a la presencia de valores anómalos\n\n\n\nCONS \n\nNo da un ajuste equilibrado entre todos los puntos\nNo existe expresión analítica para el cálculo de los coeficientes\n\n\n\n\n\n\n\n\nMinimizando la suma de los cuadrados de los residuos\nElevamos los residuos al cuadrado en vez de usar su valor absoluto para evitar que al sumarlos se compensen los positivos y negativos. Ahora se trata de minimizar:\n\\[S=\\sum_{i=1}^n \\left[ Y_i - (b_0 - b_1 X_i) \\right]^2= 0\\]\nEn vez de decir que el criterio de ajuste ha sido “minimizar la suma de los cuadrados de los residuos”, decimos que hemos ajustado por “mínimos cuadrados”. Este es el método usado en la inmensa mayoría de los casos, produce un ajuste equilibrado de la nube de puntos y está en perfecta sintonía con otras técnicas y medidas, que se contruyen en torno a criterios similares. Un inconveniente de este método de ajuste es que el modelo obtenido es sensible a la presencia de valores anómalos, cosa que no ocurre si se minimiza la suma de valores absolutos.\nEn la primera fila de la figura 14.6 tenemos una situación típica en que el ajuste por mínimos cuadrados da un mejor resultado que minimizando la suma del valor absoluto. Sin embargo, en la segunda fila tenemos un cado de puntos perfectamente alineados excepto un que muy probablemente sería un valor anómalo. Si minimizamos el valor aboluto de los residuos el ajuste ignora el valor anómalo mientras que ajustado por mínimos cuadrados el valor anómalo tiene una notable influencia sobre la recta ajustada.\n\n\n\nFigura 14.6: Ajustes obtenidos minimizando la suma de valores absolutos y la suma de los cuadrados de los residuos\n\n\n\n\nTabla 14.5: Minimizar la suma de los cuadrados de los residuos. Ventajas e inconvenientes\n\n\n\n\n\n\nPROS \n\nProporciona un ajuste muy razonable. El que queremos hacer cuando ajustamos a ojo.\nEncaja perfectamente con el resto de técnicas estadísticas que utilizamos.\n\n\n\nCONS \n\nLos valores anómales (errores o valores singulares) pueden tener bastante influencia sobre el modelo estimado. Hay que estar atentos para tratar esos puntos adecuadamente"
  },
  {
    "objectID": "62_regresionSimple.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "href": "62_regresionSimple.html#mínimos-cuadrados.-cálculo-de-los-coeficientes",
    "title": "14  Regresión Simple",
    "section": "14.2 Mínimos cuadrados. Cálculo de los coeficientes",
    "text": "14.2 Mínimos cuadrados. Cálculo de los coeficientes\nExiste una fórmula cerrada y con solución única para cada coeficiente, pero vamos a empezar identificando el valor de los coeficientes sin hacer uso de las fórmulas. Naturalmente, es mucho más rápido y más práctico usarlas o -mejor todavía- usar un paquete de software estadístico o una hoja de cálculo, pero hacerlo sin fórmulas permite entender perfectamente qué es lo que se está haciendo, y también descubrir algún detalle interesante.\n\nSin fórmulas\nRealizamos a ojo una primera estimación del valor de los coeficientes. A continuación, mediante un pequeño programa -o también usando una hoja de cálculo-, hacemos un barrido de los valores de \\(b_0\\) y \\(b_1\\) en torno a los estimados, identidicando el par que minimiza la suma de los cuadrados de los residuos.\nVayamos al diagrama de la figura 14.7 (izquierda) que ya habíamos visto en las figuras 14.2 y 14.3. La recta ajustada a ojo pasa por los puntos (-4,75; 0) y (5,75, 60) por lo que sus coeficientes son: \\(b_1\\) = 5,71 y \\(b_0\\) = 27,14. Sería mucha casualidad que esos fueran los valores exactos que estamos buscando, pero no andarán muy lejos. Vamos a crear una malla de valores de \\(b_0\\) y \\(b_1\\). Los valores de \\(b_0\\) variarán de 2 a 8 con incrementos de 0,1 y para cada uno de esos, los de \\(b_0\\) irán de 20 a 35 también en saltos de 0,1. A cada combinación de esos dos valores corresponde a una recta, y a cada recta una suma de los cuadrados de los residuos. El par de valores que minimizan esa suma de cuadrados son: \\(b_0\\) = 27,0 y \\(b_1\\) = 4,8.\n\n\n\nFigura 14.7: Recta ajustada a ojo (izq.) y suma de los cuadrados de los residuos para cada par de valores \\(b_0\\) y \\(b_1\\). En rojo, valores que la minimizan (der.).\n\n\n\n\n\n\n\n\nParaboloide de la suma de cuadrados\n\n\n\nCon los datos de nuestro ejemplo, la superficie que representa la suma de los cuadrados de los residuos es un paraboloide donde la localización del mínimo es visulamente muy clara. Pero lo nomal es que las curvas de nivel sean muy elípticas de manera que la representación no queda tan clara. Nosotros hemos logrado esa forma regular haciendo que la media de los valores de \\(X\\) sea igual a cero. De esta forma, los coeficientes son independientes y las curvas de nivel apararecen como círculos prácticamente concéntricos quedando más clara la idea que queremos representar.\n\n\n\n\nUsando las fórmulas\nEn el diagrama que representa la relación entre \\(X\\) e \\(Y\\) cada punto puede ser identificado por sus coordenadas \\((x_i, y_i)\\) con \\(1 \\leq i \\leq n\\) siendo \\(n\\) el número total de puntos. [creo que esto es redundante y habría que mejorarlo]\nCada uno de los puntos tiene un residuo asociado \\(e_i\\) y ese residuo es la diferencia entre el valor real de \\(y\\), es decir, \\(y_i\\) y su valor estimado \\(\\hat{y}_i\\), el que estará sobre la recta y que será igual a \\(b_0 + b_1 x_i\\). Por tanto, el valor del residuo asociado al punto \\(i\\) lo podemos escribir de la forma:\n\\[ e_i = y_i - \\left( b_0 + b_1 x_i \\right) \\] Por tanto, la suma de los cuadrados de los residuos, \\(S\\), será:\n\\[ S = \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right )^2 \\]\nTanto los valores de \\(y_i\\) como los de \\(x_i\\) vienen dados. La suma de cuadrados \\(S\\) es función de los valores de \\(b_0\\) y de \\(b_1\\). Se trata de hallar los valores de \\(b_0\\) y de \\(b_1\\) que minimizan esa suma de cuadrados. El mínimo lo tendremos en el punto en que la derivada de \\(S \\left(b_0, b_1 \\right )\\) respecto a \\(b_0\\) y respecto a \\(b_1\\) es igual a cero. Seguro que es un mínimo porque el máximo no está definido.\n\\[ \\frac{\\partial S}{\\partial b_0} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) \\]\n\\[ \\frac{\\partial S}{\\partial b_1} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) x_i \\]\nIgualando a cero estas expresiones:\n\\[ \\sum_{í=1}^n y_i - nb_0 - b_1 \\sum_{í=1}^n x_i = 0  \\tag{14.1}\\]\n\\[ \\sum_{í=1}^n x_i y_i - b_0 \\sum_{í=1}^n x_i - b_1 \\sum_{í=1}^n  x_i^2  = 0  \\tag{14.2}\\]\nDividiendo por \\(n\\) todos los términos de la ecuación 14.1 tenemos:\n\\[ b_0 = \\bar{y} - b_1 \\bar{x} \\]\n\n\n\n\n\n\nLa recta ajustada pasa por el punto \\((\\bar{x}, \\bar{y})\\)\n\n\n\nDe la anterior expresón para \\(b_0\\) también se decude que \\(\\bar{y} = b_0 + b_1\\bar{x}\\). Es decir, la recta ajustada minimizando la suma de los cuadrados de los residuos siempre pasa por el punto \\((\\bar{x}, \\bar{y})\\) .\n\n\nSustituyendo la expresión de \\(b_0\\) en la ecuación 14.2 tenemos:\n\\[ \\sum_{í=1}^n x_i y_i - \\bar{y} \\sum_{í=1}^n x_i +  b_1\\bar{x} \\sum_{í=1}^n x_i- b_1 \\sum_{í=1}^n  x_i^2  = 0 \\]\nPara aligerar la notación no pondremos los límites a los sumatorios, que siempre son desde \\(i=1\\) hasta \\(n\\). Despejando \\(b_1\\) llegamos a:\n\\[ b_1 = \\frac{\\sum x_i y_i - \\bar{y} \\sum x_i}{\\sum x_i^2 - \\bar{x} \\sum x_i} \\] También la expresión de \\(b_1\\) se suele dar de la forma (ver Apéndice 10.1):\n\\[ b_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}  \\tag{14.3}\\]\nA partir de la ecuación 14.3 y recordando las expresiones de la covarianza y del coeficiente de correlación, llegamos a una expresión que también se ve con frecuencia en los libros de texto, seguramente porque una calculadora sencilla da directamente los tres valores que intervienen:\n\\[ b_1 = \\frac{Cov(XY)}{s_X^2} = \\frac{r_{XY} s_X s_Y}{s_X^2} = r_{XY} \\frac{s_Y}{s_X} \\] Calculando los coeficientes que corresponden a los datos de la figura 14.7 se obtiene:\n\\[ b_0 = 26,9615 \\qquad  \\qquad b_1 = 4,8616 \\] Ahora sí, con todos los decimales que queramos, aunque dar más decimales de los que tienen los datos es añadir números que no aportant ninguna información, dan una falsa sensación de precisión y complican la lectura del resultado.\n\n\n\n\n\n\n\nPor qué le llamamos modelo de regresión\n\n\n\nSean, por ejemplo, los puntos: (4; 3), (6; 8), (8; 12), (10; 10), (12; 12). La recta ajustada es: \\(y = 1+x\\). Si en vez de ajustar \\(y = f(x)\\) se ajusta \\(x=f(y)\\) ¿se obtendrá la ecuación resultante de despejar \\(x\\) en \\(y=f(x)\\), es decir: \\(x = -1 + y\\)? Si ajustamos \\(x=f(y)\\) la ecuación será: \\(x=1,57+0,714x\\). No es lo mismo minimizar la suma de los cuadrados de los residuos medidos en dirección vertical que en dirección horizontal (esto último no son los residuos)."
  },
  {
    "objectID": "62_regresionSimple.html#calidad-del-ajuste",
    "href": "62_regresionSimple.html#calidad-del-ajuste",
    "title": "14  Regresión Simple",
    "section": "14.3 Calidad del ajuste",
    "text": "14.3 Calidad del ajuste\nEl gráfico de la izquierda de la figura 14.8 muestra la relación entre la longitud de la circunferencia (X) de los troncos de un determinado tipo de árbol y el volumen de madera (Y) que se puede obtener de ellos (Fuente: Wolfram_Data_Repository 2016). Se observa que a más circunferencia mayor volumen de madera, tal como era de esperar, y la ecuación de la recta ajustada es útil para estimar cuanta madera se obtendrá de un tronco de determinado diámetro. Sin embargo, el gráfico de la derecha se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables. En este caso el modelo ajustado no sirve absolutamente para nada. Pero los dos modelos tienen el mismo aspecto y solo a la vista del valor de sus coeficientes es imposible saber cual de los dos es útil.\n\n\n\nFigura 14.8: Relación muy clara y relación inexistent, situaciones que no pueden distinguirse solo a la vista del modelo ajustado.\n\n\nEs necesario, por tanto, completar el modelo con una medida que informe de la calidad del ajuste obtenido. Esa medida es el coeficiente de determinación \\(R^2\\).\nPara calcular el valor de \\(R^2\\) empezamos poniéndonos en el peor de los casos: suponemos que \\(X\\) e \\(Y\\) son independientes, es decir, que el valor de \\(X\\) no aporta ninguna información sobre el valor de \\(Y\\). En este caso, la recta que muestra la relación entre ambas variables es una recta horizontal: la estimación del valor de \\(Y\\) es siempre la misma, sin importar el valor de \\(X\\), y la mejor apuesta para ese valor de \\(Y\\) -a falta de cualquier otra información- es su valor medio \\(\\bar{y}\\). A la suma de los cuadrados de los residuos correspondientes a esa recta horizontal que pasa por \\(\\bar{y}\\) le llamamos \\(Q_Y\\).\nA continuación calculamos la suma de los cuadrados de los residuos correspondientes a nuestra recta ajustada (la que minimiza la suma de los cuadrados de los residuos) y le llamaremos \\(Q_R\\). Cuanto mejor sea el ajuste menor será el valor de \\(Q_R\\) y mayor la diferencia entre \\(Q_Y\\) y \\(Q_R\\).\nEl valor de \\(R^2\\) es igual a la proporción de \\(Q_Y\\) explicada por \\(X\\), es decir, la proporción en que disminuye \\(Q_Y\\) gracias a la introducción de \\(X\\) como variable explicativa, es decir:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} \\]\nVeamos este cálculo en un ejemplo con datos sencillos. En la figura 14.9 tenemos 5 puntos que podrían representar la relación entre el peso y la estatura de 5 individuos. Si, ignorando la información aportada por la estatura, siempre damos una estimación del peso igual a su valor medio, será como ajustar a una recta horizontal y tendremos una suma de los cuadrados de los residuos \\(Q_Y = 56\\). Sin embargo, si utilizamos la información que aporta la estatura y realizamos el ajuste minimizando la suma de los cuadrados de los residuos tenemos \\(Q_R = 16\\).\n\n\n\nFigura 14.9: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\nHemos reducido la suma de los cuadrados de los residuos de 56 a 16, por tanto:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} = \\frac{56 - 16}{56} = 0.7143 \\]\nNormalmente nos referimos a este valor como un porcentaje. En este caso sería el 71.43%. En los ejemplos de la figura 14.8 estos valores son del 93,5% (volumen de madera) y 1,5% (edad al morir).\n\n\n\n\n\n\n\\(R^2\\) es el cuadrado del coeficiente de correlación \\(r\\)\n\n\n\nEfectivamente, en el caso que estamos considerando de regresión simple, el coeficiente de determinación \\(R^2\\) es igual al cuadrado del coeficiente de correlación \\(r\\). Este último puede variar entre \\(-1\\) y \\(1\\) por lo que, obviamente, \\(R^2\\) varía entre 0 y 1. La demostración es corta y fácil de encontrar en internet. Por ejemplo en:"
  },
  {
    "objectID": "62_regresionSimple.html#relación-no-lineal-entre-x-e-y",
    "href": "62_regresionSimple.html#relación-no-lineal-entre-x-e-y",
    "title": "14  Regresión Simple",
    "section": "14.4 Relación no lineal entre \\(X\\) e \\(Y\\)",
    "text": "14.4 Relación no lineal entre \\(X\\) e \\(Y\\)\nSi a la vista del diagrama bivariante se observa que la relación entre \\(X\\) e \\(Y\\) no es lineal, se puede utilizar el aspecto de la nube de puntos y el conocimiento del fenómeno que se estudia para plantear un modelo que se ajuste a los datos. Los modelos polinómicos de segundo grado son muy versátiles y pueden ser una buena opción. También se puede ajustar a modelos linealizables transformando los valores de \\(X\\), los de \\(Y\\), o ambos. Si nuestros datos se ajustan a una función del tipo \\(y = \\beta_0 e^{\\beta_1 x}\\), podemos realizar el cambio \\(y' = \\ln y\\) obteniendo el modelo lineal: \\(y' = \\ln \\beta_0 + \\beta_1 x\\) a partir del cual se deducen de forma inmediata los coeficientes del modelo original. Interesados en este tipo de transformaciones para linealizar la dependencia pueden consultar Montgomery y Peck (1992) pág. 90. o Peña (2002) pág. 314.\nLa figura 14.10 (izquierda) muestra los datos de producción de electricidad de un aerogenerador según sea la velocidad del viento (datos en: Montgomery y Peck (1992), pág. 92). Se observa una relación no lineal ya que cuando la velocidad del viento es baja, pequeños incrementos en la velocidad tienen un impacto importante en la producción de electricidad, mientras que para velocidades altas la producción tiende a estabilizarse. Ajustando a una parábola se obtiene \\(y = -1,156 + 0,7229x -0,03812x^2\\) con un coeficiende de determinación \\(R^2 = 96,8%\\), lo cual no está nada mal.\nOtra opción es estudiar la producción de electricidad en función de la inversa de la velocidad del viento (figura de la derecha). Creamos la variable \\(X' = 1/X\\) y obtenemos el ajuste: \\(y = 2,979 - 6,935/x\\) con un \\(R^2 = 97,9%\\) que es también un valor excelente y, además, con un modelo más compacto. En general, trabajar con la inversa de \\(X\\) puede ser una buena alternativa al modelo cuadrático.\n\n\n\nFigura 14.10: Texto figura.\n\n\nHay que tener en cuenta que el modelo más adecuado no necesariamente es el que tiene el \\(R^2\\) más elevado. Nos interesa que el modelo sea compacto y que pueda interpretarse y sea coherente con nuestro conocimiento del fenómeno en estudio. Si vamos aumentando el grado del polinomio ajustado cada vez tendremos un mayor valor de \\(R^2\\) incluso, si tenemos pocos datos, podemos llegar a un \\(R^2\\) del 100% siendo el modelo obtenido totalmente inútil.\nVolviendo a los datos de la figura 14.9 donde a partir de los pesos de 5 personas (efectivamente son muy pocas, es solo un ejemplo) queremos modelar la relación entre peso y estatura, el modelo lineal es el más razonable. Si ajustamos los datos a un modelo cuadrático se tiene un máximo de peso en torno a una estatura de 175 cm que no tiene sentido. El polinomio de tercer grado presenta una forma que tampoco parece razonable y el de cuarto grado es un modelo con 5 parámetros (los 4 coeficientes y la ordenada en el origen) y como tenemos 5 puntos ajusta perfectamente, pero ni es un modelo razonable ni sirve en absoluto para estimar el peso de un individuo a partir de su altura (sí lo explica para los 5 individuos usados para construir el modelo, pero para esos ya lo sabíamos). Recuerde que dos puntos se ajustan perfectamente a un modelo con dos parámetros (una recta) tres puntos a un modelo con tres parámetros, … etc. Estos son modelos que explican muy bien lo que ya se sabe, pero son totalmente inútiles para hacer predicciones que es lo que -en general- se pretende.\n\n\n\nFigura 14.11: Aumentando el grado del polinomio la curva de adapta a los puntos pero solo explica lo que ya sabemos."
  },
  {
    "objectID": "62_regresionSimple.html#transformación-logarítmica",
    "href": "62_regresionSimple.html#transformación-logarítmica",
    "title": "14  Regresión Simple",
    "section": "14.5 Transformación logarítmica",
    "text": "14.5 Transformación logarítmica\nEn algunos casos, los valores de \\(X\\), los de \\(Y\\), o ambos, siguen una distribución asimétrica, con valores que aparecen agrupados cerca del origen y muy dispersos hacia los valores altos. Un ejemplo típico de esta situación se da al analizar la relación entre el peso del cerebro y el peso de cuerpo en 62 especies de mamíferos (Weisberg 2014, pág. 186). La mayoria de esos mamíferos pesan poco –la mediana es de 3,34 kg– pero algunos, como los elefantes, pesan varias toneladas y algo similar ocurre con el peso de los cerebros. Al realizar el diagrama bivariante del peso del cerebro (\\(Y\\)) frente al peso de cuerpo (\\(X\\)) prácticamente todos los puntos aparecen amontonados en la zona próxima al origen, En estas condiciones ajustar un modelo de regresión no tiene sentido, porque la mayoría de datos actúan como un solo punto y los que estan alejados tienen una gran influencia sobre la recta ajustada.\n\n\n\nFigura 14.12: Relación entre el peso del cerebro y el peso del cuerpo en 62 especies de mamíferos.\n\n\nUno puede caer en la tentación de considerar a los elefantes como valores anómalos y eliminarlos, pero esa no es una buena decisión por dos razones:\n\nRestringe la validez del modelo, ya no valdrá para todos los mamíferos considerados.\nAl eliminar esos valores y reescalar el gráfico aparecen otros valores anómalos: la persona humana (que da más reparo eliminar), la jirafa, el caballo, la vaca… y al final nos vamos quedando sin puntos.\n\nEn casos como este, la transformación logarítmica “estira” los datos permitiendo un ajuste en el que todos los puntos tienen una influencia similar. Realizando esta transformación en nuestros datos se obtiene -casi parece un milagro- una nube de puntos tal como esperamos tener cuando ajustamos a una recta.\n\n\n\nFigura 14.13: Peso del cerebro frente al peso del cuerpo antes y después de la transformación logarítmica.\n\n\nEl modelo obtenido es:\n\\[\\log(Y) = 0,9271 + 0,7517 \\log(X) \\quad \\text{con} \\quad R^2 = 91,95\\%\\] Volviendo a las variable soriginales nos queda (el cuerpo está en kg y el del cerebro en g):\n\\[Y = 8,45 · X^{3/4}\\] La transformación logarítmica de los datos es, sin duda, una buena opción en casos como este, pero también tiene efectos secundarios no deseados.\nEn primer lugar hay que tener en cuenta que los residuos (diferencia entre el valor real y el valor previsto) también están en escala logarítmica. POr ejemplo, para el elefante africano (el mayor, parece que la recta pasa por el punto) el valor real del peso del cerebro es de 5712 g y la previsión es de 6229 (+9%) y para el elefante asiático el valor real es de 4603 g mientras que el valor previsto es de 3031 g (-34%). El mamífero que presenta mayor residuo positivo es la persona humana (valor real: 1320, previsto: 185, -86%) mientras que el de mayor residuo negativo corresponde al Yapok (en inglés: Water opossum). Seguramente más interesante que el modelo en sí es conocer qué animales se separan más -por encima y por debajo- del patrón general. Sobre este tema existen muchas publicaciones. Los interesados pueden empezar explorando la Wikipedia y las referencias que incluye.\n\n\n\n\n\n\nTransformación logarítmica: No importa la base\n\n\n\nEn efecto, sea \\(y = \\ln (x)\\) y \\(z = \\log_{10}(x)\\). Tendremos que \\(e^y = x\\) y también que \\(10^z = x\\), luego \\(e^y = 10^z\\). Por tanto, \\(\\ln(e^y) = \\ln(10^z)\\) y es inmediato que: \\(y = \\ln(10)·z\\).\nPor tanto, cambiar la base del logaritmo equivale a multiplicar por una constante. En particular, para pasar del logaritmo neperiano al decimal basta con multiplicar por \\(\\ln(10)\\). El aspecto del diagrama bivariante es el mismo con independencia de la base utilizada para la transformación logarítmica, solo cambian las escalas, aunque para que al volver a las variables originales la expresión sea más compacta puede interesar elegir una base u otra.\n\n\nPosibilidad de usar los datos de Hooker, parece relación lineal pero no lo es y la transformación logarítmica da buen resultado"
  },
  {
    "objectID": "62_regresionSimple.html#las-cosas-se-complican-lo-que-tenemos-es-una-muestra",
    "href": "62_regresionSimple.html#las-cosas-se-complican-lo-que-tenemos-es-una-muestra",
    "title": "14  Regresión Simple",
    "section": "14.6 Las cosas se complican: Lo que tenemos es una muestra",
    "text": "14.6 Las cosas se complican: Lo que tenemos es una muestra\nLa interpretación de los resultados se complica cuando caemos en la cuenta de que los datos disponibles son solo una muestra de la población de interés. Supongamos que deseamos estudiar la relación entre peso y estatura en los jóvenes de cierta edad (haríamos bien en separar hombres y mujeres, pero aquí vamos a ignorar ese aspecto que trataremos en el siguiente capítulo) y que disponemos de una muestra de -pongamos- 20 jóvenes. Con los datos de esa muestra ajustamos una recta pero, en realidad, esa no es la recta que andamos buscando. Si hubiéramos tomado otra muestra la recta sería otra -distitnta- pero tan válida como la primera. Entonces, ¿cómo se interpreta la recta obtenida?\nComo en otros casos, una forma de ver lo que ocurre es simulando. En los diagramas de la figura 14.14 las estaturas (\\(X\\)) se han generado aleatoriamente de una distribución N(170; 8) y a cada estatura se le ha asignado un peso (\\(Y\\)) mediante la expresión \\(Y = X -100 +e\\), donde \\(e\\) un valor también generado aleatoriamente de una distribución N(0; 5). Tanto los valores de la estatura (en cm) como los obtenidos para los pesos (en kg) son valores razonables para una población joven. Hemos repetido la simulación 6 veces y, como es natural, cada vez hemos obtenido unos datos distintos y, por tanto, también una recta ajustada distinta.\n\n\n\nFigura 14.14: Rectas ajustadas a partir de muestras de n=20 datos de una misma población. La línea negra representa el modelo teórico\n\n\nEn la figura 14.15 (izq.) se han superpuesto los 6 diagramas anteriores pudiéndose observar el haz de rectas que se obtiene. A la derecha tenemos la misma situación superponiendo 50 simulaciones (cada una con 20 datos) añadiendo, de color verde, la recta que representa el modelo teórico, es decir, la población.\n\n\n\nFigura 14.15: Superposición de 6 (izq.) y 50 (der.) simulaciones de conjuntos de 20 datos del modelo representado con una línea verde en el gráfico de la derecha.\n\n\n\nDistribución de los coeficientes\nLa buena noticia es que si los datos cumplen unas ciertas condiciones –que en general se cumplirán– los valores de los coeficientes pertenecen a distribuciones Normales con parámetros conocidos. Siguiendo con el ejemplo anterior hemos repetido 10.000 veces la simulación obteniendo otras tantas rectas ajustadas. La 14.16 muestra los histogramas de los 10.000 valores obtenidos para \\(b_0\\) y \\(b_1\\).\n\n\n\nFigura 14.16: Distribución de los coeficientes.\n\n\nObserve que las medias de las distribuciones coinciden con verdadero valor del parámetro estimado (estamos de suerte). Las desviaciones típicas dependen de:\n\nNúmero de datos: Cuanto más datos mayor información y menos incertidumbre, por tanto, menos desviación típica en la distribución de los coeficientes.\nDesviación típica de la respuesta: A mayor variabilidad de la respuesta mayor incertidumbre y mayor variabilidad en la distribución del los coeficientes.\nEl rango de variación de los valores de la variable regresora: Si los valores de \\(x\\) están muy próximos a su media habrá mayor variabilidad en los distribución de los coeficientes. Quizá este aspecto no es tan intuitivo como los anteriores, pero se entiende muy bien a la vista de un gráfico como el de la figura 14.16. En la izquierda tenemos el mismo gráfico que en la figura 14.15 con valores de X generados de una distribución N(170; 8) mientras que en el de la derecha se ha construido de la misma forma pero los valores de X se han generadod de una N(170; 3). Al tener menos variabilidad los valores de X tenemos mayor variabilidad en los valroes de los coeficientes.\n\n\n\n\nFigura 14.17: Distribución de los coeficientes.\n\n\nConocer la distribución de los coeficientes hace posible calcular intervalos de confianza o realizar constrastes de hipótesis sobre sus valores.\n\n\nCondiciones que deben reunir los datos\nPara que los coeficientes tengan las distribuciones descritas los datos utilizados para ajustar el modelo deben cumplir las siguientes condiciones:\n\nDistribución de \\(Y\\): Dado un valor de \\(X\\), los valores de \\(Y\\) deben seguir una distribución Normal. Si \\(X\\) es la estatura e \\(Y\\) es el peso, no hace falta suponer que el peso –globalmente– sigue un distribución Normal, pero sí que los pesos para las personas de una determinada estatura siguen esa distribución.\n\n\n\nVariabilidad de \\(Y\\): La variabilidad de \\(Y\\) no depende del valor de \\(X\\). En nuestro ejemplo sería suponer que la variabilidad en el peso de las personas que miden 1,60 es la misma que en las personas que miden 1,80 m. Es posible que esto no sea exactamente así porque es habitual que cuando aumenta el nivel de la respuesta aumente también su variabilidad. Si esto ocurre lo veremos en el diagrama bivariante: la nube de puntos se irá ensanchando a medida que aumenta el valor de \\(X\\). En este caso quizá convenga transformar los datos, aunque ya estaríamos ante una situación más complicada que las que pretendemos tratar aquí.\nValores de \\(X\\): No hay ninguna exigencia especial sobre estos valores. Solo es necesario que la variable sea cuantitativa. Es decir, el día de la semana, codificado como: lunes = 1, martes = 2, … no puede ser una variable regresora porque el modelo entedería que el domingo es igual a 7 veces el lunes. Aun así, también hay formas de incluir este tipo de variables. Lo veremos en el próximo capítulo en el caso de que solo puedan tomar dos valoros posibles.\nIndependencia de los residuos: La desviación respecto al valor previsto (valor sobre la recta) en en un punto no da ninguna pista sobre la desviación en el punto siguiente. Esto no ocurre con las variables que evolucionan en el tiempo, como la temperatura o la cotización de acciones en la bolsa, en que el valor de un día está influenciado por el valor de día anterior.\n\nCuando se ajustan modelos de regresión simple, la observación del diagrama bivariante de \\(Y\\) frente a \\(X\\) ya permite valorar si es razonable suponer que se cumplen los supuestos requeridos. Si nada hace suponer lo contrario, supondremos que se cumplen. En realidad nunca se cumpliran “exactamente” pero si el comportamiento de los datos no se aleja mucho de los supuestos realizados, los intervalos de confianza y las pruebas de significación en que estamos interesdos seguirán siendo válidos a efectos prácticos.\n\n\nSignificación de los coeficientes. Visión intuitiva\nUsaremos los datos que ya vimos (apartado 14.3 ) sobre el volumen de madera que se obtiene de un árbol en función del diámetro de su tronco y sobre la edad al morir en función de la longitud de una línea de la mano. La figura 14.18 contiene las rectas ajustadas y también los valores que usaremos para la simulación.\n===No sé si esto se puede meter en algún sitio: Aunque a la vista de los gráficos ya se ve muy claro que el caso del volumen de madera la pendiente es significativa y en el de la línea de la mano seguramente no lo es, vamos a comprobar por simulación que, efectivametne, es así===\n\n\n\nFigura 14.18: Modelos ajustados. Se indica el número de datos (\\(n\\)), la desviación típica de los residuos (\\(s_R\\)), la media y la desviación típica de los valores de \\(X\\) (\\(\\bar{x}\\) y \\(s_X\\)) y la media de los valores de Y (\\(\\bar{y}\\)).\n\n\nSi no hay ninguna relación entre la variable regresora (\\(X\\)) y la respuesta (\\(Y\\)), la pendiente de la recta estará en torno a cero (será igual a cero en la población, pero no disponemos de esos datos que –además– son solo un modelo teórico). Para ver lo que significa “en torno a cero” en los casos que estamos tratando, podemos simular nubes de puntos manteniendo las características de \\(X\\) e \\(Y\\) de forma independiente, suponiendo que no existe ninguna relación entre ellas.\nEn el caso del volumen de madera empezamos generando 31 números aleatorios (una cantidad igual a nuestro tamaño de muestra) que asignamos al diámetro de los troncos. Para que estos datos se puedan considerar del mismo tipo de los que tenemos en la muestra es razonable generarlos de una distribución Normal con la media \\((\\bar{x} = 13.25)\\) y la desviación típica \\((s_X = 3.2)\\) de los valores que aparecen en la muestra, es decir: \\(X \\sim N(13.25; 3.2)\\). A continuación, a cada valor de \\(X\\) le hacemos corresponder un valor de \\(Y\\) igual a la media de los valores disponibles \\((\\bar{y} = 30.2)\\) añadiendo un número aleatorio de una distribución Normal con media cero (ni sube ni baja de forma sistemática el valor de la respuesta) y una desviación típica igual a la que presentan los residuos del modelo ajustado \\((s_R = 4.25)\\). Por tanto, tendremos: \\(Y = 30.2 + N(0; 4.25)\\).\nPara el modelo de la edad al morir hacemos exactamente lo mismo con los valores que corresponden. La tabla 14.6 contiene los usados en cada caso.\n\n\nTabla 14.6: Valores usados para la simulación.\n\n\n\n\n\n\n\n\n\n\\(n\\)\nValores de \\(X\\)\nValores de \\(Y\\)\n\n\n\n\nVolumen de madera:\n31\n\\(N(13.25; 3.2)\\)\n\\(30.2 + N(0; 4.25)\\)\n\n\nEdad al morir\n50\n\\(N(13.25; 3.2)\\)\n\\(66.7 + N(0; 14.15)\\)\n\n\n\n\nPara cada conjunto de puntos simulados se ha calculado la recta ajustada. En los dos casos la simulación se ha repetido 50 veces y en los diagramas de la figura 14.19 se han ido acumulando tanto los puntos como las rectas ajustadas. También se ha añadido, de color verde, la línea correspondiente a los datos de la muestra. Está muy claro que en el caso del volumen de madera, la recta ajustada con los datos de la muestra no se puede confundir con los generados aleatoriamente suponiendo que no hay relación entre ambas variables. Sin embargo, en el caso de la edad al morir, la recta que corresponde a los datos disponibles se confunde con las que hemos simulado, por tanto, el valor de la pendiente queda explicado por la variabilidad intrínseca de los datos, diremos que esa diferencia no es estadísticamente significativa.\n\n\n\nFigura 14.19: Distribución de los coeficientes.\n\n\n\n\n\nPruebas de significación formales\nAunque pueden no ser necesarias en casos tan claros como los que acabamos de ver, conocer la distribución de los coeficientes nos permite realizar pruebas de significación de manera más directa, sin necesidad de realizar simulaciones.\n\n\n\n\n\n\nPruebas de significación y contraste de hipótesis\n\n\n\nUna prueba de significación es un caso particular de contraste de hipótesis donde se contrasta que el valor del parámetro es igual a cero. Significación equivale a “significativamente distinto de cero” es decir, que la variabilidad aleatoria no justifica la diferencia respecto a cero.\n\n\nLo habitual es contrastar que el coeficiente, ya sea \\(\\beta_0\\) o \\(\\beta_1\\) es igual a cero frente a la alternativa de que es distinto de cero.\n\nSobre el valor de la ordenada en el origen, \\(\\beta_0\\)\nTiene interés cuando hay razones para suponer que la recta pasa por el origen de coordenadas. Permite verificar que los resultados obtenidos no están en contradicción con ese supuesto.\nSi los datos cumplen las condiciones que antes hemos comentado tendremos:\n\\[b_0 \\sim N \\left(\\beta_0; \\; \\sigma_{b_0} \\right)\\] luego si \\(\\beta_0 = 0\\):\n\\[ \\frac{b_0-0}{\\sigma_{b_0}} \\sim N (0; 1) \\]\nPero como no conocemos \\(\\sigma_{b_0}\\) y usamos su estimación \\(s_{b_0}\\), el estadístico de prueba es:\n\\[ T = \\frac{b_0}{s_{b_0}} \\] y su distribución de referencia es una \\(t\\)-Student con \\(n-2\\) grados de libertad, siendo \\(n\\) el número de datos disponibles. Los 2 grados de libertad que se pierden tienen que ver con las restricciones que presentan los residuos cuando el modelo se ha ajustado por el método de los mínimos cuadrados \\(\\left ( \\sum e_i = 0 \\; \\text{y} \\; \\sum e_ix_i = 0 \\right )\\).\nComo regla general, si \\(|T| &gt; 2\\), es decir, el valor obtenido está a más de dos desviaciones típicas de cero, se rechaza la hipótesis nula. Por supuesto, también se puede calcular el \\(p\\)-valor exacto y tomar la decisión de acuerdo con el nivel de significación establecido.\nSi se decide eliminar \\(b_0\\) del modelo es necesario recalcular el valor de \\(b_1\\) con una fórmula específica para esta situación puesto que ya no estamos aplicando el criterio de los mínimos cuadrados. En el apéndice 10.4 se comentan algunas peculiaridades de este caso particular.\n\n\nSobre el valor de la pendiente de la recta, \\(\\beta_1\\)\nEste contraste siempre tiene interés. Se trata de verificar que la pendiente de la recta es significativamente distinta de cero. Si no lo es, una recta horizontal es compatible con los datos por lo que no se puede afirmar que haya relación entre la variable regresora y la respuesta.\nEl procedimiento es idéntico que para \\(\\beta_0\\). El estadístico de prueba ahora es:\n\\[ T = \\frac{b_1}{s_{b_1}} \\sim t-\\text{Student} \\;\\text{con} \\; n-2 \\; \\text{grados de libertad} \\]\n\n\n\n\n\n\nLigando las pruebas se significación para \\(r\\) y para \\(b_1\\)\n\n\n\nAquí texto\n\n\n\n\nResultados presentados por los paquetes de software estadístico\nEn general, los paquetes de software estadístico además de presentar los coeficientes del modelo también presentan los \\(p\\)-valores asociados a los contrastes que hemos comentado. La figura 14.19 muestra parte de la salida que proporciona el paquete de software estadístico Minitan en los dos casos que estamos considerando. Los coeficientes se calculan con las fórmulas que hemos deducido en el apartado 14.2 y sus desviaciones típicas con las que aparecen en el apéndice 10.2. Tal como hemos visto, el T-value que aparece en el listado es simplemente el cociente entre el coeficiente (Coef) y su desviación típica (SE Coef) y el p-valor (P-Value) es el área de cola -multiplicado por 2, ya que es una prueba bilateral- que deja ese T-Value en una distribución t-Student con \\(n-2\\) grados de libertad, siendo \\(n\\) el número de puntos usados en cada caso.\n\n\n\nFigura 14.20: Salida del paquete de software estadístico Minitab al ajustar modelos de regresión en los dos ejemplos que hemos visto.\n\n\nTambién puede interesar contrastar otros valores para la pendiente de la recta. Para verificar que un aparato mide correctamente la concentración de monoxido de carbono (CO) en el aire, se mide la concentración de 11 muestras con valores conocidos. Los resultados obtenidos son los que se indican en la tabla 14.7 (Navidi 2010, pág. 585).\n\n\nTabla 14.7: Concentración de CO. Valores reales y valores medidos (ppm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValor real\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n\n\nValor medido\n1\n11\n21\n28\n37\n48\n56\n68\n75\n86\n96\n\n\n\n\nLa figura 14.21 (izq.) muestra el diagrama bivariante de los valores medidos frente a los reales con su recta ajustada. El aparato estará bien calibrado si -en promedio- el valor de la concentración medida es igual a la concentración real. Es decir, si los datos se ajustan al modelo: \\(y = x\\).\n\n\n\nFigura 14.21: Recta ajustada de la concentración medida frente a la real (izq.). Rectas simuladas suponiendo que -en promedio- la concentración medida es igual a la real (der.)\n\n\nUsando Minitab tenemos los coeficientes del modelo y la prueba de significación para sus valores (figura 14.22).\n\n\n\nFigura 14.22: Salida del paquete de software estadístico Minitab al ajustar modelos de regresión en los dos ejemplos que hemos visto.\n\n\nVemos que la ordenada en el origen (Minitab le llama “Constant” pero ya dijimos que esta no nos parece una denominación adecuada) no es significativamente distinta de cero por lo que hemos recalculado el modelo sin ese término. En el nuevo modelo el valor de \\(b_1\\) (coeficiente de la concentración real) es claramente significativo (significativamente distinto de cero) pero no es eso lo que nos preocupa. Lo que queremos saber es si es significativamente distinto de 1. Minitab no lo hace automaticamente pero es muy fácil hacerlo a mano:\n\\[  T =  \\frac{b_1-1}{s_{b_1}} = \\frac{0.95351-1}{0.00598} = -7.77\\] Si efectivamente \\(\\beta_1 = 1\\) el valor de \\(T\\) pertenece a una distribución \\(t\\)-Student, pero si lo fuera su valor estaría comprendido aproximadamente entre -2 y 2, y el valor que hemos obtenido está muy lejos de este intervalo. Por tanto, la hipótesis nula de que \\(\\beta_1=1\\) queda descartada. El aparato no está bien calibrado.\nEn la figura 14.21 (der.) se han representado las rectas ajustadas (color azul) de valores de la concentración medida (\\(y\\)) en función de la concentración real (\\(x\\)) generados con el modelo \\(y = x + e\\) siendo \\(e\\) valores de una distribución Normal con media cero y una desviación típica igual a la que presentan los residuos del modelo obtenido. La recta ajustada con los datos reales aparece de color verde (es la misma que tenemos en el diagrama de la izquierda), claramente se ve que no se confunde con las generadas aleatoriamente suponiendo que el aparato está bien calibrado, a partir de una concentración de 50 tiende a dar valores menores que los reales.\n\n\n\n14.6.1 Intervalos de confianza para la respuesta\n-Intervalos de confianza para las predicciones.\nMas/menos dos sigmas de los residuos y listos.\n-IC para las observaciones."
  },
  {
    "objectID": "63_regresionMultiple.html#las-cosas-no-son-como-parecen",
    "href": "63_regresionMultiple.html#las-cosas-no-son-como-parecen",
    "title": "15  Regresión Múltiple",
    "section": "15.1 Las cosas no son como parecen",
    "text": "15.1 Las cosas no son como parecen\nEjemplo con 3 variables. Ejemplo examen."
  },
  {
    "objectID": "63_regresionMultiple.html#fuerza-bruta.-aquí-va-en-serio.",
    "href": "63_regresionMultiple.html#fuerza-bruta.-aquí-va-en-serio.",
    "title": "15  Regresión Múltiple",
    "section": "15.2 Fuerza bruta. Aquí va en serio.",
    "text": "15.2 Fuerza bruta. Aquí va en serio.\nBest subsets"
  },
  {
    "objectID": "63_regresionMultiple.html#un-caso-sencillo-una-variable-cuantitativa-y-otra-cualitativa",
    "href": "63_regresionMultiple.html#un-caso-sencillo-una-variable-cuantitativa-y-otra-cualitativa",
    "title": "15  Regresión Múltiple",
    "section": "15.3 Un caso sencillo: Una variable cuantitativa y otra cualitativa",
    "text": "15.3 Un caso sencillo: Una variable cuantitativa y otra cualitativa\npeso, altura, sexo, y datos UFFI (creo)"
  },
  {
    "objectID": "63_regresionMultiple.html#ojo-con-las-explicaciones-milagrosas",
    "href": "63_regresionMultiple.html#ojo-con-las-explicaciones-milagrosas",
    "title": "15  Regresión Múltiple",
    "section": "15.4 Ojo con las explicaciones milagrosas",
    "text": "15.4 Ojo con las explicaciones milagrosas\nTantos parámetros como datos"
  },
  {
    "objectID": "63_regresionMultiple.html#modelos-explicativos-y-modelos-predictivos",
    "href": "63_regresionMultiple.html#modelos-explicativos-y-modelos-predictivos",
    "title": "15  Regresión Múltiple",
    "section": "15.5 Modelos explicativos y modelos predictivos",
    "text": "15.5 Modelos explicativos y modelos predictivos\ntexto"
  },
  {
    "objectID": "63_regresionMultiple.html#a-tener-en-cuenta",
    "href": "63_regresionMultiple.html#a-tener-en-cuenta",
    "title": "15  Regresión Múltiple",
    "section": "15.6 A tener en cuenta",
    "text": "15.6 A tener en cuenta\nAnálisis exploratoio más complicado Lo mismo con el análisis de los residuos\nOtras medidas de calidad del ajuste.\nLa regresión no sirve para todo.\nEl valor de un coeficiente, si no va acompañado del valor de su desviación típica, no sirve prácticamente para nada."
  },
  {
    "objectID": "42_Proceso_CH.html",
    "href": "42_Proceso_CH.html",
    "title": "9  Esquema de razonamiento",
    "section": "",
    "text": "Todo provisional.\nQuizá Apéndice sobre por qué este tipo de razonamiento es controvertido (libro respuestas)."
  },
  {
    "objectID": "41_Conceptos_CH.html",
    "href": "41_Conceptos_CH.html",
    "title": "8  Conceptos",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCódigo\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "43_tiposError_CH.html",
    "href": "43_tiposError_CH.html",
    "title": "10  Tipos de error",
    "section": "",
    "text": "Este título es provisional"
  },
  {
    "objectID": "51_floresAspirina.html",
    "href": "51_floresAspirina.html",
    "title": "11  ¿La Aspirina alarga la vida de las flores?",
    "section": "",
    "text": "Planteamiento de la pregunta de manera clara y concreta\nDiseño de la recogida de los datos\nAnálisis de los resultados (distribución binomial)"
  },
  {
    "objectID": "52_duracionPilas.html",
    "href": "52_duracionPilas.html",
    "title": "12  ¿Duran más las pilas caras?",
    "section": "",
    "text": "Pregunta clara y concreta\nDiseño de la recogida de los datos con todas sus vicisitudes.\nAnálisis de los datos. Prueba no paramétrica y t-Student."
  },
  {
    "objectID": "62_regresionSimple.html#calidadAjuste",
    "href": "62_regresionSimple.html#calidadAjuste",
    "title": "14  Regresión Simple",
    "section": "14.3 Calidad del ajuste",
    "text": "14.3 Calidad del ajuste\nEl gráfico de la izquierda de la figura 14.8 muestra la relación entre la longitud de la circunferencia (X) de los troncos de un determinado tipo de árbol y el volumen de madera (Y) que se puede obtener de ellos (Fuente: Wolfram_Data_Repository 2016). Se observa que a más circunferencia mayor volumen de madera, tal como era de esperar, y la ecuación de la recta ajustada es útil para estimar cuanta madera se obtendrá de un tronco de determinado diámetro. Sin embargo, el gráfico de la derecha se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables. En este caso el modelo ajustado no sirve absolutamente para nada. Pero los dos modelos tienen el mismo aspecto y solo a la vista del valor de sus coeficientes es imposible saber cual de los dos es útil.\n\n\n\nFigura 14.8: Relación muy clara y relación inexistent, situaciones que no pueden distinguirse solo a la vista del modelo ajustado.\n\n\nEs necesario, por tanto, completar el modelo con una medida que informe de la calidad del ajuste obtenido. Esa medida es el coeficiente de determinación \\(R^2\\).\nPara calcular el valor de \\(R^2\\) empezamos poniéndonos en el peor de los casos: suponemos que \\(X\\) e \\(Y\\) son independientes, es decir, que el valor de \\(X\\) no aporta ninguna información sobre el valor de \\(Y\\). En este caso, la recta que muestra la relación entre ambas variables es una recta horizontal: la estimación del valor de \\(Y\\) es siempre la misma, sin importar el valor de \\(X\\), y la mejor apuesta para ese valor de \\(Y\\) -a falta de cualquier otra información- es su valor medio \\(\\bar{y}\\). A la suma de los cuadrados de los residuos correspondientes a esa recta horizontal que pasa por \\(\\bar{y}\\) le llamamos \\(Q_Y\\).\nA continuación calculamos la suma de los cuadrados de los residuos correspondientes a nuestra recta ajustada (la que minimiza la suma de los cuadrados de los residuos) y le llamaremos \\(Q_R\\). Cuanto mejor sea el ajuste menor será el valor de \\(Q_R\\) y mayor la diferencia entre \\(Q_Y\\) y \\(Q_R\\).\nEl valor de \\(R^2\\) es igual a la proporción de \\(Q_Y\\) explicada por \\(X\\), es decir, la proporción en que disminuye \\(Q_Y\\) gracias a la introducción de \\(X\\) como variable explicativa, es decir:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} \\]\nVeamos este cálculo en un ejemplo con datos sencillos. En la figura 14.9 tenemos 5 puntos que podrían representar la relación entre el peso y la estatura de 5 individuos. Si, ignorando la información aportada por la estatura, siempre damos una estimación del peso igual a su valor medio, será como ajustar a una recta horizontal y tendremos una suma de los cuadrados de los residuos \\(Q_Y = 56\\). Sin embargo, si utilizamos la información que aporta la estatura y realizamos el ajuste minimizando la suma de los cuadrados de los residuos tenemos \\(Q_R = 16\\).\n\n\n\nFigura 14.9: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\nHemos reducido la suma de los cuadrados de los residuos de 56 a 16, por tanto:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} = \\frac{56 - 16}{56} = 0.7143 \\]\nNormalmente nos referimos a este valor como un porcentaje. En este caso sería el 71.43%. En los ejemplos de la figura 14.8 estos valores son del 93,5% (volumen de madera) y 1,5% (edad al morir).\n\n\n\n\n\n\n\\(R^2\\) es el cuadrado del coeficiente de correlación \\(r\\)\n\n\n\nEfectivamente, en el caso que estamos considerando de regresión simple, el coeficiente de determinación \\(R^2\\) es igual al cuadrado del coeficiente de correlación \\(r\\). Este último puede variar entre \\(-1\\) y \\(1\\) por lo que, obviamente, \\(R^2\\) varía entre 0 y 1. La demostración es corta y fácil de encontrar en internet. Por ejemplo en:"
  },
  {
    "objectID": "62_regresionSimple.html#sec-calidadAjuste",
    "href": "62_regresionSimple.html#sec-calidadAjuste",
    "title": "14  Regresión Simple",
    "section": "14.3 Calidad del ajuste",
    "text": "14.3 Calidad del ajuste\nEl gráfico de la izquierda de la figura 14.8 muestra la relación entre la longitud de la circunferencia (X) de los troncos de un determinado tipo de árbol y el volumen de madera (Y) que se puede obtener de ellos (Fuente: Wolfram_Data_Repository 2016). Se observa que a más circunferencia mayor volumen de madera, tal como era de esperar, y la ecuación de la recta ajustada es útil para estimar cuanta madera se obtendrá de un tronco de determinado diámetro. Sin embargo, el gráfico de la derecha se ha realizado con los datos de un estudio publicado por Wilson y Mather (1974) citado por Draper y Smith (1998) donde se analiza la relación entre la edad al morir y la longitud de cierta línea de la mano a partir de una muestra de 50 personas fallecidas. A la vista del diagrama queda claro que no hay ninguna relación entre ambas variables. En este caso el modelo ajustado no sirve absolutamente para nada. Pero los dos modelos tienen el mismo aspecto y solo a la vista del valor de sus coeficientes es imposible saber cual de los dos es útil.\n\n\n\nFigura 14.8: Relación muy clara y relación inexistent, situaciones que no pueden distinguirse solo a la vista del modelo ajustado.\n\n\nEs necesario, por tanto, completar el modelo con una medida que informe de la calidad del ajuste obtenido. Esa medida es el coeficiente de determinación \\(R^2\\).\nPara calcular el valor de \\(R^2\\) empezamos poniéndonos en el peor de los casos: suponemos que \\(X\\) e \\(Y\\) son independientes, es decir, que el valor de \\(X\\) no aporta ninguna información sobre el valor de \\(Y\\). En este caso, la recta que muestra la relación entre ambas variables es una recta horizontal: la estimación del valor de \\(Y\\) es siempre la misma, sin importar el valor de \\(X\\), y la mejor apuesta para ese valor de \\(Y\\) -a falta de cualquier otra información- es su valor medio \\(\\bar{y}\\). A la suma de los cuadrados de los residuos correspondientes a esa recta horizontal que pasa por \\(\\bar{y}\\) le llamamos \\(Q_Y\\).\nA continuación calculamos la suma de los cuadrados de los residuos correspondientes a nuestra recta ajustada (la que minimiza la suma de los cuadrados de los residuos) y le llamaremos \\(Q_R\\). Cuanto mejor sea el ajuste menor será el valor de \\(Q_R\\) y mayor la diferencia entre \\(Q_Y\\) y \\(Q_R\\).\nEl valor de \\(R^2\\) es igual a la proporción de \\(Q_Y\\) explicada por \\(X\\), es decir, la proporción en que disminuye \\(Q_Y\\) gracias a la introducción de \\(X\\) como variable explicativa, es decir:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} \\]\nVeamos este cálculo en un ejemplo con datos sencillos. En la figura 14.9 tenemos 5 puntos que podrían representar la relación entre el peso y la estatura de 5 individuos. Si, ignorando la información aportada por la estatura, siempre damos una estimación del peso igual a su valor medio, será como ajustar a una recta horizontal y tendremos una suma de los cuadrados de los residuos \\(Q_Y = 56\\). Sin embargo, si utilizamos la información que aporta la estatura y realizamos el ajuste minimizando la suma de los cuadrados de los residuos tenemos \\(Q_R = 16\\).\n\n\n\nFigura 14.9: Múltiples opciones para minimizar la suma de los residuos en valor absoluto.\n\n\nHemos reducido la suma de los cuadrados de los residuos de 56 a 16, por tanto:\n\\[ R^2 = \\frac{Q_Y - Q_R}{Q_Y} = \\frac{56 - 16}{56} = 0.7143 \\]\nNormalmente nos referimos a este valor como un porcentaje. En este caso sería el 71.43%. En los ejemplos de la figura 14.8 estos valores son del 93,5% (volumen de madera) y 1,5% (edad al morir).\n\n\n\n\n\n\n\\(R^2\\) es el cuadrado del coeficiente de correlación \\(r\\)\n\n\n\nEfectivamente, en el caso que estamos considerando de regresión simple, el coeficiente de determinación \\(R^2\\) es igual al cuadrado del coeficiente de correlación \\(r\\). Este último puede variar entre \\(-1\\) y \\(1\\) por lo que, obviamente, \\(R^2\\) varía entre 0 y 1. La demostración es corta y fácil de encontrar en internet. Por ejemplo en:"
  },
  {
    "objectID": "62_regresionSimple.html#sec-calculoCoeficientes",
    "href": "62_regresionSimple.html#sec-calculoCoeficientes",
    "title": "14  Regresión Simple",
    "section": "14.2 Mínimos cuadrados. Cálculo de los coeficientes",
    "text": "14.2 Mínimos cuadrados. Cálculo de los coeficientes\nExiste una fórmula cerrada y con solución única para cada coeficiente, pero vamos a empezar identificando el valor de los coeficientes sin hacer uso de las fórmulas. Naturalmente, es mucho más rápido y más práctico usarlas o -mejor todavía- usar un paquete de software estadístico o una hoja de cálculo, pero hacerlo sin fórmulas permite entender perfectamente qué es lo que se está haciendo, y también descubrir algún detalle interesante.\n\nSin fórmulas\nRealizamos a ojo una primera estimación del valor de los coeficientes. A continuación, mediante un pequeño programa -o también usando una hoja de cálculo-, hacemos un barrido de los valores de \\(b_0\\) y \\(b_1\\) en torno a los estimados, identidicando el par que minimiza la suma de los cuadrados de los residuos.\nVayamos al diagrama de la figura 14.7 (izquierda) que ya habíamos visto en las figuras 14.2 y 14.3. La recta ajustada a ojo pasa por los puntos (-4,75; 0) y (5,75, 60) por lo que sus coeficientes son: \\(b_1\\) = 5,71 y \\(b_0\\) = 27,14. Sería mucha casualidad que esos fueran los valores exactos que estamos buscando, pero no andarán muy lejos. Vamos a crear una malla de valores de \\(b_0\\) y \\(b_1\\). Los valores de \\(b_0\\) variarán de 2 a 8 con incrementos de 0,1 y para cada uno de esos, los de \\(b_0\\) irán de 20 a 35 también en saltos de 0,1. A cada combinación de esos dos valores corresponde a una recta, y a cada recta una suma de los cuadrados de los residuos. El par de valores que minimizan esa suma de cuadrados son: \\(b_0\\) = 27,0 y \\(b_1\\) = 4,8.\n\n\n\nFigura 14.7: Recta ajustada a ojo (izq.) y suma de los cuadrados de los residuos para cada par de valores \\(b_0\\) y \\(b_1\\). En rojo, valores que la minimizan (der.).\n\n\n\n\n\n\n\n\nParaboloide de la suma de cuadrados\n\n\n\nCon los datos de nuestro ejemplo, la superficie que representa la suma de los cuadrados de los residuos es un paraboloide donde la localización del mínimo es visulamente muy clara. Pero lo nomal es que las curvas de nivel sean muy elípticas de manera que la representación no queda tan clara. Nosotros hemos logrado esa forma regular haciendo que la media de los valores de \\(X\\) sea igual a cero. De esta forma, los coeficientes son independientes y las curvas de nivel apararecen como círculos prácticamente concéntricos quedando más clara la idea que queremos representar.\n\n\n\n\nUsando las fórmulas\nEn el diagrama que representa la relación entre \\(X\\) e \\(Y\\) cada punto puede ser identificado por sus coordenadas \\((x_i, y_i)\\) con \\(1 \\leq i \\leq n\\) siendo \\(n\\) el número total de puntos. [creo que esto es redundante y habría que mejorarlo]\nCada uno de los puntos tiene un residuo asociado \\(e_i\\) y ese residuo es la diferencia entre el valor real de \\(y\\), es decir, \\(y_i\\) y su valor estimado \\(\\hat{y}_i\\), el que estará sobre la recta y que será igual a \\(b_0 + b_1 x_i\\). Por tanto, el valor del residuo asociado al punto \\(i\\) lo podemos escribir de la forma:\n\\[ e_i = y_i - \\left( b_0 + b_1 x_i \\right) \\] Por tanto, la suma de los cuadrados de los residuos, \\(S\\), será:\n\\[ S = \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right )^2 \\]\nTanto los valores de \\(y_i\\) como los de \\(x_i\\) vienen dados. La suma de cuadrados \\(S\\) es función de los valores de \\(b_0\\) y de \\(b_1\\). Se trata de hallar los valores de \\(b_0\\) y de \\(b_1\\) que minimizan esa suma de cuadrados. El mínimo lo tendremos en el punto en que la derivada de \\(S \\left(b_0, b_1 \\right )\\) respecto a \\(b_0\\) y respecto a \\(b_1\\) es igual a cero. Seguro que es un mínimo porque el máximo no está definido.\n\\[ \\frac{\\partial S}{\\partial b_0} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) \\]\n\\[ \\frac{\\partial S}{\\partial b_1} = -2 \\sum_{í=1}^n \\left(y_i - b_0 - b_1 x_i \\right ) x_i \\]\nIgualando a cero estas expresiones:\n\\[ \\sum_{í=1}^n y_i - nb_0 - b_1 \\sum_{í=1}^n x_i = 0  \\tag{14.1}\\]\n\\[ \\sum_{í=1}^n x_i y_i - b_0 \\sum_{í=1}^n x_i - b_1 \\sum_{í=1}^n  x_i^2  = 0  \\tag{14.2}\\]\nDividiendo por \\(n\\) todos los términos de la ecuación 14.1 tenemos:\n\\[ b_0 = \\bar{y} - b_1 \\bar{x} \\]\n\n\n\n\n\n\nLa recta ajustada pasa por el punto \\((\\bar{x}, \\bar{y})\\)\n\n\n\nDe la anterior expresón para \\(b_0\\) también se decude que \\(\\bar{y} = b_0 + b_1\\bar{x}\\). Es decir, la recta ajustada minimizando la suma de los cuadrados de los residuos siempre pasa por el punto \\((\\bar{x}, \\bar{y})\\) .\n\n\nSustituyendo la expresión de \\(b_0\\) en la ecuación 14.2 tenemos:\n\\[ \\sum_{í=1}^n x_i y_i - \\bar{y} \\sum_{í=1}^n x_i +  b_1\\bar{x} \\sum_{í=1}^n x_i- b_1 \\sum_{í=1}^n  x_i^2  = 0 \\]\nPara aligerar la notación no pondremos los límites a los sumatorios, que siempre son desde \\(i=1\\) hasta \\(n\\). Despejando \\(b_1\\) llegamos a:\n\\[ b_1 = \\frac{\\sum x_i y_i - \\bar{y} \\sum x_i}{\\sum x_i^2 - \\bar{x} \\sum x_i} \\] También la expresión de \\(b_1\\) se suele dar de la forma (ver Apéndice 10.1):\n\\[ b_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}  \\tag{14.3}\\]\nA partir de la ecuación 14.3 y recordando las expresiones de la covarianza y del coeficiente de correlación, llegamos a una expresión que también se ve con frecuencia en los libros de texto, seguramente porque una calculadora sencilla da directamente los tres valores que intervienen:\n\\[ b_1 = \\frac{Cov(XY)}{s_X^2} = \\frac{r_{XY} s_X s_Y}{s_X^2} = r_{XY} \\frac{s_Y}{s_X} \\] Calculando los coeficientes que corresponden a los datos de la figura 14.7 se obtiene:\n\\[ b_0 = 26,9615 \\qquad  \\qquad b_1 = 4,8616 \\] Ahora sí, con todos los decimales que queramos, aunque dar más decimales de los que tienen los datos es añadir números que no aportant ninguna información, dan una falsa sensación de precisión y complican la lectura del resultado.\n\n\n\n\n\n\n\nPor qué le llamamos modelo de regresión\n\n\n\nSean, por ejemplo, los puntos: (4; 3), (6; 8), (8; 12), (10; 10), (12; 12). La recta ajustada es: \\(y = 1+x\\). Si en vez de ajustar \\(y = f(x)\\) se ajusta \\(x=f(y)\\) ¿se obtendrá la ecuación resultante de despejar \\(x\\) en \\(y=f(x)\\), es decir: \\(x = -1 + y\\)? Si ajustamos \\(x=f(y)\\) la ecuación será: \\(x=1,57+0,714x\\). No es lo mismo minimizar la suma de los cuadrados de los residuos medidos en dirección vertical que en dirección horizontal (esto último no son los residuos)."
  },
  {
    "objectID": "11_sintesisNumerica.html",
    "href": "11_sintesisNumerica.html",
    "title": "1  Síntesis numérica de datos",
    "section": "",
    "text": "Apéndice 1.1"
  },
  {
    "objectID": "11_sintesisNumerica.html#acknowledgments",
    "href": "11_sintesisNumerica.html#acknowledgments",
    "title": "1  Síntesis numérica de datos",
    "section": "1.5 Acknowledgments",
    "text": "1.5 Acknowledgments\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\ntexto\n¿Qué hace esto aquí?"
  },
  {
    "objectID": "11_sintesisNumerica.html#apéndice-1.1-otras-medias",
    "href": "11_sintesisNumerica.html#apéndice-1.1-otras-medias",
    "title": "1  Síntesis numérica de datos",
    "section": "Apéndice 1.1: Otras medias",
    "text": "Apéndice 1.1: Otras medias\n\nMedia geométrica\nSi una población tenía 10.000 habitantes en el año cero, creció el primer año a una tasa del 5 %, el segundo a una tasa del 20 % y el tercer año al 50 % ¿A que tasa promedio ha crecido en estos tres años?\n\n\nTabla 1.2: Evolución de la población\n\n\n\n\n\n\n\n\n\nAño\nPoblación inicial\nTasa de crecimiento\nFactor de expansión\nPoblación al final del año\n\n\n\n\n1\n10.000\n0,05\n1,05\n10.500\n\n\n2\n10.500\n0,20\n1,20\n12.600\n\n\n3\n12.600\n0,50\n1,50\n18.900\n\n\n\n\nSi calculamos la media aritmética de la tasa de crecimiento tenemos: \\((0,05 + 0,20 + 0,50)/3 = 0,25\\) y el factor medio de expansión sería \\(1,25\\). Pero si la población hubiera crecido los tres años de esta forma, no se llegaría al mismo resultado final:\n\n\nTabla 1.3: Evolución de la población\n\n\n\n\n\n\n\n\n\nAño\nPoblación inicial\nTasa de crecimiento\nFactor de expansión\nPoblación al final del año\n\n\n\n\n1\n10.000\n0,25\n1,25\n12.500\n\n\n2\n12.500\n0,25\n1,25\n15.625\n\n\n3\n15.625\n0,25\n1,25\n19.531\n\n\n\n\nPor tanto, la media aritmética no es un buen indicador de la tasa media de crecimiento.\nSi la población crece a una tasa constante \\(i\\), para que al final del tercer año tenga el mismo efecto que las tasas del ejemplo, se debe verificar que:\n\\[ 10\\,000(1+i)(1+i)(1+i)=10\\,000(1+0,05)(1+0,20)(1+0,50) \\] De donde:\n\\[(1+i)= \\sqrt[3]{1,05 \\cdot 1,20 \\cdot 1,50}=1,2364\\]\nSi se hubiera tenido este factor de expansión cada año (nótese que es la media geométrica), hubiera conducido a una población final igual a la que tenemos.\n\n\n\n\n\n\nCuriosidades sobre la media geométrica\n\n\n\n\nA diferencia de la media aritmética, la media geométrica sólo se define para números positivos.\nLa media geométrica nunca es mayor que la media aritmética. La demostración para el caso de 2 valores es fácil por reducción al absurdo. Supongamos que: \\(\\sqrt{ab} &gt; (a+b)/2\\), entonces \\(ab &gt; (a^2+2ab+b^2)/4\\), de donde: \\(0 &gt;a^2-2ab+b^2\\). Como \\(a^2-2ab+b^2=(a-b)^2\\) es imposible que este valor sea negativo, luego es imposible que \\(\\sqrt{ab}&gt;(a+b)/2\\).\n\n\n\n\n\nMedia armónica\nSe define la media armónica de \\(x_1, x_2, ..., x_N\\) como:\n\\[Mh = \\frac{N}{\\large{\\frac{1}{x_1}+\\frac{1}{x_2}+...+\\frac{1}{x_N}}}\\]\nParece que esto sea un retorcimiento sin ningún interés, pero no. Si un coche recorre cierta distancia a una velocidad de 100 km/h y vuelve por el mismo camino a 120 km/h, la velocidad media a que ha realizado el viaje es:\n\\[Mh = \\frac{2}{\\large{\\frac{1}{100}+\\frac{1}{120}}} = 109,1 \\text{ km/h}\\]\ny no 110 km/h como en principio se podría pensar.\nObserve que la velocidad es igual a la distancia recorrida dividida por el tiempo tardado en recorrerla, es decir \\(v=d/t\\) y por tanto \\(t=d/v\\). En nuestro caso, si la distancia a recorrer es \\(d\\), el tiempo tardado en la ida es \\(t_1 =d/100\\) y el tiempo tardado en el regreso es \\(t_2 =d/120\\). De esta manera el tiempo total invertido en todo el recorrido \\((2d)\\) será \\(t=t_1+t_2\\) y la velocidad media se calcula de la forma:\n\\[ \\text{Velocidad media} = \\frac{\\text{Distancia total recorrida}}{\\text{Tiempo total empleado}} = \\frac{2d}{\\large{\\frac{d}{100}+\\frac{d}{120}}} \\] Otro ejemplo: Un avión recorre 3000 km. Los 1000 primeros a 700 km/h, los 1000 siguientes a 800 km/h, y los 1000 restantes a 900 km/h ¿Cuál ha sido su velocidad media? No ha sido 800 km/h sino 791,6 km/h.\n\n\n\n\nEl País, 2024. https://elpais.com/economia/negocios/2024-05-04/mi-jefe-cobra-77-veces-mas-que-yo-estas-son-las-empresas-espanolas-con-mayor-desigualdad-salarial-entre-directivos-y-empleados.html.\n\n\nINE. 2023. «Encuesta Anual de Estructura Salarial. Año 2021». Instituto Nacional de EStadística (INE), España. https://www.ine.es/prensa/ees_2021.pdf."
  },
  {
    "objectID": "11_sintesisNumerica.html#apéndice-a-otras-medias",
    "href": "11_sintesisNumerica.html#apéndice-a-otras-medias",
    "title": "1  Síntesis numérica de datos",
    "section": "Apéndice A: Otras medias",
    "text": "Apéndice A: Otras medias\n\nMedia geométrica\n\n\nMedia armónica"
  }
]